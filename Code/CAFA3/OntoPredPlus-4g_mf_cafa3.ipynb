{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a056c8a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Mar 29 16:28:05 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.106.00   Driver Version: 460.106.00   CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-DGXS...  Off  | 00000000:07:00.0 Off |                    0 |\n",
      "| N/A   41C    P0    53W / 300W |      0MiB / 32508MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-DGXS...  Off  | 00000000:08:00.0 Off |                    0 |\n",
      "| N/A   40C    P0    52W / 300W |      0MiB / 32508MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla V100-DGXS...  Off  | 00000000:0E:00.0 Off |                    0 |\n",
      "| N/A   40C    P0    53W / 300W |      0MiB / 32508MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla V100-DGXS...  Off  | 00000000:0F:00.0 Off |                    0 |\n",
      "| N/A   41C    P0    53W / 300W |      0MiB / 32508MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86add81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fb10a5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-29 16:28:07.538848: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-29 16:28:07.700793: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 10161890724206226324\n",
      "xla_global_id: -1\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-29 16:28:10.894631: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-29 16:28:10.912796: E tensorflow/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error\n",
      "2023-03-29 16:28:10.912825: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: dgx\n",
      "2023-03-29 16:28:10.912836: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: dgx\n",
      "2023-03-29 16:28:10.912888: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: NOT_FOUND: was unable to find libcuda.so DSO loaded into this program\n",
      "2023-03-29 16:28:10.912920: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 460.106.0\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8be3b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import copy\n",
    "import math\n",
    "# import gdown\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import tqdm.notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6e98a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "aspect = ['molecular','function']\n",
    "aspect_abbr = 'mf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ce40988",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'cafa3_train/mf_df.csv'\n",
    "# if not os.path.exists(file):\n",
    "#     url = \"https://drive.google.com/file/d/1RyeLQPFTMWAIr-OzELTWIx60ln-mZ7g_/view?usp=sharing\"\n",
    "#     output = file\n",
    "#     gdown.download(url=url, output=output, quiet=False, fuzzy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd949fb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Entry</th>\n",
       "      <th>Sequence</th>\n",
       "      <th>Gene Ontology (molecular function)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A0A060X6Z0</td>\n",
       "      <td>MPISSSSSSSTKSMRRAASELERSDSVTSPRFIGRRQSLIEDARKE...</td>\n",
       "      <td>GO:0003824;GO:0016491;GO:0016705;GO:0004497;GO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A0A078CGE6</td>\n",
       "      <td>MARQMTSSQFHKSKTLDNKYMLGDEIGKGAYGRVYIGLDLENGDFV...</td>\n",
       "      <td>GO:0016773;GO:0003824;GO:0004672;GO:0004674;GO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A0A086F3E3</td>\n",
       "      <td>MTKGRLEAFSDGVLAIIITIMVLELKVPEGSSWASLQPILPRFLAY...</td>\n",
       "      <td>GO:0022892;GO:0022838;GO:0005216;GO:0015075;GO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A0A087X1C5</td>\n",
       "      <td>MGLEALVPLAMIVAIFLLLVDLMHRHQRWAARYPPGPLPLPGLGNL...</td>\n",
       "      <td>GO:0003824;GO:0016491;GO:0016705;GO:0004497;GO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A0A096SRM5</td>\n",
       "      <td>MAANGGDHTSARPHVVLLPSAGMGHLVPFARLAVALSEGHGCNVSV...</td>\n",
       "      <td>GO:0003824;GO:0016758;GO:0016740;GO:0008194;GO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36105</th>\n",
       "      <td>V5JFY4</td>\n",
       "      <td>MGPWTLLLLHLPLVVSMLPAPTNVSIVSFNLEHTLTWLPGPETPDN...</td>\n",
       "      <td>GO:0004872;GO:0004888;GO:0003674;GO:0005488;GO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36106</th>\n",
       "      <td>V5YM14</td>\n",
       "      <td>MRPNLLAAAIAVPLSLLAAQIAQAGEGMWVPQQLPEIAGPLKKAGL...</td>\n",
       "      <td>GO:0003824;GO:0005515;GO:0042803;GO:0046983;GO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36107</th>\n",
       "      <td>V5YMB3</td>\n",
       "      <td>MRHPAFRLTLLASTVAFALAPQAAQAAPSAADRIAGTELIARDALF...</td>\n",
       "      <td>GO:0003824;GO:0008233;GO:0016787;GO:0003674;GO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36108</th>\n",
       "      <td>V9GXG1</td>\n",
       "      <td>MPYAEITVNLGKVTLGEENRKKMTNSCLKRHENSSLVQAVCALLNS...</td>\n",
       "      <td>GO:0004518;GO:0003824;GO:0016787;GO:0044877;GO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36109</th>\n",
       "      <td>W5EP13</td>\n",
       "      <td>MLLFAPTPPPSPATAHRRPGGSAASCIRCSSVRELDRSPSRPPLPP...</td>\n",
       "      <td>GO:0003824;GO:0016787;GO:0016791;GO:0042578;GO...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>36110 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Entry                                           Sequence  \\\n",
       "0      A0A060X6Z0  MPISSSSSSSTKSMRRAASELERSDSVTSPRFIGRRQSLIEDARKE...   \n",
       "1      A0A078CGE6  MARQMTSSQFHKSKTLDNKYMLGDEIGKGAYGRVYIGLDLENGDFV...   \n",
       "2      A0A086F3E3  MTKGRLEAFSDGVLAIIITIMVLELKVPEGSSWASLQPILPRFLAY...   \n",
       "3      A0A087X1C5  MGLEALVPLAMIVAIFLLLVDLMHRHQRWAARYPPGPLPLPGLGNL...   \n",
       "4      A0A096SRM5  MAANGGDHTSARPHVVLLPSAGMGHLVPFARLAVALSEGHGCNVSV...   \n",
       "...           ...                                                ...   \n",
       "36105      V5JFY4  MGPWTLLLLHLPLVVSMLPAPTNVSIVSFNLEHTLTWLPGPETPDN...   \n",
       "36106      V5YM14  MRPNLLAAAIAVPLSLLAAQIAQAGEGMWVPQQLPEIAGPLKKAGL...   \n",
       "36107      V5YMB3  MRHPAFRLTLLASTVAFALAPQAAQAAPSAADRIAGTELIARDALF...   \n",
       "36108      V9GXG1  MPYAEITVNLGKVTLGEENRKKMTNSCLKRHENSSLVQAVCALLNS...   \n",
       "36109      W5EP13  MLLFAPTPPPSPATAHRRPGGSAASCIRCSSVRELDRSPSRPPLPP...   \n",
       "\n",
       "                      Gene Ontology (molecular function)  \n",
       "0      GO:0003824;GO:0016491;GO:0016705;GO:0004497;GO...  \n",
       "1      GO:0016773;GO:0003824;GO:0004672;GO:0004674;GO...  \n",
       "2      GO:0022892;GO:0022838;GO:0005216;GO:0015075;GO...  \n",
       "3      GO:0003824;GO:0016491;GO:0016705;GO:0004497;GO...  \n",
       "4      GO:0003824;GO:0016758;GO:0016740;GO:0008194;GO...  \n",
       "...                                                  ...  \n",
       "36105  GO:0004872;GO:0004888;GO:0003674;GO:0005488;GO...  \n",
       "36106  GO:0003824;GO:0005515;GO:0042803;GO:0046983;GO...  \n",
       "36107  GO:0003824;GO:0008233;GO:0016787;GO:0003674;GO...  \n",
       "36108  GO:0004518;GO:0003824;GO:0016787;GO:0044877;GO...  \n",
       "36109  GO:0003824;GO:0016787;GO:0016791;GO:0042578;GO...  \n",
       "\n",
       "[36110 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(file)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "724323ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "677\n"
     ]
    }
   ],
   "source": [
    "go_terms_bp = set()\n",
    "for idx, row in df.iterrows():\n",
    "    for term in row['Gene Ontology ('+' '.join(aspect)+')'].split(';'):\n",
    "        go_terms_bp.add(term)\n",
    "go_terms_bp = list(go_terms_bp)\n",
    "go_terms_bp.sort()\n",
    "print(len(go_terms_bp))\n",
    "# print(go_terms_bp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72f8a1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_segments(sequence,segment_size=100,gap=30):\n",
    "    segments = []\n",
    "    start = 0\n",
    "    end = segment_size\n",
    "    while end <= len(sequence):\n",
    "        segments.append(sequence[start:end])\n",
    "        start += gap\n",
    "        end += gap\n",
    "    last_segment = sequence[start:]\n",
    "    segments.append(last_segment)\n",
    "    \n",
    "    return segments\n",
    "\n",
    "def get_training_data(df,segment_size=100,gap=30):\n",
    "    training_data = list()\n",
    "    for idx,row in tqdm.tqdm(df.iterrows()):\n",
    "        labels = [0] * len(go_terms_bp)\n",
    "        for term in row['Gene Ontology ('+' '.join(aspect)+')'].split(';'):\n",
    "            labels[go_terms_bp.index(term)] = 1\n",
    "        segments = get_segments(row['Sequence'],segment_size,gap)\n",
    "        for segment in segments:\n",
    "            training_data.append([row['Entry'],segment,labels])\n",
    "    return training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f1d6c8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13c861be80b147ec804edf91b96d693e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "601353\n"
     ]
    }
   ],
   "source": [
    "training_data = get_training_data(df,gap=30)\n",
    "print(len(training_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c0991f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngrams(segment,n=3):\n",
    "    ngrams = []\n",
    "    for i in range(len(segment)-n+1):\n",
    "        ngrams.append(segment[i:i+n])\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5358736d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing from scratch...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ecd234472c9429a89cfae86245c6b08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/848802 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "848802\n"
     ]
    }
   ],
   "source": [
    "# Generate training data of ngrams\n",
    "# if os.path.exists('bp/training_data_4grams.npy'):\n",
    "#     print('Loading saved ngrams...')\n",
    "#     training_data_ngrams = np.load('bp/training_data_4grams.npy',allow_pickle=True)\n",
    "# else:\n",
    "print('Preparing from scratch...')\n",
    "training_data_ngrams = []\n",
    "\n",
    "for i in tqdm.tqdm(range(len(training_data))):\n",
    "    training_data_ngrams.append([training_data[i][0],get_ngrams(training_data[i][1],n=4),training_data[i][2]])\n",
    "        \n",
    "#     np.save('bp/training_data_4grams.npy',training_data_ngrams)\n",
    "    \n",
    "print(len(training_data_ngrams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4acd14a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_skip_grams(segment,skip=1,n=3):\n",
    "    skip_grams = []\n",
    "    window_size = skip + n\n",
    "    for i in range(len(segment)-window_size+1):\n",
    "        window = segment[i:i+window_size]\n",
    "        indices = list(range(window_size))\n",
    "        indices.pop(0)\n",
    "        for idx in indices[::-1]:\n",
    "            temp = ''\n",
    "            for j in range(window_size):\n",
    "                if j!=idx:\n",
    "                    temp+=window[j]\n",
    "            skip_grams.append(temp)\n",
    "\n",
    "    return skip_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fa91e8e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing from scratch...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b6deeec7a6645be94db66083858dc8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/848802 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "848802\n"
     ]
    }
   ],
   "source": [
    "# if os.path.exists('bp/training_data_skip1_4grams.npy'):\n",
    "#     print('Loading saved skip grams...')\n",
    "#     training_data_skip_grams = np.load('bp/training_data_skip1_4grams.npy',allow_pickle=True)\n",
    "# else:\n",
    "print('Preparing from scratch...')\n",
    "training_data_skip_grams = []\n",
    "for i in tqdm.tqdm(range(len(training_data))):\n",
    "    training_data_skip_grams.append([training_data[i][0],get_skip_grams(training_data[i][1],n=4),training_data[i][2]])\n",
    "#     np.save('bp/training_data_skip1_4grams.npy',training_data_skip_grams)\n",
    "print(len(training_data_skip_grams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11d856c7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-4445aae0c090>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_addons\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtfa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e391f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assuming training_data as global variable\n",
    "\n",
    "def train_test_split(X,y,fold_no,prev_index,Kfolds=5):\n",
    "    test_split = 1/Kfolds\n",
    "    \n",
    "    start_index = prev_index\n",
    "    end_index = (fold_no + 1) * (test_split) * len(X)\n",
    "    end_index = round(end_index)\n",
    "    \n",
    "    if end_index==len(X):\n",
    "        end_index -= 1\n",
    "    \n",
    "    entry = training_data[end_index][0]\n",
    "    entries = [sample[0] for sample in training_data]\n",
    "    \n",
    "    first_occurence = entries.index(entry)\n",
    "    entries.reverse()\n",
    "    \n",
    "    last_occurence = entries.index(entry)\n",
    "    last_occurence = len(entries) - last_occurence - 1\n",
    "    \n",
    "    del entries\n",
    "    gc.collect()\n",
    "    \n",
    "    end_index = first_occurence if (abs(end_index-first_occurence) < abs(end_index-last_occurence)) else last_occurence\n",
    "    \n",
    "    X_test = X[start_index:end_index+1]\n",
    "    y_test = y[start_index:end_index+1]\n",
    "    X_train = X[:start_index]\n",
    "    X_train.extend(X[end_index+1:])\n",
    "    y_train = y[:start_index]\n",
    "    y_train.extend(y[end_index+1:])\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test, start_index, end_index + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "730f9429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAX_WORDS = 13824\n",
    "# MAX_LEN_NG = 98 #100\n",
    "# MAX_LEN_SG = 291 #300\n",
    "MAX_WORDS = 331776\n",
    "MAX_LEN_NG = 97 #100\n",
    "MAX_LEN_SG = 384 #300\n",
    "# MAX_WORDS = 7962624\n",
    "# MAX_LEN_NG = 96 #100\n",
    "# MAX_LEN_SG = 475 #300\n",
    "\n",
    "def tokenization(X_train,X_test,maxlen):\n",
    "\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=MAX_WORDS)\n",
    "    tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "    X_train = tokenizer.texts_to_sequences(X_train)\n",
    "    X_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "    X_train = tf.keras.preprocessing.sequence.pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
    "    X_test = tf.keras.preprocessing.sequence.pad_sequences(X_test, padding='post', maxlen=maxlen)\n",
    "    \n",
    "    return X_train, X_test, vocab_size, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "10a7f090",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "\n",
    "false_negative_penalty = 6\n",
    "false_positive_penalty = 1\n",
    "\n",
    "def custom_loss(y_true, y_logit):\n",
    "\n",
    "    loss = float(0)\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_logit = tf.cast(y_logit, tf.float32)\n",
    "    \n",
    "    first_term = false_negative_penalty * float(y_true) * - K.log(y_logit + K.epsilon())\n",
    "    second_term = false_positive_penalty * (1 - float(y_true)) * - K.log(1 - y_logit + K.epsilon())\n",
    "    \n",
    "    loss = K.mean(first_term+second_term)\n",
    "\n",
    "    return loss\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    \n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)), axis=1)\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)), axis=1)\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    \n",
    "    return K.mean(precision)\n",
    "\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)), axis=1)\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)), axis=1)\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return K.mean(recall)\n",
    "\n",
    "def f1_score(y_true, y_pred):\n",
    "    rec = recall(y_true,y_pred)\n",
    "    prec = precision(y_true,y_pred)\n",
    "    f1 = 2*prec*rec/(prec+rec)\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "36c30cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class attention(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, return_sequences=True,**kwargs):\n",
    "        self.return_sequences = return_sequences\n",
    "        super(attention,self).__init__()\n",
    "\n",
    "    def get_config(self):\n",
    "      config = super().get_config().copy()\n",
    "      config.update({\n",
    "          'return_sequences': self.return_sequences \n",
    "      })\n",
    "      return config\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        \n",
    "        self.W=self.add_weight(name=\"att_weight\", shape=(input_shape[-1],1),\n",
    "                               initializer=\"normal\")\n",
    "        self.b=self.add_weight(name=\"att_bias\", shape=(input_shape[1],1),\n",
    "                               initializer=\"zeros\")\n",
    "        \n",
    "        super(attention,self).build(input_shape)\n",
    "        \n",
    "    def call(self, x):\n",
    "        \n",
    "        e = K.tanh(K.dot(x,self.W)+self.b)\n",
    "        a = K.softmax(e, axis=1)\n",
    "        output = x*a\n",
    "        \n",
    "        if self.return_sequences:\n",
    "            return output\n",
    "        \n",
    "        return K.sum(output, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "03b912ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 677 #For bp (Change according to aspects)\n",
    "\n",
    "def get_model_ng_sg(vocab_size_ng, vocab_size_sg):\n",
    "    #Input layers\n",
    "\n",
    "    input_ngrams = tf.keras.layers.Input(shape=(MAX_LEN_NG,)) \n",
    "    input_skip_grams = tf.keras.layers.Input(shape=(MAX_LEN_SG,)) \n",
    "\n",
    "    #embeddings\n",
    "    embedding_layer_ngrams = tf.keras.layers.Embedding(vocab_size_ng, 32)(input_ngrams)\n",
    "    embedding_layer_skip_grams = tf.keras.layers.Embedding(vocab_size_sg, 32)(input_skip_grams)\n",
    "\n",
    "    #BI-LSTMs for each of the inputs\n",
    "    sequence_output_1 = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,return_sequences=True))(embedding_layer_ngrams)\n",
    "    attention_output_1 = attention(return_sequences=False)(sequence_output_1)\n",
    "    dropout_1 = tf.keras.layers.Dropout(0.3)(attention_output_1)\n",
    "    dense_layer_1 = tf.keras.layers.Dense(NUM_CLASSES, activation='sigmoid')(dropout_1)\n",
    "\n",
    "    sequence_output_2 = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True))(embedding_layer_skip_grams)\n",
    "    attention_output_2 = attention(return_sequences=False)(sequence_output_2)\n",
    "    dropout_2 = tf.keras.layers.Dropout(0.3)(attention_output_2)\n",
    "    dense_layer_2 = tf.keras.layers.Dense(NUM_CLASSES, activation='sigmoid')(dropout_2)\n",
    "\n",
    "    max_layer = tf.keras.layers.Maximum()([dense_layer_1,dense_layer_2])\n",
    "\n",
    "    model = tf.keras.models.Model(\n",
    "        inputs=[\n",
    "            input_ngrams,\n",
    "            input_skip_grams\n",
    "        ], \n",
    "        outputs=max_layer)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84680b09",
   "metadata": {},
   "source": [
    "### Model using skip grams and ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6163d660",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Considering ngrams and skip grams\n",
    "X_train_ng = [' '.join(sample[1]) for sample in training_data_ngrams]\n",
    "X_train_sg = [' '.join(sample[1]) for sample in training_data_skip_grams]\n",
    "y_train = [sample[2] for sample in training_data]\n",
    "\n",
    "del training_data_ngrams\n",
    "del training_data_skip_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "32366243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "848802 848802\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train_ng),len(X_train_sg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27dfda2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Entry</th>\n",
       "      <th>Sequence</th>\n",
       "      <th>Gene Ontology (molecular function)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>T100900000026</td>\n",
       "      <td>MAESFKELDPDSSMGKALEMTCAIQNQLARILAEFEMTLERDVLQP...</td>\n",
       "      <td>GO:0003824;GO:0016818;GO:0016462;GO:0003924;GO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>T100900000046</td>\n",
       "      <td>MRLCIPQVLLALFLSMLTAPGEGSRRRATQEDTTQPALLRLSDHLL...</td>\n",
       "      <td>GO:0005515;GO:0005488;GO:0003674;GO:0042802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>T100900000115</td>\n",
       "      <td>MNNLSFSELCCLFCCPPCPGKIASKLAFLPPDPTYTLMCDESGSRW...</td>\n",
       "      <td>GO:0016790;GO:0003824;GO:0016788;GO:0016787;GO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>T100900000116</td>\n",
       "      <td>MPEPGPRMNGFSLGELCWLFCCPPCPSRIAAKLAFLPPEPTYTVLA...</td>\n",
       "      <td>GO:0016790;GO:0003824;GO:0016788;GO:0016787;GO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>T100900000161</td>\n",
       "      <td>MADDLEQQPQGWLSSWLPTWRPTSMSQLKNVEARILQCLQNKFLAR...</td>\n",
       "      <td>GO:0004620;GO:0003824;GO:0052689;GO:0016788;GO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1132</th>\n",
       "      <td>T992870001087</td>\n",
       "      <td>MIDGKTANEIFDSIRQHIIAGTLRAEDSLPPVRELASELKVNRNTV...</td>\n",
       "      <td>GO:0030170;GO:0003700;GO:1901363;GO:0001071;GO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1133</th>\n",
       "      <td>T992870001259</td>\n",
       "      <td>MKQGLQLRLSQQLAMTPQLQQAIRLLQLSTLELQQELQQALENNPL...</td>\n",
       "      <td>GO:1901363;GO:1990837;GO:0001071;GO:0003690;GO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1134</th>\n",
       "      <td>T992870001336</td>\n",
       "      <td>MDYQNNVSEERVAEMIWDAVSEGATLKDVHGIPQDMMDGLYAHAYE...</td>\n",
       "      <td>GO:0005515;GO:0005488;GO:0003674;GO:0042802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1135</th>\n",
       "      <td>T992870001601</td>\n",
       "      <td>MTVDSNTSSGRGNDPEQIDLIELLLQLWRGKMTIIVAVIIAILLAV...</td>\n",
       "      <td>GO:0005515;GO:0005488;GO:0003674;GO:0042802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1136</th>\n",
       "      <td>T992870001784</td>\n",
       "      <td>MEAIKGSDVNVPDAVFAWLLDGRGGVKPLEDNDVIDSQHPCWLHLN...</td>\n",
       "      <td>GO:0005515;GO:0005488;GO:0003674;GO:0042802</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1137 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Entry                                           Sequence  \\\n",
       "0     T100900000026  MAESFKELDPDSSMGKALEMTCAIQNQLARILAEFEMTLERDVLQP...   \n",
       "1     T100900000046  MRLCIPQVLLALFLSMLTAPGEGSRRRATQEDTTQPALLRLSDHLL...   \n",
       "2     T100900000115  MNNLSFSELCCLFCCPPCPGKIASKLAFLPPDPTYTLMCDESGSRW...   \n",
       "3     T100900000116  MPEPGPRMNGFSLGELCWLFCCPPCPSRIAAKLAFLPPEPTYTVLA...   \n",
       "4     T100900000161  MADDLEQQPQGWLSSWLPTWRPTSMSQLKNVEARILQCLQNKFLAR...   \n",
       "...             ...                                                ...   \n",
       "1132  T992870001087  MIDGKTANEIFDSIRQHIIAGTLRAEDSLPPVRELASELKVNRNTV...   \n",
       "1133  T992870001259  MKQGLQLRLSQQLAMTPQLQQAIRLLQLSTLELQQELQQALENNPL...   \n",
       "1134  T992870001336  MDYQNNVSEERVAEMIWDAVSEGATLKDVHGIPQDMMDGLYAHAYE...   \n",
       "1135  T992870001601  MTVDSNTSSGRGNDPEQIDLIELLLQLWRGKMTIIVAVIIAILLAV...   \n",
       "1136  T992870001784  MEAIKGSDVNVPDAVFAWLLDGRGGVKPLEDNDVIDSQHPCWLHLN...   \n",
       "\n",
       "                     Gene Ontology (molecular function)  \n",
       "0     GO:0003824;GO:0016818;GO:0016462;GO:0003924;GO...  \n",
       "1           GO:0005515;GO:0005488;GO:0003674;GO:0042802  \n",
       "2     GO:0016790;GO:0003824;GO:0016788;GO:0016787;GO...  \n",
       "3     GO:0016790;GO:0003824;GO:0016788;GO:0016787;GO...  \n",
       "4     GO:0004620;GO:0003824;GO:0052689;GO:0016788;GO...  \n",
       "...                                                 ...  \n",
       "1132  GO:0030170;GO:0003700;GO:1901363;GO:0001071;GO...  \n",
       "1133  GO:1901363;GO:1990837;GO:0001071;GO:0003690;GO...  \n",
       "1134        GO:0005515;GO:0005488;GO:0003674;GO:0042802  \n",
       "1135        GO:0005515;GO:0005488;GO:0003674;GO:0042802  \n",
       "1136        GO:0005515;GO:0005488;GO:0003674;GO:0042802  \n",
       "\n",
       "[1137 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv('cafa3_test/mf_df.csv')\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37914add",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "181ae16809f449bcad0607fe9101620f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16015\n"
     ]
    }
   ],
   "source": [
    "testing_data = get_training_data(test_df,gap=30)\n",
    "print(len(testing_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bedb4917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing from scratch...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02e24c753a794bbb9664055045e11f7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22753 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22753\n"
     ]
    }
   ],
   "source": [
    "print('Preparing from scratch...')\n",
    "testing_data_ngrams = []\n",
    "\n",
    "for i in tqdm.tqdm(range(len(testing_data))):\n",
    "    testing_data_ngrams.append([testing_data[i][0],get_ngrams(testing_data[i][1],n=4),testing_data[i][2]])\n",
    "    \n",
    "print(len(testing_data_ngrams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "da1d8eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing from scratch...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42add90f6e914a2897ef1ee3742b221c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22753 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22753\n"
     ]
    }
   ],
   "source": [
    "print('Preparing from scratch...')\n",
    "testing_data_skip_grams = []\n",
    "\n",
    "for i in tqdm.tqdm(range(len(testing_data))):\n",
    "    testing_data_skip_grams.append([testing_data[i][0],get_skip_grams(testing_data[i][1],n=4),testing_data[i][2]])\n",
    "    \n",
    "print(len(testing_data_skip_grams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5eac38b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Considering ngrams and skip grams\n",
    "X_test_ng = [' '.join(sample[1]) for sample in testing_data_ngrams]\n",
    "X_test_sg = [' '.join(sample[1]) for sample in testing_data_skip_grams]\n",
    "y_test = [sample[2] for sample in testing_data]\n",
    "\n",
    "del testing_data_ngrams\n",
    "del testing_data_skip_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2079b237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22753 22753\n"
     ]
    }
   ],
   "source": [
    "print(len(X_test_ng),len(X_test_sg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2f8a951e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# recs = []\n",
    "# precs = []\n",
    "# f1s = []\n",
    "\n",
    "# for i in range(1):\n",
    "    \n",
    "# #     print('Splitting into train-test...')\n",
    "# #     X_train_ng, y_train, X_test_ng, y_test, start_index, prev_index1 = train_test_split(X_ng,y,i,prev_index,Kfolds)\n",
    "# #     X_train_sg, _, X_test_sg, _, _, _= train_test_split(X_sg,y,i,prev_index,Kfolds)\n",
    "    \n",
    "# #     prev_index = prev_index1\n",
    "    \n",
    "#     print('Tokenizing...')\n",
    "#     X_train_ng, X_test_ng, vocab_size_ng, tokenizer1 = tokenization(X_train_ng, X_test_ng, MAX_LEN_NG)  \n",
    "#     X_train_sg, X_test_sg, vocabsize_sg, tokenizer2 = tokenization(X_train_sg, X_test_sg, MAX_LEN_SG)\n",
    "    \n",
    "   \n",
    "# print('Shuffling...')   \n",
    "#     shuffled = [[X_train_ng[i],X_train_sg[i],y_train[i]] for i in range(len(X_train_ng))]\n",
    "#     np.random.shuffle(shuffled)\n",
    "\n",
    "#     X_train_ng = [shuffled[i][0] for i in range(len(shuffled))]\n",
    "#     X_train_sg = [shuffled[i][1] for i in range(len(shuffled))]\n",
    "#     y_train = [shuffled[i][2] for i in range(len(shuffled))]\n",
    "#     X_train_ng = np.array(X_train_ng)\n",
    "#     X_train_sg = np.array(X_train_sg)\n",
    "#     y_train = np.array(y_train)\n",
    "    \n",
    "    \n",
    "#     model = get_model_ng_sg(vocab_size_ng, vocab_size_sg)\n",
    "    \n",
    "#     model.compile(\n",
    "#         loss=custom_loss, \n",
    "#         optimizer='adam', \n",
    "#         metrics=[\n",
    "#             recall,\n",
    "#             precision,\n",
    "#             f1_score\n",
    "#         ])\n",
    "\n",
    "#     print('Training...')\n",
    "#     history = model.fit([X_train_ng,X_train_sg], y_train, batch_size=32, epochs=10,validation_split=0.2)\n",
    "    \n",
    "#     print('Evaluating model...')\n",
    "#     predictions = model.predict([X_test_ng,X_test_sg])\n",
    "    \n",
    "#     print('Computing Metrics...\\n')\n",
    "#     compute_metrics(predictions)\n",
    "    \n",
    "# #     recs.append(rec.numpy())\n",
    "# #     precs.append(prec.numpy())\n",
    "# #     f1s.append(f1.numpy())\n",
    "\n",
    "# # print('Recall:',sum(recs)/len(recs))\n",
    "# # print('Precision:',sum(precs)/len(precs))\n",
    "# # print('F1-Score:',sum(f1s)/len(f1s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66234f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import FUNC_DICT, Ontology, NAMESPACES\n",
    "NUM_CLASSES = 677\n",
    "\n",
    "train_df_original = pd.read_pickle('data_cafa3/train_data.pkl')\n",
    "test_df_original = pd.read_pickle('data_cafa3/test_data.pkl')\n",
    "annotations = train_df_original['annotations'].values\n",
    "annotations = list(map(lambda x: set(x), annotations))\n",
    "test_annotations = test_df_original['annotations'].values\n",
    "test_annotations = list(map(lambda x: set(x), test_annotations))\n",
    "\n",
    "go_rels = Ontology('data_cafa3/go.obo', with_rels=True)\n",
    "go_rels.calculate_ic(annotations + test_annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08fb6188",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_annotations(real_annots, pred_annots):\n",
    "    total = 0\n",
    "    p = 0.0\n",
    "    r = 0.0\n",
    "    p_total= 0\n",
    "    fps = []\n",
    "    fns = []\n",
    "    ru = 0.0\n",
    "    mi = 0.0\n",
    "    for i in range(len(real_annots)):\n",
    "        if len(real_annots[i]) == 0:\n",
    "            continue\n",
    "        tp = set(real_annots[i]).intersection(set(pred_annots[i]))\n",
    "        fp = set(pred_annots[i]) - tp\n",
    "        fn = set(real_annots[i]) - tp\n",
    "        for go_id in fp:\n",
    "            mi += go_rels.get_ic(go_id)\n",
    "        for go_id in fn:\n",
    "            ru += go_rels.get_ic(go_id)\n",
    "        fps.append(fp)\n",
    "        fns.append(fn)\n",
    "        tpn = len(tp)\n",
    "        fpn = len(fp)\n",
    "        fnn = len(fn)\n",
    "        total += 1\n",
    "        recall = tpn / (1.0 * (tpn + fnn))\n",
    "        r += recall\n",
    "        if len(pred_annots[i]) > 0:\n",
    "            p_total += 1\n",
    "            precision = tpn / (1.0 * (tpn + fpn))\n",
    "            p += precision\n",
    "    ru /= total\n",
    "    mi /= total\n",
    "    r /= total\n",
    "    if p_total > 0:\n",
    "        p /= p_total\n",
    "    f = 0.0\n",
    "    if p + r > 0:\n",
    "        f = 2 * p * r / (p + r)\n",
    "    s = math.sqrt(ru * ru + mi * mi)\n",
    "    return f, p, r, fps, fns, s, ru, mi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3d5d51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d734cd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save('bp_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb50bb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('bp_predictions',predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ab990bc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16015\n"
     ]
    }
   ],
   "source": [
    "predictions = np.load('mf_predictions.npy')\n",
    "print(len(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "704437fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.5649812e-06, 7.0553106e-06, 2.6265203e-05, ..., 3.3719673e-06,\n",
       "        1.2522912e-05, 1.8447822e-04],\n",
       "       [1.5237396e-06, 2.1675476e-06, 2.1345656e-06, ..., 2.4749274e-06,\n",
       "        3.7304794e-06, 1.1089611e-04],\n",
       "       [8.5519679e-07, 1.1818065e-06, 2.0842876e-06, ..., 1.3200661e-06,\n",
       "        1.5538770e-06, 1.5533180e-03],\n",
       "       ...,\n",
       "       [2.0263253e-03, 6.8799150e-04, 8.4737950e-04, ..., 4.8182919e-03,\n",
       "        1.2230397e-04, 8.5061352e-04],\n",
       "       [4.4994707e-05, 3.4236509e-05, 5.1157840e-04, ..., 4.7979091e-04,\n",
       "        1.2377290e-05, 3.1822205e-03],\n",
       "       [1.3809440e-04, 2.3452632e-05, 1.4409448e-04, ..., 1.2636981e-04,\n",
       "        2.3777982e-06, 7.5844483e-04]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f2506bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_annotations = []\n",
    "for i in range(len(df)):\n",
    "    terms_set = set(df.iloc[i]['Gene Ontology (molecular function)'].split(';'))\n",
    "    train_annotations.append(terms_set)\n",
    "\n",
    "prot_index = {}\n",
    "for i, row in enumerate(df.itertuples()):\n",
    "    prot_index[row.Entry] = i\n",
    "\n",
    "diamond_scores_file = 'data_cafa3/cafa3_matches_mf.tsv'\n",
    "diamond_scores = {}\n",
    "with open(diamond_scores_file) as f:\n",
    "    for line in f:\n",
    "        it = line.strip().split()\n",
    "        if it[0] not in diamond_scores:\n",
    "            diamond_scores[it[0]] = {}\n",
    "        diamond_scores[it[0]][it[1]] = float(it[2])\n",
    "\n",
    "blast_preds = []\n",
    "#print('Diamond preds')\n",
    "for i, row in enumerate(test_df.itertuples()):\n",
    "    annots = {}\n",
    "    prot_id = row.Entry\n",
    "    # BlastKNN\n",
    "    if prot_id in diamond_scores:\n",
    "        sim_prots = diamond_scores[prot_id]\n",
    "        allgos = set()\n",
    "        total_score = 0.0\n",
    "        for p_id, score in sim_prots.items():\n",
    "            allgos |= train_annotations[prot_index[p_id]]\n",
    "            total_score += score\n",
    "        allgos = list(sorted(allgos))\n",
    "        sim = np.zeros(len(allgos), dtype=np.float32)\n",
    "        for j, go_id in enumerate(allgos):\n",
    "            s = 0.0\n",
    "            for p_id, score in sim_prots.items():\n",
    "                if go_id in train_annotations[prot_index[p_id]]:\n",
    "                    s += score\n",
    "            sim[j] = s / total_score\n",
    "        ind = np.argsort(-sim)\n",
    "        for go_id, score in zip(allgos, sim):\n",
    "            annots[go_id] = score\n",
    "    blast_preds.append(annots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "746a9e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "96\n"
     ]
    }
   ],
   "source": [
    "#Verifying \n",
    "count = 0\n",
    "for term in blast_preds[0].keys():\n",
    "    if term not in go_terms_bp:\n",
    "        print(term)\n",
    "        count+=1\n",
    "print(count)\n",
    "print(len(blast_preds[0].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "537c58b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_predictions = []\n",
    "actual_y_test = []\n",
    "\n",
    "current_entry = ''\n",
    "counter = 0\n",
    "total_counts = 0\n",
    "start_index = 0\n",
    "\n",
    "if len(predictions) == len(testing_data):\n",
    "    temp = np.zeros(NUM_CLASSES)\n",
    "    for i in range(len(predictions)):\n",
    "        if current_entry != testing_data[start_index+i][0]:\n",
    "            #compute prev\n",
    "            if i!=0:\n",
    "                temp /= counter\n",
    "                final_predictions.append(temp)\n",
    "\n",
    "            #reset\n",
    "            total_counts += counter\n",
    "            counter = 1\n",
    "            temp = np.zeros(NUM_CLASSES)\n",
    "\n",
    "            #init new\n",
    "            current_entry = testing_data[start_index+i][0]\n",
    "            temp += np.array(predictions[i])\n",
    "            actual_y_test.append(testing_data[start_index+i][2])\n",
    "        else:\n",
    "            temp += np.array(predictions[i])\n",
    "            counter += 1\n",
    "\n",
    "    total_counts += counter\n",
    "    temp /= counter\n",
    "    final_predictions.append(temp)\n",
    "\n",
    "else:\n",
    "    print('Lengths of predictions dont match with test data')\n",
    "\n",
    "final_predictions = np.array(final_predictions, dtype=float)\n",
    "actual_y_test = np.array(actual_y_test, dtype=float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9f4065d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bce140742f6545bbb78f526b0daea3e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/101 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fmax: 0.566, Smin: 7.276, threshold: 0.5, AUPR: 0.577\n"
     ]
    }
   ],
   "source": [
    "best_fmax = 0.0    \n",
    "final_predictions_avg = copy.deepcopy(final_predictions)\n",
    "for i in range(len(final_predictions_avg)):\n",
    "    for j in range(len(go_terms_bp)):\n",
    "        if go_terms_bp[j] in blast_preds[i].keys():\n",
    "            final_predictions_avg[i][j] = (blast_preds[i][go_terms_bp[j]] + final_predictions_avg[i][j])/2\n",
    "\n",
    "fmax = 0.0\n",
    "tmax = 0.0\n",
    "precisions = []\n",
    "recalls = []\n",
    "smin = 1000000.0\n",
    "rus = []\n",
    "mis = []\n",
    "    \n",
    "\n",
    "for t in tqdm.tqdm(range(0, 101)):\n",
    "\n",
    "    threshold = t / 100.0\n",
    "    pred_annots = []\n",
    "    real_annots = []\n",
    "    for i in range(len(final_predictions_avg)):\n",
    "        new_preds = []\n",
    "        new_ys = []\n",
    "        for j in range(NUM_CLASSES):\n",
    "            if final_predictions_avg[i][j]>=threshold:\n",
    "                new_preds.append(go_terms_bp[j]) #GO_TERMS_BP\n",
    "            if actual_y_test[i][j]==1:\n",
    "                new_ys.append(go_terms_bp[j])\n",
    "        pred_annots.append(new_preds)\n",
    "        real_annots.append(new_ys)\n",
    "\n",
    "    fscore, prec, rec, fps, fns, s, ru, mi = evaluate_annotations(real_annots, pred_annots)\n",
    "    avg_fp = sum(map(lambda x: len(x), fps)) / len(fps)\n",
    "    avg_ic = sum(map(lambda x: sum(map(lambda go_id: go_rels.get_ic(go_id), x)), fps)) / len(fps)\n",
    "    precisions.append(prec)\n",
    "    recalls.append(rec)\n",
    "\n",
    "    if fmax < fscore:\n",
    "        fmax = fscore\n",
    "        tmax = threshold\n",
    "    if smin > s:\n",
    "        smin = s\n",
    "precisions = np.array(precisions)\n",
    "recalls = np.array(recalls)\n",
    "sorted_index = np.argsort(recalls)\n",
    "recalls = recalls[sorted_index]\n",
    "precisions = precisions[sorted_index]\n",
    "aupr = np.trapz(precisions, recalls)\n",
    "\n",
    "print(f'Fmax: {fmax:0.3f}, Smin: {smin:0.3f}, threshold: {tmax}, AUPR: {aupr:0.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b7e2ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
