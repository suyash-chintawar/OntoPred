{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a056c8a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Jan 13 10:15:31 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.106.00   Driver Version: 460.106.00   CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-DGXS...  Off  | 00000000:07:00.0 Off |                    0 |\n",
      "| N/A   38C    P0    52W / 300W |      0MiB / 32508MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-DGXS...  Off  | 00000000:08:00.0 Off |                    0 |\n",
      "| N/A   37C    P0    51W / 300W |      0MiB / 32508MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla V100-DGXS...  Off  | 00000000:0E:00.0 Off |                    0 |\n",
      "| N/A   37C    P0    52W / 300W |      0MiB / 32508MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla V100-DGXS...  Off  | 00000000:0F:00.0 Off |                    0 |\n",
      "| N/A   38C    P0    53W / 300W |      0MiB / 32508MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86add81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fb10a5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-13 10:15:36.194761: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-13 10:15:36.359649: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 1547661802082346120\n",
      "xla_global_id: -1\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 32476168192\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 12804516843052416027\n",
      "physical_device_desc: \"device: 0, name: Tesla V100-DGXS-32GB, pci bus id: 0000:0f:00.0, compute capability: 7.0\"\n",
      "xla_global_id: 416903419\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-13 10:15:39.467694: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-13 10:15:43.598307: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /device:GPU:0 with 30971 MB memory:  -> device: 0, name: Tesla V100-DGXS-32GB, pci bus id: 0000:0f:00.0, compute capability: 7.0\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8be3b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import gdown\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import tqdm.notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6e98a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "aspect = ['biological','process']\n",
    "aspect_abbr = 'bp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ce40988",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = '_'.join(aspect)+'.csv'\n",
    "if not os.path.exists(file):\n",
    "    url = \"https://drive.google.com/file/d/1RyeLQPFTMWAIr-OzELTWIx60ln-mZ7g_/view?usp=sharing\"\n",
    "    output = file\n",
    "    gdown.download(url=url, output=output, quiet=False, fuzzy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd949fb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Entry</th>\n",
       "      <th>Entry Name</th>\n",
       "      <th>Organism</th>\n",
       "      <th>Length</th>\n",
       "      <th>Sequence</th>\n",
       "      <th>Gene Ontology (biological process)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>F4KFS5</td>\n",
       "      <td>GMI1_ARATH</td>\n",
       "      <td>Arabidopsis thaliana (Mouse-ear cress)</td>\n",
       "      <td>1598</td>\n",
       "      <td>MSSRRSVKRSLVLDDDDDEDIFYNFKVLLPNGTSVKLTLKNPEPEI...</td>\n",
       "      <td>GO:0000724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B4ESY8</td>\n",
       "      <td>URK_PROMH</td>\n",
       "      <td>Proteus mirabilis (strain HI4320)</td>\n",
       "      <td>213</td>\n",
       "      <td>MADTAHQCTIVGIAGASASGKSLIASTLYRELRAQVGDHNIGVIPE...</td>\n",
       "      <td>GO:0016310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Q9H479</td>\n",
       "      <td>FN3K_HUMAN</td>\n",
       "      <td>Homo sapiens (Human)</td>\n",
       "      <td>309</td>\n",
       "      <td>MEQLLRAELRTATLRAFGGPGAGCISEGRAYDTDAGPVFVKVNRRT...</td>\n",
       "      <td>GO:0016310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B4PRE2</td>\n",
       "      <td>DGKH_DROYA</td>\n",
       "      <td>Drosophila yakuba (Fruit fly)</td>\n",
       "      <td>1917</td>\n",
       "      <td>MSHLKLDTLHVQRSPRGSRRSSRSSGRSSACSSGSISPVPIIPIIS...</td>\n",
       "      <td>GO:0016310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>P17129</td>\n",
       "      <td>PSPB_CANLF</td>\n",
       "      <td>Canis lupus familiaris (Dog) (Canis familiaris)</td>\n",
       "      <td>363</td>\n",
       "      <td>LLWLLLLPTLCGLGAADWSAPSLACARGPAFWCQSLEQALQCRALG...</td>\n",
       "      <td>GO:0006629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72465</th>\n",
       "      <td>P44371</td>\n",
       "      <td>RS2_HAEIN</td>\n",
       "      <td>Haemophilus influenzae (strain ATCC 51907 / DS...</td>\n",
       "      <td>240</td>\n",
       "      <td>MAQVSMRDMINAGVHFGHQTRYWNPQMKPFIFGARNGVHIINLEKT...</td>\n",
       "      <td>GO:0006412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72466</th>\n",
       "      <td>A1V880</td>\n",
       "      <td>RS13_BURMS</td>\n",
       "      <td>Burkholderia mallei (strain SAVP1)</td>\n",
       "      <td>121</td>\n",
       "      <td>MARIAGVNIPNHQHTEIGLTAIFGIGRTRARSICVASGVAFSKKVK...</td>\n",
       "      <td>GO:0006412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72467</th>\n",
       "      <td>Q04955</td>\n",
       "      <td>FLIG_CAUVC</td>\n",
       "      <td>Caulobacter vibrioides (strain ATCC 19089 / CB...</td>\n",
       "      <td>340</td>\n",
       "      <td>MAMKLAVNDVKNLSGPEKAAIVLLALGEDHTRIWEALDDEEIKEVS...</td>\n",
       "      <td>GO:0006935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72468</th>\n",
       "      <td>B5R0P3</td>\n",
       "      <td>PURA_SALEP</td>\n",
       "      <td>Salmonella enteritidis PT4 (strain P125109)</td>\n",
       "      <td>432</td>\n",
       "      <td>MGNNVVVLGTQWGDEGKGKIVDLLTERAKYVVRYQGGHNAGHTLVI...</td>\n",
       "      <td>GO:0044208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72469</th>\n",
       "      <td>Q80W96</td>\n",
       "      <td>ISCA1_RAT</td>\n",
       "      <td>Rattus norvegicus (Rat)</td>\n",
       "      <td>129</td>\n",
       "      <td>MSASLVRATVRAVSKRKLQPTRAALTLTPSAVNKIKQLLKDKPEHV...</td>\n",
       "      <td>GO:0016226</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>72470 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Entry  Entry Name                                           Organism  \\\n",
       "0      F4KFS5  GMI1_ARATH             Arabidopsis thaliana (Mouse-ear cress)   \n",
       "1      B4ESY8   URK_PROMH                  Proteus mirabilis (strain HI4320)   \n",
       "2      Q9H479  FN3K_HUMAN                               Homo sapiens (Human)   \n",
       "3      B4PRE2  DGKH_DROYA                      Drosophila yakuba (Fruit fly)   \n",
       "4      P17129  PSPB_CANLF    Canis lupus familiaris (Dog) (Canis familiaris)   \n",
       "...       ...         ...                                                ...   \n",
       "72465  P44371   RS2_HAEIN  Haemophilus influenzae (strain ATCC 51907 / DS...   \n",
       "72466  A1V880  RS13_BURMS                 Burkholderia mallei (strain SAVP1)   \n",
       "72467  Q04955  FLIG_CAUVC  Caulobacter vibrioides (strain ATCC 19089 / CB...   \n",
       "72468  B5R0P3  PURA_SALEP        Salmonella enteritidis PT4 (strain P125109)   \n",
       "72469  Q80W96   ISCA1_RAT                            Rattus norvegicus (Rat)   \n",
       "\n",
       "       Length                                           Sequence  \\\n",
       "0        1598  MSSRRSVKRSLVLDDDDDEDIFYNFKVLLPNGTSVKLTLKNPEPEI...   \n",
       "1         213  MADTAHQCTIVGIAGASASGKSLIASTLYRELRAQVGDHNIGVIPE...   \n",
       "2         309  MEQLLRAELRTATLRAFGGPGAGCISEGRAYDTDAGPVFVKVNRRT...   \n",
       "3        1917  MSHLKLDTLHVQRSPRGSRRSSRSSGRSSACSSGSISPVPIIPIIS...   \n",
       "4         363  LLWLLLLPTLCGLGAADWSAPSLACARGPAFWCQSLEQALQCRALG...   \n",
       "...       ...                                                ...   \n",
       "72465     240  MAQVSMRDMINAGVHFGHQTRYWNPQMKPFIFGARNGVHIINLEKT...   \n",
       "72466     121  MARIAGVNIPNHQHTEIGLTAIFGIGRTRARSICVASGVAFSKKVK...   \n",
       "72467     340  MAMKLAVNDVKNLSGPEKAAIVLLALGEDHTRIWEALDDEEIKEVS...   \n",
       "72468     432  MGNNVVVLGTQWGDEGKGKIVDLLTERAKYVVRYQGGHNAGHTLVI...   \n",
       "72469     129  MSASLVRATVRAVSKRKLQPTRAALTLTPSAVNKIKQLLKDKPEHV...   \n",
       "\n",
       "      Gene Ontology (biological process)  \n",
       "0                             GO:0000724  \n",
       "1                             GO:0016310  \n",
       "2                             GO:0016310  \n",
       "3                             GO:0016310  \n",
       "4                             GO:0006629  \n",
       "...                                  ...  \n",
       "72465                         GO:0006412  \n",
       "72466                         GO:0006412  \n",
       "72467                         GO:0006935  \n",
       "72468                         GO:0044208  \n",
       "72469                         GO:0016226  \n",
       "\n",
       "[72470 rows x 6 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(file)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "724323ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201\n",
      "['GO:0000027', 'GO:0000103', 'GO:0000105', 'GO:0000122', 'GO:0000162', 'GO:0000209', 'GO:0000398', 'GO:0000724', 'GO:0000902', 'GO:0000917', 'GO:0001525', 'GO:0001934', 'GO:0002098', 'GO:0002181', 'GO:0002250', 'GO:0002949', 'GO:0005975', 'GO:0005978', 'GO:0006006', 'GO:0006094', 'GO:0006096', 'GO:0006099', 'GO:0006164', 'GO:0006166', 'GO:0006189', 'GO:0006207', 'GO:0006226', 'GO:0006235', 'GO:0006241', 'GO:0006260', 'GO:0006275', 'GO:0006281', 'GO:0006284', 'GO:0006289', 'GO:0006298', 'GO:0006310', 'GO:0006325', 'GO:0006338', 'GO:0006351', 'GO:0006353', 'GO:0006355', 'GO:0006357', 'GO:0006364', 'GO:0006366', 'GO:0006396', 'GO:0006397', 'GO:0006400', 'GO:0006401', 'GO:0006402', 'GO:0006412', 'GO:0006417', 'GO:0006420', 'GO:0006424', 'GO:0006426', 'GO:0006432', 'GO:0006433', 'GO:0006457', 'GO:0006468', 'GO:0006470', 'GO:0006508', 'GO:0006511', 'GO:0006520', 'GO:0006526', 'GO:0006541', 'GO:0006556', 'GO:0006605', 'GO:0006629', 'GO:0006633', 'GO:0006730', 'GO:0006744', 'GO:0006777', 'GO:0006782', 'GO:0006807', 'GO:0006811', 'GO:0006814', 'GO:0006886', 'GO:0006897', 'GO:0006914', 'GO:0006915', 'GO:0006935', 'GO:0006952', 'GO:0006954', 'GO:0006955', 'GO:0006974', 'GO:0006979', 'GO:0007049', 'GO:0007059', 'GO:0007155', 'GO:0007165', 'GO:0007186', 'GO:0007283', 'GO:0007399', 'GO:0007420', 'GO:0007507', 'GO:0008033', 'GO:0008284', 'GO:0008285', 'GO:0008360', 'GO:0008380', 'GO:0008615', 'GO:0008616', 'GO:0008652', 'GO:0008654', 'GO:0009073', 'GO:0009086', 'GO:0009089', 'GO:0009097', 'GO:0009098', 'GO:0009099', 'GO:0009102', 'GO:0009103', 'GO:0009117', 'GO:0009228', 'GO:0009229', 'GO:0009231', 'GO:0009234', 'GO:0009236', 'GO:0009245', 'GO:0009249', 'GO:0009252', 'GO:0009408', 'GO:0009410', 'GO:0009423', 'GO:0009432', 'GO:0009435', 'GO:0010468', 'GO:0010628', 'GO:0010629', 'GO:0015031', 'GO:0015937', 'GO:0015940', 'GO:0015979', 'GO:0016042', 'GO:0016055', 'GO:0016114', 'GO:0016192', 'GO:0016226', 'GO:0016260', 'GO:0016310', 'GO:0016311', 'GO:0016477', 'GO:0016567', 'GO:0017038', 'GO:0018105', 'GO:0019062', 'GO:0019253', 'GO:0019264', 'GO:0019284', 'GO:0019288', 'GO:0019464', 'GO:0019509', 'GO:0019684', 'GO:0019877', 'GO:0022904', 'GO:0030091', 'GO:0030154', 'GO:0030163', 'GO:0030435', 'GO:0030490', 'GO:0031119', 'GO:0032259', 'GO:0035556', 'GO:0035999', 'GO:0042026', 'GO:0042254', 'GO:0042274', 'GO:0042450', 'GO:0042742', 'GO:0042773', 'GO:0042777', 'GO:0043065', 'GO:0043066', 'GO:0043093', 'GO:0043161', 'GO:0043419', 'GO:0044205', 'GO:0044208', 'GO:0044209', 'GO:0044210', 'GO:0045087', 'GO:0045727', 'GO:0045892', 'GO:0045893', 'GO:0045944', 'GO:0046677', 'GO:0046718', 'GO:0046777', 'GO:0050821', 'GO:0051301', 'GO:0051321', 'GO:0051607', 'GO:0051726', 'GO:0055085', 'GO:0055129', 'GO:0060271', 'GO:0065002', 'GO:0070475', 'GO:0070814', 'GO:0071555', 'GO:0097056', 'GO:2001295']\n"
     ]
    }
   ],
   "source": [
    "go_terms_bp = set()\n",
    "for idx, row in df.iterrows():\n",
    "    for term in row['Gene Ontology ('+' '.join(aspect)+')'].split(';'):\n",
    "        go_terms_bp.add(term)\n",
    "go_terms_bp = list(go_terms_bp)\n",
    "go_terms_bp.sort()\n",
    "print(len(go_terms_bp))\n",
    "print(go_terms_bp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72f8a1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_segments(sequence,segment_size=100,gap=30):\n",
    "    segments = []\n",
    "    start = 0\n",
    "    end = segment_size\n",
    "    while end <= len(sequence):\n",
    "        segments.append(sequence[start:end])\n",
    "        start += gap\n",
    "        end += gap\n",
    "    last_segment = sequence[start:]\n",
    "    segments.append(last_segment)\n",
    "    \n",
    "    return segments\n",
    "\n",
    "def get_training_data(df,segment_size=100,gap=30):\n",
    "    training_data = list()\n",
    "    for idx,row in tqdm.tqdm(df.iterrows()):\n",
    "        labels = [0] * len(go_terms_bp)\n",
    "        for term in row['Gene Ontology ('+' '.join(aspect)+')'].split(';'):\n",
    "            labels[go_terms_bp.index(term)] = 1\n",
    "        segments = get_segments(row['Sequence'],segment_size,gap)\n",
    "        for segment in segments:\n",
    "            training_data.append([row['Entry'],segment,labels])\n",
    "    return training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f1d6c8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d48bd463c1dc409c9c9c2bd93f759d2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "794486\n"
     ]
    }
   ],
   "source": [
    "training_data = get_training_data(df)\n",
    "print(len(training_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c0991f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngrams(segment,n=3):\n",
    "    ngrams = []\n",
    "    for i in range(len(segment)-n+1):\n",
    "        ngrams.append(segment[i:i+n])\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5358736d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading saved ngrams...\n",
      "794486\n"
     ]
    }
   ],
   "source": [
    "# Generate training data of ngrams\n",
    "if os.path.exists('bp/training_data_4grams.npy'):\n",
    "    print('Loading saved ngrams...')\n",
    "    training_data_ngrams = np.load('bp/training_data_4grams.npy',allow_pickle=True)\n",
    "else:\n",
    "    print('Preparing from scratch...')\n",
    "    training_data_ngrams = []\n",
    "\n",
    "    for i in tqdm.tqdm(range(len(training_data))):\n",
    "        training_data_ngrams.append([training_data[i][0],get_ngrams(training_data[i][1],n=4),training_data[i][2]])\n",
    "        \n",
    "    np.save('bp/training_data_4grams.npy',training_data_ngrams)\n",
    "    \n",
    "print(len(training_data_ngrams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4acd14a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_skip_grams(segment,skip=1,n=3):\n",
    "    skip_grams = []\n",
    "    window_size = skip + n\n",
    "    for i in range(len(segment)-window_size+1):\n",
    "        window = segment[i:i+window_size]\n",
    "        indices = list(range(window_size))\n",
    "        indices.pop(0)\n",
    "        for idx in indices[::-1]:\n",
    "            temp = ''\n",
    "            for j in range(window_size):\n",
    "                if j!=idx:\n",
    "                    temp+=window[j]\n",
    "            skip_grams.append(temp)\n",
    "\n",
    "    return skip_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fa91e8e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading saved skip grams...\n",
      "794486\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists('bp/training_data_skip1_4grams.npy'):\n",
    "    print('Loading saved skip grams...')\n",
    "    training_data_skip_grams = np.load('bp/training_data_skip1_4grams.npy',allow_pickle=True)\n",
    "else:\n",
    "    print('Preparing from scratch...')\n",
    "    training_data_skip_grams = []\n",
    "    for i in tqdm.tqdm(range(len(training_data))):\n",
    "        training_data_skip_grams.append([training_data[i][0],get_skip_grams(training_data[i][1],n=4),training_data[i][2]])\n",
    "    np.save('bp/training_data_skip1_4grams.npy',training_data_skip_grams)\n",
    "print(len(training_data_skip_grams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "11d856c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0e391f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assuming training_data as global variable\n",
    "\n",
    "def train_test_split(X,y,fold_no,prev_index,Kfolds=5):\n",
    "    test_split = 1/Kfolds\n",
    "    \n",
    "    start_index = prev_index\n",
    "    end_index = (fold_no + 1) * (test_split) * len(X)\n",
    "    end_index = round(end_index)\n",
    "    \n",
    "    if end_index==len(X):\n",
    "        end_index -= 1\n",
    "    \n",
    "    entry = training_data[end_index][0]\n",
    "    entries = [sample[0] for sample in training_data]\n",
    "    \n",
    "    first_occurence = entries.index(entry)\n",
    "    entries.reverse()\n",
    "    \n",
    "    last_occurence = entries.index(entry)\n",
    "    last_occurence = len(entries) - last_occurence - 1\n",
    "    \n",
    "    del entries\n",
    "    gc.collect()\n",
    "    \n",
    "    end_index = first_occurence if (abs(end_index-first_occurence) < abs(end_index-last_occurence)) else last_occurence\n",
    "    \n",
    "    X_test = X[start_index:end_index+1]\n",
    "    y_test = y[start_index:end_index+1]\n",
    "    X_train = X[:start_index]\n",
    "    X_train.extend(X[end_index+1:])\n",
    "    y_train = y[:start_index]\n",
    "    y_train.extend(y[end_index+1:])\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test, start_index, end_index + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "730f9429",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_WORDS = 331776\n",
    "MAX_LEN_NG = 97 #100\n",
    "MAX_LEN_SG = 384 #300\n",
    "\n",
    "def tokenization(X_train,X_test,maxlen):\n",
    "\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=MAX_WORDS)\n",
    "    tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "    X_train = tokenizer.texts_to_sequences(X_train)\n",
    "    X_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "    X_train = tf.keras.preprocessing.sequence.pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
    "    X_test = tf.keras.preprocessing.sequence.pad_sequences(X_test, padding='post', maxlen=maxlen)\n",
    "    \n",
    "    return X_train, X_test, vocab_size, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "10a7f090",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "\n",
    "false_negative_penalty = 6\n",
    "false_positive_penalty = 1\n",
    "\n",
    "def custom_loss(y_true, y_logit):\n",
    "\n",
    "    loss = float(0)\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_logit = tf.cast(y_logit, tf.float32)\n",
    "    \n",
    "    first_term = false_negative_penalty * float(y_true) * - K.log(y_logit + K.epsilon())\n",
    "    second_term = false_positive_penalty * (1 - float(y_true)) * - K.log(1 - y_logit + K.epsilon())\n",
    "    \n",
    "    loss = K.mean(first_term+second_term)\n",
    "\n",
    "    return loss\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    \n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)), axis=1)\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)), axis=1)\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    \n",
    "    return K.mean(precision)\n",
    "\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)), axis=1)\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)), axis=1)\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return K.mean(recall)\n",
    "\n",
    "def f1_score(y_true, y_pred):\n",
    "    rec = recall(y_true,y_pred)\n",
    "    prec = precision(y_true,y_pred)\n",
    "    f1 = 2*prec*rec/(prec+rec)\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "36c30cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class attention(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, return_sequences=True,**kwargs):\n",
    "        self.return_sequences = return_sequences\n",
    "        super(attention,self).__init__()\n",
    "\n",
    "    def get_config(self):\n",
    "      config = super().get_config().copy()\n",
    "      config.update({\n",
    "          'return_sequences': self.return_sequences \n",
    "      })\n",
    "      return config\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        \n",
    "        self.W=self.add_weight(name=\"att_weight\", shape=(input_shape[-1],1),\n",
    "                               initializer=\"normal\")\n",
    "        self.b=self.add_weight(name=\"att_bias\", shape=(input_shape[1],1),\n",
    "                               initializer=\"zeros\")\n",
    "        \n",
    "        super(attention,self).build(input_shape)\n",
    "        \n",
    "    def call(self, x):\n",
    "        \n",
    "        e = K.tanh(K.dot(x,self.W)+self.b)\n",
    "        a = K.softmax(e, axis=1)\n",
    "        output = x*a\n",
    "        \n",
    "        if self.return_sequences:\n",
    "            return output\n",
    "        \n",
    "        return K.sum(output, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b6dbee46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Base model using ngrams \n",
    "NUM_CLASSES = 201\n",
    "\n",
    "def get_model_ng(vocab_size_ng):\n",
    "    input_ngrams = tf.keras.layers.Input(shape=(MAX_LEN_NG,))\n",
    "\n",
    "    embedding_layer = tf.keras.layers.Embedding(vocab_size_ng, 32)(input_ngrams)\n",
    "\n",
    "    LSTM_Layer_1 = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(70,return_sequences=True))(embedding_layer)\n",
    "    attention_output_1 = attention(return_sequences=False)(LSTM_Layer_1)\n",
    "    dropout = tf.keras.layers.Dropout(0.3)(attention_output_1)\n",
    "    dense_layer_1 = tf.keras.layers.Dense(NUM_CLASSES, activation='sigmoid')(dropout)\n",
    "\n",
    "    model = tf.keras.models.Model(\n",
    "        inputs=input_ngrams, \n",
    "        outputs=dense_layer_1\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "014f93a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Base model using skip grams \n",
    "NUM_CLASSES = 201\n",
    "\n",
    "def get_model_sg(vocab_size_sg):\n",
    "    input_skip_grams = tf.keras.layers.Input(shape=(MAX_LEN_SG,))\n",
    "\n",
    "    embedding_layer = tf.keras.layers.Embedding(vocab_size_sg, 32)(input_skip_grams)\n",
    "\n",
    "    LSTM_Layer_1 = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(70,return_sequences=True))(embedding_layer)\n",
    "    attention_output_1 = attention(return_sequences=False)(LSTM_Layer_1)\n",
    "    dropout = tf.keras.layers.Dropout(0.3)(attention_output_1)\n",
    "    dense_layer_1 = tf.keras.layers.Dense(NUM_CLASSES, activation='sigmoid')(dropout)\n",
    "\n",
    "    model = tf.keras.models.Model(\n",
    "        inputs=input_skip_grams, \n",
    "        outputs=dense_layer_1\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "03b912ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 201 #For bp (Change according to aspects)\n",
    "\n",
    "def get_model_ng_sg(vocab_size_ng, vocab_size_sg):\n",
    "    #Input layers\n",
    "\n",
    "    input_ngrams = tf.keras.layers.Input(shape=(MAX_LEN_NG,)) \n",
    "    input_skip_grams = tf.keras.layers.Input(shape=(MAX_LEN_SG,)) \n",
    "\n",
    "    #embeddings\n",
    "    embedding_layer_ngrams = tf.keras.layers.Embedding(vocab_size_ng, 32)(input_ngrams)\n",
    "    embedding_layer_skip_grams = tf.keras.layers.Embedding(vocab_size_sg, 32)(input_skip_grams)\n",
    "\n",
    "    #BI-LSTMs for each of the inputs\n",
    "    sequence_output_1 = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(70,return_sequences=True))(embedding_layer_ngrams)\n",
    "    attention_output_1 = attention(return_sequences=False)(sequence_output_1)\n",
    "    dropout_1 = tf.keras.layers.Dropout(0.3)(attention_output_1)\n",
    "    dense_layer_1 = tf.keras.layers.Dense(NUM_CLASSES, activation='sigmoid')(dropout_1)\n",
    "\n",
    "    sequence_output_2 = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(70, return_sequences=True))(embedding_layer_skip_grams)\n",
    "    attention_output_2 = attention(return_sequences=False)(sequence_output_2)\n",
    "    dropout_2 = tf.keras.layers.Dropout(0.3)(attention_output_2)\n",
    "    dense_layer_2 = tf.keras.layers.Dense(NUM_CLASSES, activation='sigmoid')(dropout_2)\n",
    "\n",
    "    max_layer = tf.keras.layers.Maximum()([dense_layer_1,dense_layer_2])\n",
    "\n",
    "    model = tf.keras.models.Model(\n",
    "        inputs=[\n",
    "            input_ngrams,\n",
    "            input_skip_grams\n",
    "        ], \n",
    "        outputs=max_layer)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4236670c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(predictions, start_index, end_index):\n",
    "    final_predictions = []\n",
    "    actual_y_test = []\n",
    "\n",
    "    current_entry = ''\n",
    "    counter = 0\n",
    "    total_counts = 0\n",
    "\n",
    "    if len(predictions) == len(training_data[start_index: end_index]):\n",
    "        temp = np.zeros(NUM_CLASSES)\n",
    "        for i in range(len(predictions)):\n",
    "            if current_entry != training_data[start_index+i][0]:\n",
    "                #compute prev\n",
    "                if i!=0:\n",
    "                    temp /= counter\n",
    "                    final_predictions.append(temp)\n",
    "\n",
    "                #reset\n",
    "                total_counts += counter\n",
    "                counter = 1\n",
    "                temp = np.zeros(NUM_CLASSES)\n",
    "\n",
    "                #init new\n",
    "                current_entry = training_data[start_index+i][0]\n",
    "                temp += np.array(predictions[i])\n",
    "                actual_y_test.append(training_data[start_index+i][2])\n",
    "            else:\n",
    "                temp += np.array(predictions[i])\n",
    "                counter += 1\n",
    "\n",
    "        total_counts += counter\n",
    "        temp /= counter\n",
    "        final_predictions.append(temp)\n",
    "\n",
    "    else:\n",
    "        print('Lengths of predictions dont match with test data')\n",
    "    \n",
    "    final_predictions = np.array(final_predictions, dtype=float)\n",
    "    actual_y_test = np.array(actual_y_test, dtype=float)\n",
    "    \n",
    "    rec = recall(actual_y_test,final_predictions)\n",
    "    prec = precision(actual_y_test,final_predictions)\n",
    "    f1 = f1_score(actual_y_test,final_predictions)\n",
    "    \n",
    "    return rec,prec,f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1454935",
   "metadata": {},
   "source": [
    "### Model using ngrams only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "894bcfa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Considering ngrams\n",
    "X_ng = [' '.join(sample[1]) for sample in training_data_ngrams]\n",
    "y = [sample[2] for sample in training_data]\n",
    "\n",
    "del training_data_ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f6664241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "****Fold: 1 ****\n",
      "Splitting into train-test...\n",
      "Tokenizing...\n",
      "Shuffling...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-13 10:24:53.398451: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30971 MB memory:  -> device: 0, name: Tesla V100-DGXS-32GB, pci bus id: 0000:0f:00.0, compute capability: 7.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-13 10:24:59.727594: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15890/15890 [==============================] - 187s 11ms/step - loss: 0.1366 - recall: 0.1903 - precision: 0.1338 - f1_score: nan - val_loss: 0.0931 - val_recall: 0.4157 - val_precision: 0.3302 - val_f1_score: 0.3670\n",
      "Epoch 2/5\n",
      "15890/15890 [==============================] - 183s 11ms/step - loss: 0.0757 - recall: 0.5431 - precision: 0.4312 - f1_score: 0.4795 - val_loss: 0.0633 - val_recall: 0.6210 - val_precision: 0.5447 - val_f1_score: 0.5797\n",
      "Epoch 3/5\n",
      "15890/15890 [==============================] - 182s 11ms/step - loss: 0.0488 - recall: 0.7272 - precision: 0.6012 - f1_score: 0.6573 - val_loss: 0.0487 - val_recall: 0.7245 - val_precision: 0.6403 - val_f1_score: 0.6792\n",
      "Epoch 4/5\n",
      "15890/15890 [==============================] - 182s 11ms/step - loss: 0.0339 - recall: 0.8321 - precision: 0.6971 - f1_score: 0.7578 - val_loss: 0.0415 - val_recall: 0.7749 - val_precision: 0.6977 - val_f1_score: 0.7337\n",
      "Epoch 5/5\n",
      "15890/15890 [==============================] - 182s 11ms/step - loss: 0.0252 - recall: 0.8895 - precision: 0.7611 - f1_score: 0.8196 - val_loss: 0.0369 - val_recall: 0.8131 - val_precision: 0.7308 - val_f1_score: 0.7692\n",
      "\n",
      "Evaluating model...\n",
      "4966/4966 [==============================] - 20s 4ms/step\n",
      "\n",
      "\n",
      "****Fold: 2 ****\n",
      "Splitting into train-test...\n",
      "Tokenizing...\n",
      "Shuffling...\n",
      "Training...\n",
      "Epoch 1/5\n",
      "15890/15890 [==============================] - 187s 12ms/step - loss: 0.1394 - recall: 0.1767 - precision: 0.1301 - f1_score: nan - val_loss: 0.0958 - val_recall: 0.3989 - val_precision: 0.3247 - val_f1_score: 0.3570\n",
      "Epoch 2/5\n",
      "15890/15890 [==============================] - 182s 11ms/step - loss: 0.0759 - recall: 0.5435 - precision: 0.4358 - f1_score: 0.4826 - val_loss: 0.0632 - val_recall: 0.6165 - val_precision: 0.5453 - val_f1_score: 0.5780\n",
      "Epoch 3/5\n",
      "15890/15890 [==============================] - 182s 11ms/step - loss: 0.0484 - recall: 0.7321 - precision: 0.6007 - f1_score: 0.6589 - val_loss: 0.0489 - val_recall: 0.7267 - val_precision: 0.6391 - val_f1_score: 0.6794\n",
      "Epoch 4/5\n",
      "15890/15890 [==============================] - 182s 11ms/step - loss: 0.0336 - recall: 0.8369 - precision: 0.6979 - f1_score: 0.7603 - val_loss: 0.0415 - val_recall: 0.7798 - val_precision: 0.6963 - val_f1_score: 0.7351\n",
      "Epoch 5/5\n",
      "15890/15890 [==============================] - 182s 11ms/step - loss: 0.0250 - recall: 0.8930 - precision: 0.7626 - f1_score: 0.8219 - val_loss: 0.0391 - val_recall: 0.8029 - val_precision: 0.7319 - val_f1_score: 0.7652\n",
      "\n",
      "Evaluating model...\n",
      "4966/4966 [==============================] - 20s 4ms/step\n",
      "\n",
      "\n",
      "****Fold: 3 ****\n",
      "Splitting into train-test...\n",
      "Tokenizing...\n",
      "Shuffling...\n",
      "Training...\n",
      "Epoch 1/5\n",
      "15890/15890 [==============================] - 186s 12ms/step - loss: 0.1396 - recall: 0.1699 - precision: 0.1210 - f1_score: nan - val_loss: 0.0948 - val_recall: 0.3974 - val_precision: 0.3243 - val_f1_score: 0.3562\n",
      "Epoch 2/5\n",
      "15890/15890 [==============================] - 182s 11ms/step - loss: 0.0760 - recall: 0.5426 - precision: 0.4315 - f1_score: 0.4795 - val_loss: 0.0630 - val_recall: 0.6037 - val_precision: 0.5431 - val_f1_score: 0.5712\n",
      "Epoch 3/5\n",
      "15890/15890 [==============================] - 183s 12ms/step - loss: 0.0495 - recall: 0.7227 - precision: 0.5935 - f1_score: 0.6508 - val_loss: 0.0498 - val_recall: 0.7099 - val_precision: 0.6326 - val_f1_score: 0.6684\n",
      "Epoch 4/5\n",
      "15890/15890 [==============================] - 182s 11ms/step - loss: 0.0355 - recall: 0.8217 - precision: 0.6835 - f1_score: 0.7454 - val_loss: 0.0441 - val_recall: 0.7502 - val_precision: 0.6863 - val_f1_score: 0.7163\n",
      "Epoch 5/5\n",
      "15890/15890 [==============================] - 181s 11ms/step - loss: 0.0270 - recall: 0.8795 - precision: 0.7459 - f1_score: 0.8064 - val_loss: 0.0382 - val_recall: 0.8111 - val_precision: 0.7123 - val_f1_score: 0.7579\n",
      "\n",
      "Evaluating model...\n",
      "4966/4966 [==============================] - 20s 4ms/step\n",
      "\n",
      "\n",
      "****Fold: 4 ****\n",
      "Splitting into train-test...\n",
      "Tokenizing...\n",
      "Shuffling...\n",
      "Training...\n",
      "Epoch 1/5\n",
      "15890/15890 [==============================] - 186s 11ms/step - loss: 0.1388 - recall: 0.1743 - precision: 0.1258 - f1_score: nan - val_loss: 0.0964 - val_recall: 0.3910 - val_precision: 0.3102 - val_f1_score: 0.3449\n",
      "Epoch 2/5\n",
      "15890/15890 [==============================] - 182s 11ms/step - loss: 0.0754 - recall: 0.5464 - precision: 0.4374 - f1_score: 0.4846 - val_loss: 0.0619 - val_recall: 0.6427 - val_precision: 0.5452 - val_f1_score: 0.5892\n",
      "Epoch 3/5\n",
      "15890/15890 [==============================] - 182s 11ms/step - loss: 0.0469 - recall: 0.7417 - precision: 0.6139 - f1_score: 0.6709 - val_loss: 0.0475 - val_recall: 0.7450 - val_precision: 0.6368 - val_f1_score: 0.6860\n",
      "Epoch 4/5\n",
      "15890/15890 [==============================] - 182s 11ms/step - loss: 0.0325 - recall: 0.8435 - precision: 0.7078 - f1_score: 0.7689 - val_loss: 0.0406 - val_recall: 0.7818 - val_precision: 0.6992 - val_f1_score: 0.7376\n",
      "Epoch 5/5\n",
      "15890/15890 [==============================] - 181s 11ms/step - loss: 0.0243 - recall: 0.8969 - precision: 0.7699 - f1_score: 0.8279 - val_loss: 0.0382 - val_recall: 0.8061 - val_precision: 0.7371 - val_f1_score: 0.7696\n",
      "\n",
      "Evaluating model...\n",
      "4966/4966 [==============================] - 20s 4ms/step\n",
      "\n",
      "\n",
      "****Fold: 5 ****\n",
      "Splitting into train-test...\n",
      "Tokenizing...\n",
      "Shuffling...\n",
      "Training...\n",
      "Epoch 1/5\n",
      "15890/15890 [==============================] - 186s 11ms/step - loss: 0.1357 - recall: 0.1965 - precision: 0.1492 - f1_score: nan - val_loss: 0.0899 - val_recall: 0.4332 - val_precision: 0.3620 - val_f1_score: 0.3935\n",
      "Epoch 2/5\n",
      "15890/15890 [==============================] - 182s 11ms/step - loss: 0.0729 - recall: 0.5637 - precision: 0.4527 - f1_score: 0.5010 - val_loss: 0.0615 - val_recall: 0.6257 - val_precision: 0.5471 - val_f1_score: 0.5831\n",
      "Epoch 3/5\n",
      "15890/15890 [==============================] - 181s 11ms/step - loss: 0.0480 - recall: 0.7316 - precision: 0.6038 - f1_score: 0.6607 - val_loss: 0.0485 - val_recall: 0.7135 - val_precision: 0.6417 - val_f1_score: 0.6751\n",
      "Epoch 4/5\n",
      "15890/15890 [==============================] - 181s 11ms/step - loss: 0.0341 - recall: 0.8306 - precision: 0.6953 - f1_score: 0.7561 - val_loss: 0.0418 - val_recall: 0.7652 - val_precision: 0.6990 - val_f1_score: 0.7301\n",
      "Epoch 5/5\n",
      "15890/15890 [==============================] - 181s 11ms/step - loss: 0.0258 - recall: 0.8869 - precision: 0.7576 - f1_score: 0.8164 - val_loss: 0.0393 - val_recall: 0.7879 - val_precision: 0.7320 - val_f1_score: 0.7584\n",
      "\n",
      "Evaluating model...\n",
      "4966/4966 [==============================] - 20s 4ms/step\n",
      "Recall: 0.6332526787540942\n",
      "Precision: 0.6389277432590253\n",
      "F1-Score: 0.6360735705058636\n"
     ]
    }
   ],
   "source": [
    "Kfolds = 5\n",
    "prev_index = 0\n",
    "\n",
    "recs = []\n",
    "precs = []\n",
    "f1s = []\n",
    "\n",
    "for i in range(Kfolds):\n",
    "    print('\\n\\n****Fold:', i+1,'****')\n",
    "    \n",
    "    print('Splitting into train-test...')\n",
    "    X_train_ng, y_train, X_test_ng, y_test, start_index, prev_index = train_test_split(X_ng,y,i,prev_index,Kfolds)\n",
    "\n",
    "    print('Tokenizing...')\n",
    "    X_train_ng, X_test_ng, vocab_size_ng, tokenizer1 = tokenization(X_train_ng, X_test_ng, MAX_LEN_NG)\n",
    "    \n",
    "    print('Shuffling...')\n",
    "    shuffled = [[X_train_ng[i],y_train[i]] for i in range(len(X_train_ng))]\n",
    "    np.random.shuffle(shuffled)\n",
    "\n",
    "    X_train_ng = [shuffled[i][0] for i in range(len(shuffled))]\n",
    "    y_train = [shuffled[i][1] for i in range(len(shuffled))]\n",
    "    X_train_ng = np.array(X_train_ng)\n",
    "    y_train = np.array(y_train)\n",
    "    \n",
    "    \n",
    "    \n",
    "    model = get_model_ng(vocab_size_ng)\n",
    "    \n",
    "    model.compile(\n",
    "        loss=custom_loss, \n",
    "        optimizer='adam', \n",
    "        metrics=[\n",
    "            recall,\n",
    "            precision,\n",
    "            f1_score\n",
    "        ])\n",
    "    \n",
    "    print('Training...')\n",
    "    history = model.fit(X_train_ng, y_train, batch_size=32, epochs=5,validation_split=0.2)\n",
    "    \n",
    "    print('\\nEvaluating model...')\n",
    "    predictions = model.predict(X_test_ng)\n",
    "    \n",
    "    rec, prec, f1 = compute_metrics(predictions, start_index, prev_index)\n",
    "    \n",
    "    recs.append(rec.numpy())\n",
    "    precs.append(prec.numpy())\n",
    "    f1s.append(f1.numpy())\n",
    "\n",
    "print('Recall:',sum(recs)/len(recs))\n",
    "print('Precision:',sum(precs)/len(precs))\n",
    "print('F1-Score:',sum(f1s)/len(f1s))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e28840b",
   "metadata": {},
   "source": [
    "### Model using skip grams only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fbba7e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Considering skip grams\n",
    "X_sg = [' '.join(sample[1]) for sample in training_data_skip_grams]\n",
    "y = [sample[2] for sample in training_data]\n",
    "\n",
    "del training_data_skip_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "33e1124b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "****Fold: 1 ****\n",
      "Splitting into train-test...\n",
      "Tokenizing...\n",
      "Shuffling...\n",
      "Training...\n",
      "Epoch 1/5\n",
      "15890/15890 [==============================] - 518s 32ms/step - loss: 0.1448 - recall: 0.1327 - precision: 0.0933 - f1_score: nan - val_loss: 0.1060 - val_recall: 0.3114 - val_precision: 0.2558 - val_f1_score: 0.2797\n",
      "Epoch 2/5\n",
      "15890/15890 [==============================] - 507s 32ms/step - loss: 0.0878 - recall: 0.4524 - precision: 0.3564 - f1_score: 0.3975 - val_loss: 0.0748 - val_recall: 0.5536 - val_precision: 0.4598 - val_f1_score: 0.5015\n",
      "Epoch 3/5\n",
      "15890/15890 [==============================] - 507s 32ms/step - loss: 0.0600 - recall: 0.6409 - precision: 0.5229 - f1_score: 0.5749 - val_loss: 0.0577 - val_recall: 0.6519 - val_precision: 0.5721 - val_f1_score: 0.6087\n",
      "Epoch 4/5\n",
      "15890/15890 [==============================] - 506s 32ms/step - loss: 0.0430 - recall: 0.7623 - precision: 0.6296 - f1_score: 0.6887 - val_loss: 0.0478 - val_recall: 0.7332 - val_precision: 0.6433 - val_f1_score: 0.6847\n",
      "Epoch 5/5\n",
      "15890/15890 [==============================] - 507s 32ms/step - loss: 0.0325 - recall: 0.8388 - precision: 0.7023 - f1_score: 0.7637 - val_loss: 0.0424 - val_recall: 0.7796 - val_precision: 0.6869 - val_f1_score: 0.7297\n",
      "Evaluating model...\n",
      "4966/4966 [==============================] - 63s 13ms/step\n",
      "\n",
      "\n",
      "****Fold: 2 ****\n",
      "Splitting into train-test...\n",
      "Tokenizing...\n",
      "Shuffling...\n",
      "Training...\n",
      "Epoch 1/5\n",
      "15890/15890 [==============================] - 510s 32ms/step - loss: 0.1483 - recall: 0.1199 - precision: 0.0915 - f1_score: nan - val_loss: 0.1099 - val_recall: 0.2945 - val_precision: 0.2345 - val_f1_score: 0.2599\n",
      "Epoch 2/5\n",
      "15890/15890 [==============================] - 506s 32ms/step - loss: 0.0893 - recall: 0.4449 - precision: 0.3499 - f1_score: 0.3904 - val_loss: 0.0746 - val_recall: 0.5450 - val_precision: 0.4550 - val_f1_score: 0.4950\n",
      "Epoch 3/5\n",
      "15890/15890 [==============================] - 506s 32ms/step - loss: 0.0594 - recall: 0.6467 - precision: 0.5260 - f1_score: 0.5791 - val_loss: 0.0574 - val_recall: 0.6675 - val_precision: 0.5725 - val_f1_score: 0.6156\n",
      "Epoch 4/5\n",
      "15890/15890 [==============================] - 506s 32ms/step - loss: 0.0428 - recall: 0.7652 - precision: 0.6291 - f1_score: 0.6895 - val_loss: 0.0484 - val_recall: 0.7269 - val_precision: 0.6391 - val_f1_score: 0.6795\n",
      "Epoch 5/5\n",
      "15890/15890 [==============================] - 507s 32ms/step - loss: 0.0326 - recall: 0.8394 - precision: 0.7006 - f1_score: 0.7629 - val_loss: 0.0436 - val_recall: 0.7647 - val_precision: 0.6830 - val_f1_score: 0.7209\n",
      "Evaluating model...\n",
      "4966/4966 [==============================] - 64s 13ms/step\n",
      "\n",
      "\n",
      "****Fold: 3 ****\n",
      "Splitting into train-test...\n",
      "Tokenizing...\n",
      "Shuffling...\n",
      "Training...\n",
      "Epoch 1/5\n",
      "15890/15890 [==============================] - 509s 32ms/step - loss: 0.1483 - recall: 0.1190 - precision: 0.0909 - f1_score: nan - val_loss: 0.1079 - val_recall: 0.3142 - val_precision: 0.2440 - val_f1_score: 0.2734\n",
      "Epoch 2/5\n",
      "15890/15890 [==============================] - 506s 32ms/step - loss: 0.0879 - recall: 0.4528 - precision: 0.3536 - f1_score: 0.3958 - val_loss: 0.0724 - val_recall: 0.5413 - val_precision: 0.4739 - val_f1_score: 0.5046\n",
      "Epoch 3/5\n",
      "15890/15890 [==============================] - 506s 32ms/step - loss: 0.0592 - recall: 0.6468 - precision: 0.5264 - f1_score: 0.5794 - val_loss: 0.0573 - val_recall: 0.6363 - val_precision: 0.5787 - val_f1_score: 0.6055\n",
      "Epoch 4/5\n",
      "15890/15890 [==============================] - 506s 32ms/step - loss: 0.0426 - recall: 0.7645 - precision: 0.6297 - f1_score: 0.6896 - val_loss: 0.0481 - val_recall: 0.7175 - val_precision: 0.6418 - val_f1_score: 0.6770\n",
      "Epoch 5/5\n",
      "15890/15890 [==============================] - 506s 32ms/step - loss: 0.0324 - recall: 0.8389 - precision: 0.7009 - f1_score: 0.7629 - val_loss: 0.0422 - val_recall: 0.7739 - val_precision: 0.6851 - val_f1_score: 0.7262\n",
      "Evaluating model...\n",
      "4966/4966 [==============================] - 64s 13ms/step\n",
      "\n",
      "\n",
      "****Fold: 4 ****\n",
      "Splitting into train-test...\n",
      "Tokenizing...\n",
      "Shuffling...\n",
      "Training...\n",
      "Epoch 1/5\n",
      "15890/15890 [==============================] - 511s 32ms/step - loss: 0.1450 - recall: 0.1298 - precision: 0.0967 - f1_score: nan - val_loss: 0.1046 - val_recall: 0.3204 - val_precision: 0.2613 - val_f1_score: 0.2867\n",
      "Epoch 2/5\n",
      "15890/15890 [==============================] - 507s 32ms/step - loss: 0.0860 - recall: 0.4653 - precision: 0.3656 - f1_score: 0.4082 - val_loss: 0.0730 - val_recall: 0.5604 - val_precision: 0.4712 - val_f1_score: 0.5111\n",
      "Epoch 3/5\n",
      "15890/15890 [==============================] - 506s 32ms/step - loss: 0.0586 - recall: 0.6513 - precision: 0.5312 - f1_score: 0.5841 - val_loss: 0.0569 - val_recall: 0.6513 - val_precision: 0.5820 - val_f1_score: 0.6141\n",
      "Epoch 4/5\n",
      "15890/15890 [==============================] - 508s 32ms/step - loss: 0.0422 - recall: 0.7680 - precision: 0.6339 - f1_score: 0.6936 - val_loss: 0.0482 - val_recall: 0.7174 - val_precision: 0.6453 - val_f1_score: 0.6788\n",
      "Epoch 5/5\n",
      "15890/15890 [==============================] - 507s 32ms/step - loss: 0.0318 - recall: 0.8425 - precision: 0.7058 - f1_score: 0.7673 - val_loss: 0.0431 - val_recall: 0.7578 - val_precision: 0.6946 - val_f1_score: 0.7243\n",
      "Evaluating model...\n",
      "4966/4966 [==============================] - 64s 13ms/step\n",
      "\n",
      "\n",
      "****Fold: 5 ****\n",
      "Splitting into train-test...\n",
      "Tokenizing...\n",
      "Shuffling...\n",
      "Training...\n",
      "Epoch 1/5\n",
      "15890/15890 [==============================] - 511s 32ms/step - loss: 0.1454 - recall: 0.1324 - precision: 0.0973 - f1_score: nan - val_loss: 0.1061 - val_recall: 0.3166 - val_precision: 0.2565 - val_f1_score: 0.2823\n",
      "Epoch 2/5\n",
      "15890/15890 [==============================] - 506s 32ms/step - loss: 0.0888 - recall: 0.4439 - precision: 0.3468 - f1_score: 0.3881 - val_loss: 0.0765 - val_recall: 0.5011 - val_precision: 0.4516 - val_f1_score: 0.4744\n",
      "Epoch 3/5\n",
      "15890/15890 [==============================] - 507s 32ms/step - loss: 0.0608 - recall: 0.6345 - precision: 0.5149 - f1_score: 0.5674 - val_loss: 0.0581 - val_recall: 0.6426 - val_precision: 0.5678 - val_f1_score: 0.6022\n",
      "Epoch 4/5\n",
      "15890/15890 [==============================] - 507s 32ms/step - loss: 0.0441 - recall: 0.7528 - precision: 0.6186 - f1_score: 0.6782 - val_loss: 0.0487 - val_recall: 0.7310 - val_precision: 0.6277 - val_f1_score: 0.6747\n",
      "Epoch 5/5\n",
      "15890/15890 [==============================] - 506s 32ms/step - loss: 0.0339 - recall: 0.8269 - precision: 0.6884 - f1_score: 0.7504 - val_loss: 0.0445 - val_recall: 0.7466 - val_precision: 0.6819 - val_f1_score: 0.7122\n",
      "Evaluating model...\n",
      "4966/4966 [==============================] - 64s 13ms/step\n",
      "Recall: 0.6232809821041151\n",
      "Precision: 0.626732935586773\n",
      "F1-Score: 0.6249997212891993\n"
     ]
    }
   ],
   "source": [
    "Kfolds = 5\n",
    "prev_index = 0\n",
    "\n",
    "recs = []\n",
    "precs = []\n",
    "f1s = []\n",
    "\n",
    "for i in range(Kfolds):\n",
    "    print('\\n\\n****Fold:', i+1,'****')\n",
    "    \n",
    "    print('Splitting into train-test...')\n",
    "    X_train_sg, y_train, X_test_sg, y_test, start_index, prev_index = train_test_split(X_sg,y,i,prev_index,Kfolds)\n",
    "\n",
    "    print('Tokenizing...')\n",
    "    X_train_sg, X_test_sg, vocab_size_sg, tokenizer1 = tokenization(X_train_sg, X_test_sg, MAX_LEN_SG)\n",
    "    \n",
    "    print('Shuffling...')\n",
    "    shuffled = [[X_train_sg[i],y_train[i]] for i in range(len(X_train_sg))]\n",
    "    np.random.shuffle(shuffled)\n",
    "\n",
    "    X_train_sg = [shuffled[i][0] for i in range(len(shuffled))]\n",
    "    y_train = [shuffled[i][1] for i in range(len(shuffled))]\n",
    "    X_train_sg = np.array(X_train_sg)\n",
    "    y_train = np.array(y_train)\n",
    "    \n",
    "\n",
    "    model = get_model_sg(vocab_size_sg)\n",
    "    \n",
    "    model.compile(\n",
    "        loss=custom_loss, \n",
    "        optimizer='adam', \n",
    "        metrics=[\n",
    "            recall,\n",
    "            precision,\n",
    "            f1_score\n",
    "        ])\n",
    "    \n",
    "    print('Training...')\n",
    "    history = model.fit(X_train_sg, y_train, batch_size=32, epochs=5,validation_split=0.2)\n",
    "    \n",
    "    print('Evaluating model...')\n",
    "    predictions = model.predict(X_test_sg)\n",
    "    \n",
    "    rec, prec, f1 = compute_metrics(predictions, start_index, prev_index)\n",
    "    \n",
    "    recs.append(rec.numpy())\n",
    "    precs.append(prec.numpy())\n",
    "    f1s.append(f1.numpy())\n",
    "\n",
    "print('Recall:',sum(recs)/len(recs))\n",
    "print('Precision:',sum(precs)/len(precs))\n",
    "print('F1-Score:',sum(f1s)/len(f1s))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84680b09",
   "metadata": {},
   "source": [
    "### Model using skip grams and ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6163d660",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Considering ngrams and skip grams\n",
    "X_ng = [' '.join(sample[1]) for sample in training_data_ngrams]\n",
    "X_sg = [' '.join(sample[1]) for sample in training_data_skip_grams]\n",
    "y = [sample[2] for sample in training_data]\n",
    "\n",
    "del training_data_ngrams\n",
    "del training_data_skip_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "32366243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "794486 794486\n"
     ]
    }
   ],
   "source": [
    "print(len(X_ng),len(X_sg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "eaad7bdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "****Fold: 1 ****\n",
      "Splitting into train-test...\n",
      "Tokenizing...\n",
      "Shuffling...\n",
      "Training...\n",
      "Epoch 1/5\n",
      "15890/15890 [==============================] - 715s 45ms/step - loss: 0.1341 - recall: 0.2067 - precision: 0.1581 - f1_score: nan - val_loss: 0.0903 - val_recall: 0.4502 - val_precision: 0.3569 - val_f1_score: 0.3971\n",
      "Epoch 2/5\n",
      "15890/15890 [==============================] - 709s 45ms/step - loss: 0.0714 - recall: 0.5777 - precision: 0.4650 - f1_score: 0.5141 - val_loss: 0.0607 - val_recall: 0.6320 - val_precision: 0.5702 - val_f1_score: 0.5989\n",
      "Epoch 3/5\n",
      "15890/15890 [==============================] - 708s 45ms/step - loss: 0.0451 - recall: 0.7526 - precision: 0.6320 - f1_score: 0.6862 - val_loss: 0.0469 - val_recall: 0.7370 - val_precision: 0.6601 - val_f1_score: 0.6958\n",
      "Epoch 4/5\n",
      "15890/15890 [==============================] - 708s 45ms/step - loss: 0.0313 - recall: 0.8470 - precision: 0.7284 - f1_score: 0.7825 - val_loss: 0.0405 - val_recall: 0.7892 - val_precision: 0.7158 - val_f1_score: 0.7502\n",
      "Epoch 5/5\n",
      "15890/15890 [==============================] - 708s 45ms/step - loss: 0.0231 - recall: 0.8998 - precision: 0.7916 - f1_score: 0.8417 - val_loss: 0.0367 - val_recall: 0.8116 - val_precision: 0.7601 - val_f1_score: 0.7845\n",
      "Evaluating model...\n",
      "4966/4966 [==============================] - 80s 16ms/step\n",
      "\n",
      "\n",
      "****Fold: 2 ****\n",
      "Splitting into train-test...\n",
      "Tokenizing...\n",
      "Shuffling...\n",
      "Training...\n",
      "Epoch 1/5\n",
      "15890/15890 [==============================] - 701s 44ms/step - loss: 0.1399 - recall: 0.1769 - precision: 0.1316 - f1_score: nan - val_loss: 0.0952 - val_recall: 0.4138 - val_precision: 0.3190 - val_f1_score: 0.3591\n",
      "Epoch 2/5\n",
      "15890/15890 [==============================] - 691s 43ms/step - loss: 0.0731 - recall: 0.5696 - precision: 0.4553 - f1_score: 0.5048 - val_loss: 0.0596 - val_recall: 0.6625 - val_precision: 0.5687 - val_f1_score: 0.6112\n",
      "Epoch 3/5\n",
      "15890/15890 [==============================] - 691s 44ms/step - loss: 0.0433 - recall: 0.7703 - precision: 0.6433 - f1_score: 0.7002 - val_loss: 0.0443 - val_recall: 0.7572 - val_precision: 0.6723 - val_f1_score: 0.7116\n",
      "Epoch 4/5\n",
      "15890/15890 [==============================] - 691s 44ms/step - loss: 0.0284 - recall: 0.8702 - precision: 0.7450 - f1_score: 0.8020 - val_loss: 0.0372 - val_recall: 0.8095 - val_precision: 0.7294 - val_f1_score: 0.7669\n",
      "Epoch 5/5\n",
      "15890/15890 [==============================] - 691s 44ms/step - loss: 0.0202 - recall: 0.9206 - precision: 0.8117 - f1_score: 0.8621 - val_loss: 0.0349 - val_recall: 0.8315 - val_precision: 0.7660 - val_f1_score: 0.7970\n",
      "Evaluating model...\n",
      "4966/4966 [==============================] - 82s 16ms/step\n",
      "\n",
      "\n",
      "****Fold: 3 ****\n",
      "Splitting into train-test...\n",
      "Tokenizing...\n",
      "Shuffling...\n",
      "Training...\n",
      "Epoch 1/5\n",
      "15890/15890 [==============================] - 701s 44ms/step - loss: 0.1355 - recall: 0.2060 - precision: 0.1567 - f1_score: nan - val_loss: 0.0881 - val_recall: 0.4613 - val_precision: 0.3744 - val_f1_score: 0.4124\n",
      "Epoch 2/5\n",
      "15890/15890 [==============================] - 695s 44ms/step - loss: 0.0691 - recall: 0.5918 - precision: 0.4854 - f1_score: 0.5322 - val_loss: 0.0589 - val_recall: 0.6497 - val_precision: 0.5781 - val_f1_score: 0.6112\n",
      "Epoch 3/5\n",
      "15890/15890 [==============================] - 697s 44ms/step - loss: 0.0439 - recall: 0.7588 - precision: 0.6425 - f1_score: 0.6950 - val_loss: 0.0460 - val_recall: 0.7412 - val_precision: 0.6631 - val_f1_score: 0.6993\n",
      "Epoch 4/5\n",
      "15890/15890 [==============================] - 697s 44ms/step - loss: 0.0307 - recall: 0.8498 - precision: 0.7325 - f1_score: 0.7860 - val_loss: 0.0406 - val_recall: 0.7882 - val_precision: 0.7119 - val_f1_score: 0.7476\n",
      "Epoch 5/5\n",
      "15890/15890 [==============================] - 698s 44ms/step - loss: 0.0231 - recall: 0.8997 - precision: 0.7909 - f1_score: 0.8412 - val_loss: 0.0369 - val_recall: 0.8097 - val_precision: 0.7510 - val_f1_score: 0.7788\n",
      "Evaluating model...\n",
      "4966/4966 [==============================] - 83s 16ms/step\n",
      "\n",
      "\n",
      "****Fold: 4 ****\n",
      "Splitting into train-test...\n",
      "Tokenizing...\n",
      "Shuffling...\n",
      "Training...\n",
      "Epoch 1/5\n",
      "15890/15890 [==============================] - 713s 44ms/step - loss: 0.1359 - recall: 0.1913 - precision: 0.1395 - f1_score: nan - val_loss: 0.0922 - val_recall: 0.4090 - val_precision: 0.3453 - val_f1_score: 0.3735\n",
      "Epoch 2/5\n",
      "15890/15890 [==============================] - 705s 44ms/step - loss: 0.0716 - recall: 0.5759 - precision: 0.4679 - f1_score: 0.5151 - val_loss: 0.0609 - val_recall: 0.6444 - val_precision: 0.5588 - val_f1_score: 0.5978\n",
      "Epoch 3/5\n",
      "15890/15890 [==============================] - 705s 44ms/step - loss: 0.0441 - recall: 0.7592 - precision: 0.6422 - f1_score: 0.6950 - val_loss: 0.0453 - val_recall: 0.7563 - val_precision: 0.6688 - val_f1_score: 0.7092\n",
      "Epoch 4/5\n",
      "15890/15890 [==============================] - 706s 44ms/step - loss: 0.0296 - recall: 0.8587 - precision: 0.7418 - f1_score: 0.7953 - val_loss: 0.0382 - val_recall: 0.8050 - val_precision: 0.7187 - val_f1_score: 0.7588\n",
      "Epoch 5/5\n",
      "15890/15890 [==============================] - 705s 44ms/step - loss: 0.0215 - recall: 0.9105 - precision: 0.8050 - f1_score: 0.8539 - val_loss: 0.0343 - val_recall: 0.8372 - val_precision: 0.7552 - val_f1_score: 0.7935\n",
      "Evaluating model...\n",
      "4966/4966 [==============================] - 83s 16ms/step\n",
      "\n",
      "\n",
      "****Fold: 5 ****\n",
      "Splitting into train-test...\n",
      "Tokenizing...\n",
      "Shuffling...\n",
      "Training...\n",
      "Epoch 1/5\n",
      "15890/15890 [==============================] - 716s 45ms/step - loss: 0.1377 - recall: 0.1823 - precision: 0.1374 - f1_score: nan - val_loss: 0.0941 - val_recall: 0.4082 - val_precision: 0.3096 - val_f1_score: 0.3508\n",
      "Epoch 2/5\n",
      "15890/15890 [==============================] - 707s 44ms/step - loss: 0.0734 - recall: 0.5634 - precision: 0.4431 - f1_score: 0.4947 - val_loss: 0.0609 - val_recall: 0.6415 - val_precision: 0.5604 - val_f1_score: 0.5975\n",
      "Epoch 3/5\n",
      "15890/15890 [==============================] - 708s 45ms/step - loss: 0.0461 - recall: 0.7473 - precision: 0.6235 - f1_score: 0.6789 - val_loss: 0.0475 - val_recall: 0.7285 - val_precision: 0.6521 - val_f1_score: 0.6875\n",
      "Epoch 4/5\n",
      "15890/15890 [==============================] - 707s 45ms/step - loss: 0.0320 - recall: 0.8449 - precision: 0.7206 - f1_score: 0.7771 - val_loss: 0.0402 - val_recall: 0.7940 - val_precision: 0.7101 - val_f1_score: 0.7492\n",
      "Epoch 5/5\n",
      "15890/15890 [==============================] - 707s 45ms/step - loss: 0.0234 - recall: 0.9007 - precision: 0.7854 - f1_score: 0.8385 - val_loss: 0.0366 - val_recall: 0.8158 - val_precision: 0.7524 - val_f1_score: 0.7823\n",
      "Evaluating model...\n",
      "4966/4966 [==============================] - 81s 16ms/step\n",
      "Recall: 0.6483603977662\n",
      "Precision: 0.6547201642461157\n",
      "F1-Score: 0.6515234152491681\n"
     ]
    }
   ],
   "source": [
    "Kfolds = 5\n",
    "prev_index = 0\n",
    "\n",
    "recs = []\n",
    "precs = []\n",
    "f1s = []\n",
    "\n",
    "for i in range(Kfolds):\n",
    "    print('\\n\\n****Fold:', i+1,'****')\n",
    "    \n",
    "    print('Splitting into train-test...')\n",
    "    X_train_ng, y_train, X_test_ng, y_test, start_index, prev_index1 = train_test_split(X_ng,y,i,prev_index,Kfolds)\n",
    "    X_train_sg, _, X_test_sg, _, _, _= train_test_split(X_sg,y,i,prev_index,Kfolds)\n",
    "    \n",
    "    prev_index = prev_index1\n",
    "    \n",
    "    print('Tokenizing...')\n",
    "    X_train_ng, X_test_ng, vocab_size_ng, tokenizer1 = tokenization(X_train_ng, X_test_ng, MAX_LEN_NG)  \n",
    "    X_train_sg, X_test_sg, vocab_size_sg, tokenizer2 = tokenization(X_train_sg, X_test_sg, MAX_LEN_SG)\n",
    "    \n",
    "   \n",
    "    print('Shuffling...')   \n",
    "    shuffled = [[X_train_ng[i],X_train_sg[i],y_train[i]] for i in range(len(X_train_ng))]\n",
    "    np.random.shuffle(shuffled)\n",
    "\n",
    "    X_train_ng = [shuffled[i][0] for i in range(len(shuffled))]\n",
    "    X_train_sg = [shuffled[i][1] for i in range(len(shuffled))]\n",
    "    y_train = [shuffled[i][2] for i in range(len(shuffled))]\n",
    "    X_train_ng = np.array(X_train_ng)\n",
    "    X_train_sg = np.array(X_train_sg)\n",
    "    y_train = np.array(y_train)\n",
    "    \n",
    "    \n",
    "    model = get_model_ng_sg(vocab_size_ng, vocab_size_sg)\n",
    "    \n",
    "    model.compile(\n",
    "        loss=custom_loss, \n",
    "        optimizer='adam', \n",
    "        metrics=[\n",
    "            recall,\n",
    "            precision,\n",
    "            f1_score\n",
    "        ])\n",
    "\n",
    "    print('Training...')\n",
    "    history = model.fit([X_train_ng,X_train_sg], y_train, batch_size=32, epochs=5,validation_split=0.2)\n",
    "    \n",
    "    print('Evaluating model...')\n",
    "    predictions = model.predict([X_test_ng,X_test_sg])\n",
    "    \n",
    "    rec, prec, f1 = compute_metrics(predictions, start_index, prev_index)\n",
    "    \n",
    "    recs.append(rec.numpy())\n",
    "    precs.append(prec.numpy())\n",
    "    f1s.append(f1.numpy())\n",
    "\n",
    "print('Recall:',sum(recs)/len(recs))\n",
    "print('Precision:',sum(precs)/len(precs))\n",
    "print('F1-Score:',sum(f1s)/len(f1s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d3bd13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
