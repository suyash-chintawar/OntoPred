{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a056c8a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jan  5 11:00:48 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.106.00   Driver Version: 460.106.00   CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-DGXS...  Off  | 00000000:07:00.0 Off |                    0 |\n",
      "| N/A   39C    P0    52W / 300W |      0MiB / 32508MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-DGXS...  Off  | 00000000:08:00.0 Off |                    0 |\n",
      "| N/A   39C    P0    52W / 300W |      0MiB / 32508MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla V100-DGXS...  Off  | 00000000:0E:00.0 Off |                    0 |\n",
      "| N/A   38C    P0    53W / 300W |      0MiB / 32508MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla V100-DGXS...  Off  | 00000000:0F:00.0 Off |                    0 |\n",
      "| N/A   39C    P0    53W / 300W |      0MiB / 32508MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86add81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fb10a5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-26 23:16:12.000488: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-26 23:16:12.188820: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 9667993972176814861\n",
      "xla_global_id: -1\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 32479117312\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 18279729406772842858\n",
      "physical_device_desc: \"device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:62:00.0, compute capability: 7.0\"\n",
      "xla_global_id: 416903419\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-26 23:16:15.848529: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-26 23:16:18.741713: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /device:GPU:0 with 30974 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:62:00.0, compute capability: 7.0\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8be3b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import tqdm.notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bfb404ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1-1_OHJFk-h9O8eTg5a7lH4QFk4_LFHMG\n",
      "To: /home/191it109/project/home/molecular_function.csv\n",
      "100%|██████████████████████████████████████| 37.9M/37.9M [00:02<00:00, 16.8MB/s]\n"
     ]
    }
   ],
   "source": [
    "!gdown https://drive.google.com/uc?id=1-1_OHJFk-h9O8eTg5a7lH4QFk4_LFHMG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd949fb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Entry</th>\n",
       "      <th>Entry Name</th>\n",
       "      <th>Organism</th>\n",
       "      <th>Length</th>\n",
       "      <th>Sequence</th>\n",
       "      <th>Gene Ontology (molecular function)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B4ESY8</td>\n",
       "      <td>URK_PROMH</td>\n",
       "      <td>Proteus mirabilis (strain HI4320)</td>\n",
       "      <td>213</td>\n",
       "      <td>MADTAHQCTIVGIAGASASGKSLIASTLYRELRAQVGDHNIGVIPE...</td>\n",
       "      <td>GO:0005524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Q9H479</td>\n",
       "      <td>FN3K_HUMAN</td>\n",
       "      <td>Homo sapiens (Human)</td>\n",
       "      <td>309</td>\n",
       "      <td>MEQLLRAELRTATLRAFGGPGAGCISEGRAYDTDAGPVFVKVNRRT...</td>\n",
       "      <td>GO:0005524;GO:0016301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B4PRE2</td>\n",
       "      <td>DGKH_DROYA</td>\n",
       "      <td>Drosophila yakuba (Fruit fly)</td>\n",
       "      <td>1917</td>\n",
       "      <td>MSHLKLDTLHVQRSPRGSRRSSRSSGRSSACSSGSISPVPIIPIIS...</td>\n",
       "      <td>GO:0005524;GO:0046872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A3DMQ4</td>\n",
       "      <td>RS4_STAMF</td>\n",
       "      <td>Staphylothermus marinus (strain ATCC 43588 / D...</td>\n",
       "      <td>168</td>\n",
       "      <td>MGDPKKPRKKWEGPRHPWRKEVLVQELKLLGTYGLRNKRELWRAQT...</td>\n",
       "      <td>GO:0019843;GO:0003735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>P55656</td>\n",
       "      <td>Y4SO_SINFN</td>\n",
       "      <td>Sinorhizobium fredii (strain NBRC 101917 / NGR...</td>\n",
       "      <td>705</td>\n",
       "      <td>MENKSLQPPLPRSERRIRVLHNDVTIDSYGWLRDREDPDVLAYLEA...</td>\n",
       "      <td>GO:0004252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76736</th>\n",
       "      <td>Q7U7P8</td>\n",
       "      <td>BIOB_PARMW</td>\n",
       "      <td>Parasynechococcus marenigrum (strain WH8102)</td>\n",
       "      <td>325</td>\n",
       "      <td>MTFTIRHDWTIAEIQALLELPLMELLWQAQYVHRAANPGYRVQLAS...</td>\n",
       "      <td>GO:0051537;GO:0051539;GO:0005506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76737</th>\n",
       "      <td>Q55855</td>\n",
       "      <td>GLK_SYNY3</td>\n",
       "      <td>Synechocystis sp. (strain PCC 6803 / Kazusa)</td>\n",
       "      <td>355</td>\n",
       "      <td>MGAMGVNFLAGDIGGTKTILALVTINESSPGLARPVTLFEQTYSSP...</td>\n",
       "      <td>GO:0005524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76738</th>\n",
       "      <td>Q7TPM3</td>\n",
       "      <td>TRI17_MOUSE</td>\n",
       "      <td>Mus musculus (Mouse)</td>\n",
       "      <td>477</td>\n",
       "      <td>MDAVELARRLQEEATCSICLDYFTDPVMTACGHNFCRECIQMSWEK...</td>\n",
       "      <td>GO:0061630;GO:0004842;GO:0008270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76739</th>\n",
       "      <td>B1IPK9</td>\n",
       "      <td>SYT_ECOLC</td>\n",
       "      <td>Escherichia coli (strain ATCC 8739 / DSM 1576 ...</td>\n",
       "      <td>642</td>\n",
       "      <td>MPVITLPDGSQRHYDHAVSPMDVALDIGPGLAKACIAGRVNGELVD...</td>\n",
       "      <td>GO:0005524;GO:0046872;GO:0000049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76740</th>\n",
       "      <td>B7I2R7</td>\n",
       "      <td>GLYA_ACIB5</td>\n",
       "      <td>Acinetobacter baumannii (strain AB0057)</td>\n",
       "      <td>417</td>\n",
       "      <td>MFANISISEFDPELAQAIASEDERQEAHIELIASENYCSPAVMEAQ...</td>\n",
       "      <td>GO:0004372;GO:0030170</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>76741 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Entry   Entry Name                                           Organism  \\\n",
       "0      B4ESY8    URK_PROMH                  Proteus mirabilis (strain HI4320)   \n",
       "1      Q9H479   FN3K_HUMAN                               Homo sapiens (Human)   \n",
       "2      B4PRE2   DGKH_DROYA                      Drosophila yakuba (Fruit fly)   \n",
       "3      A3DMQ4    RS4_STAMF  Staphylothermus marinus (strain ATCC 43588 / D...   \n",
       "4      P55656   Y4SO_SINFN  Sinorhizobium fredii (strain NBRC 101917 / NGR...   \n",
       "...       ...          ...                                                ...   \n",
       "76736  Q7U7P8   BIOB_PARMW       Parasynechococcus marenigrum (strain WH8102)   \n",
       "76737  Q55855    GLK_SYNY3       Synechocystis sp. (strain PCC 6803 / Kazusa)   \n",
       "76738  Q7TPM3  TRI17_MOUSE                               Mus musculus (Mouse)   \n",
       "76739  B1IPK9    SYT_ECOLC  Escherichia coli (strain ATCC 8739 / DSM 1576 ...   \n",
       "76740  B7I2R7   GLYA_ACIB5            Acinetobacter baumannii (strain AB0057)   \n",
       "\n",
       "       Length                                           Sequence  \\\n",
       "0         213  MADTAHQCTIVGIAGASASGKSLIASTLYRELRAQVGDHNIGVIPE...   \n",
       "1         309  MEQLLRAELRTATLRAFGGPGAGCISEGRAYDTDAGPVFVKVNRRT...   \n",
       "2        1917  MSHLKLDTLHVQRSPRGSRRSSRSSGRSSACSSGSISPVPIIPIIS...   \n",
       "3         168  MGDPKKPRKKWEGPRHPWRKEVLVQELKLLGTYGLRNKRELWRAQT...   \n",
       "4         705  MENKSLQPPLPRSERRIRVLHNDVTIDSYGWLRDREDPDVLAYLEA...   \n",
       "...       ...                                                ...   \n",
       "76736     325  MTFTIRHDWTIAEIQALLELPLMELLWQAQYVHRAANPGYRVQLAS...   \n",
       "76737     355  MGAMGVNFLAGDIGGTKTILALVTINESSPGLARPVTLFEQTYSSP...   \n",
       "76738     477  MDAVELARRLQEEATCSICLDYFTDPVMTACGHNFCRECIQMSWEK...   \n",
       "76739     642  MPVITLPDGSQRHYDHAVSPMDVALDIGPGLAKACIAGRVNGELVD...   \n",
       "76740     417  MFANISISEFDPELAQAIASEDERQEAHIELIASENYCSPAVMEAQ...   \n",
       "\n",
       "      Gene Ontology (molecular function)  \n",
       "0                             GO:0005524  \n",
       "1                  GO:0005524;GO:0016301  \n",
       "2                  GO:0005524;GO:0046872  \n",
       "3                  GO:0019843;GO:0003735  \n",
       "4                             GO:0004252  \n",
       "...                                  ...  \n",
       "76736   GO:0051537;GO:0051539;GO:0005506  \n",
       "76737                         GO:0005524  \n",
       "76738   GO:0061630;GO:0004842;GO:0008270  \n",
       "76739   GO:0005524;GO:0046872;GO:0000049  \n",
       "76740              GO:0004372;GO:0030170  \n",
       "\n",
       "[76741 rows x 6 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('molecular_function.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "724323ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "149\n",
      "['GO:0000049', 'GO:0000107', 'GO:0000166', 'GO:0000175', 'GO:0000287', 'GO:0000976', 'GO:0000977', 'GO:0000978', 'GO:0000981', 'GO:0001228', 'GO:0002161', 'GO:0003676', 'GO:0003677', 'GO:0003678', 'GO:0003682', 'GO:0003684', 'GO:0003690', 'GO:0003697', 'GO:0003700', 'GO:0003712', 'GO:0003713', 'GO:0003723', 'GO:0003724', 'GO:0003729', 'GO:0003735', 'GO:0003743', 'GO:0003746', 'GO:0003755', 'GO:0003779', 'GO:0003824', 'GO:0003861', 'GO:0003887', 'GO:0003899', 'GO:0003911', 'GO:0003924', 'GO:0003989', 'GO:0004017', 'GO:0004176', 'GO:0004190', 'GO:0004222', 'GO:0004252', 'GO:0004359', 'GO:0004372', 'GO:0004497', 'GO:0004519', 'GO:0004521', 'GO:0004523', 'GO:0004672', 'GO:0004674', 'GO:0004814', 'GO:0004826', 'GO:0004827', 'GO:0004834', 'GO:0004842', 'GO:0004930', 'GO:0005096', 'GO:0005102', 'GO:0005125', 'GO:0005179', 'GO:0005198', 'GO:0005344', 'GO:0005506', 'GO:0005507', 'GO:0005509', 'GO:0005516', 'GO:0005524', 'GO:0005525', 'GO:0008017', 'GO:0008083', 'GO:0008121', 'GO:0008137', 'GO:0008168', 'GO:0008233', 'GO:0008237', 'GO:0008270', 'GO:0008289', 'GO:0008483', 'GO:0008564', 'GO:0009055', 'GO:0009378', 'GO:0009381', 'GO:0010181', 'GO:0015293', 'GO:0015297', 'GO:0016149', 'GO:0016151', 'GO:0016168', 'GO:0016301', 'GO:0016462', 'GO:0016491', 'GO:0016597', 'GO:0016655', 'GO:0016705', 'GO:0016740', 'GO:0016743', 'GO:0016783', 'GO:0016787', 'GO:0016829', 'GO:0016853', 'GO:0016879', 'GO:0016887', 'GO:0019825', 'GO:0019843', 'GO:0019899', 'GO:0019901', 'GO:0019904', 'GO:0020037', 'GO:0022857', 'GO:0030145', 'GO:0030170', 'GO:0030246', 'GO:0030527', 'GO:0030976', 'GO:0030983', 'GO:0031072', 'GO:0031267', 'GO:0031625', 'GO:0032549', 'GO:0042802', 'GO:0042803', 'GO:0043022', 'GO:0043565', 'GO:0044877', 'GO:0046872', 'GO:0046933', 'GO:0046961', 'GO:0046982', 'GO:0046983', 'GO:0048038', 'GO:0050136', 'GO:0050567', 'GO:0050660', 'GO:0050661', 'GO:0051015', 'GO:0051082', 'GO:0051087', 'GO:0051287', 'GO:0051537', 'GO:0051539', 'GO:0061630', 'GO:0070180', 'GO:0071949', 'GO:0090729', 'GO:0097367', 'GO:0106029', 'GO:0106310', 'GO:0140662', 'GO:0140664', 'GO:1990837']\n"
     ]
    }
   ],
   "source": [
    "go_terms_mf = set()\n",
    "for idx, row in df.iterrows():\n",
    "  for term in row['Gene Ontology (molecular function)'].split(';'):\n",
    "    go_terms_mf.add(term)\n",
    "go_terms_mf = list(go_terms_mf)\n",
    "go_terms_mf.sort()\n",
    "print(len(go_terms_mf))\n",
    "print(go_terms_mf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72f8a1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_segments(sequence,segment_size=100,gap=30):\n",
    "  segments = []\n",
    "  start = 0\n",
    "  end = segment_size\n",
    "  while end <= len(sequence):\n",
    "    segments.append(sequence[start:end])\n",
    "    start += gap\n",
    "    end += gap\n",
    "  last_segment = sequence[start:]\n",
    "  segments.append(last_segment)\n",
    "  \n",
    "  return segments\n",
    "\n",
    "def get_training_data(df,segment_size=100,gap=30):\n",
    "  training_data = list()\n",
    "  for idx,row in tqdm.tqdm(df.iterrows()):\n",
    "    labels = [0] * len(go_terms_mf)\n",
    "    for term in row['Gene Ontology (molecular function)'].split(';'):\n",
    "      labels[go_terms_mf.index(term)] = 1\n",
    "    segments = get_segments(row['Sequence'],segment_size,gap)\n",
    "    for segment in segments:\n",
    "      training_data.append([row['Entry'],segment,labels])\n",
    "  return training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f1d6c8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c9b348fb5154878bdb1007b4faa81e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "872283\n"
     ]
    }
   ],
   "source": [
    "training_data = get_training_data(df)\n",
    "print(len(training_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1c0991f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngrams(segment,n=3):\n",
    "  ngrams = []\n",
    "  for i in range(len(segment)-n+1):\n",
    "    ngrams.append(segment[i:i+n])\n",
    "  return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5358736d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5b1a9be2ebd422fa656299221ea4d16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/872283 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "872283\n"
     ]
    }
   ],
   "source": [
    "# Generate training data of ngrams\n",
    "training_data_ngrams = []\n",
    "\n",
    "for i in tqdm.tqdm(range(len(training_data))):\n",
    "    training_data_ngrams.append([training_data[i][0],get_ngrams(training_data[i][1],n=3),training_data[i][2]])\n",
    "print(len(training_data_ngrams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c435a362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('mf/training_data_ngrams.npy',training_data_ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6784a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load array if already stored\n",
    "# training_data_ngrams = np.load('drive/MyDrive/mp/mf/training_data_ngrams.npy',allow_pickle=True)\n",
    "# print(len(training_data_ngrams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "4acd14a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_skip_grams(segment,skip=1,n=3):\n",
    "  skip_grams = []\n",
    "  window_size = skip + n\n",
    "  for i in range(len(segment)-window_size+1):\n",
    "    window = segment[i:i+window_size]\n",
    "    indices = list(range(window_size))\n",
    "    indices.pop(0)\n",
    "    for idx in indices[::-1]:\n",
    "      temp = ''\n",
    "      for j in range(window_size):\n",
    "        if j!=idx:\n",
    "          temp+=window[j]\n",
    "      skip_grams.append(temp)\n",
    "\n",
    "  return skip_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "fa91e8e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19525df308634bb29db346706d27d0be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/872283 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "872283\n"
     ]
    }
   ],
   "source": [
    "# Generate training data of skip grams\n",
    "training_data_skip_grams = []\n",
    "for i in tqdm.tqdm(range(len(training_data))):\n",
    "    training_data_skip_grams.append([training_data[i][0],get_skip_grams(training_data[i][1],n=3),training_data[i][2]])\n",
    "print(len(training_data_skip_grams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d9bc2ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('mf/training_data_skip_grams.npy',training_data_skip_grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "aca0eff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load array if already stored\n",
    "# training_data_skip_grams = np.load('mf/training_data_skip_grams.npy',allow_pickle=True)\n",
    "# print(len(training_data_skip_grams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11d856c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e391f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assuming training_data as global variable\n",
    "\n",
    "def train_test_split(X,y,fold_no,prev_index,Kfolds=5):\n",
    "    test_split = 1/Kfolds\n",
    "    \n",
    "    start_index = prev_index\n",
    "    end_index = (fold_no + 1) * (test_split) * len(X)\n",
    "    end_index = round(end_index)\n",
    "    \n",
    "    if end_index==len(X):\n",
    "        end_index -= 1\n",
    "    \n",
    "    entry = training_data[end_index][0]\n",
    "    entries = [sample[0] for sample in training_data]\n",
    "    \n",
    "    first_occurence = entries.index(entry)\n",
    "    entries.reverse()\n",
    "    \n",
    "    last_occurence = entries.index(entry)\n",
    "    last_occurence = len(entries) - last_occurence - 1\n",
    "    \n",
    "    del entries\n",
    "    gc.collect()\n",
    "    \n",
    "    end_index = first_occurence if (abs(end_index-first_occurence) < abs(end_index-last_occurence)) else last_occurence\n",
    "    \n",
    "    X_test = X[start_index:end_index+1]\n",
    "    y_test = y[start_index:end_index+1]\n",
    "    X_train = X[:start_index]\n",
    "    X_train.extend(X[end_index+1:])\n",
    "    y_train = y[:start_index]\n",
    "    y_train.extend(y[end_index+1:])\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test, start_index, end_index + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "730f9429",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_WORDS = 13824\n",
    "MAX_LEN_NG = 98 #100\n",
    "MAX_LEN_SG = 291 #300\n",
    "\n",
    "def tokenization(X_train,X_test,maxlen):\n",
    "\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=MAX_WORDS)\n",
    "    tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "    X_train = tokenizer.texts_to_sequences(X_train)\n",
    "    X_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "    X_train = tf.keras.preprocessing.sequence.pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
    "    X_test = tf.keras.preprocessing.sequence.pad_sequences(X_test, padding='post', maxlen=maxlen)\n",
    "    \n",
    "    return X_train, X_test, vocab_size, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "10a7f090",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "\n",
    "false_negative_penalty = 6\n",
    "false_positive_penalty = 1\n",
    "\n",
    "def custom_loss(y_true, y_logit):\n",
    "\n",
    "    loss = float(0)\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_logit = tf.cast(y_logit, tf.float32)\n",
    "    \n",
    "    first_term = false_negative_penalty * float(y_true) * - K.log(y_logit + K.epsilon())\n",
    "    second_term = false_positive_penalty * (1 - float(y_true)) * - K.log(1 - y_logit + K.epsilon())\n",
    "    \n",
    "    loss = K.mean(first_term+second_term)\n",
    "\n",
    "    return loss\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    \n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)), axis=1)\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)), axis=1)\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    \n",
    "    return K.mean(precision)\n",
    "\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)), axis=1)\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)), axis=1)\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return K.mean(recall)\n",
    "\n",
    "def f1_score(y_true, y_pred):\n",
    "    rec = recall(y_true,y_pred)\n",
    "    prec = precision(y_true,y_pred)\n",
    "    f1 = 2*prec*rec/(prec+rec)\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "36c30cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class attention(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, return_sequences=True,**kwargs):\n",
    "        self.return_sequences = return_sequences\n",
    "        super(attention,self).__init__()\n",
    "\n",
    "    def get_config(self):\n",
    "      config = super().get_config().copy()\n",
    "      config.update({\n",
    "          'return_sequences': self.return_sequences \n",
    "      })\n",
    "      return config\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        \n",
    "        self.W=self.add_weight(name=\"att_weight\", shape=(input_shape[-1],1),\n",
    "                               initializer=\"normal\")\n",
    "        self.b=self.add_weight(name=\"att_bias\", shape=(input_shape[1],1),\n",
    "                               initializer=\"zeros\")\n",
    "        \n",
    "        super(attention,self).build(input_shape)\n",
    "        \n",
    "    def call(self, x):\n",
    "        \n",
    "        e = K.tanh(K.dot(x,self.W)+self.b)\n",
    "        a = K.softmax(e, axis=1)\n",
    "        output = x*a\n",
    "        \n",
    "        if self.return_sequences:\n",
    "            return output\n",
    "        \n",
    "        return K.sum(output, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b6dbee46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Base model using ngrams \n",
    "NUM_CLASSES = 149\n",
    "\n",
    "def get_model_ng(vocab_size_ng):\n",
    "    input_ngrams = tf.keras.layers.Input(shape=(MAX_LEN_NG,))\n",
    "\n",
    "    embedding_layer = tf.keras.layers.Embedding(vocab_size_ng, 32)(input_ngrams)\n",
    "\n",
    "    LSTM_Layer_1 = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(70,return_sequences=True))(embedding_layer)\n",
    "    attention_output_1 = attention(return_sequences=False)(LSTM_Layer_1)\n",
    "    dropout = tf.keras.layers.Dropout(0.3)(attention_output_1)\n",
    "    dense_layer_1 = tf.keras.layers.Dense(NUM_CLASSES, activation='sigmoid')(dropout)\n",
    "\n",
    "    model = tf.keras.models.Model(\n",
    "        inputs=input_ngrams, \n",
    "        outputs=dense_layer_1\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "014f93a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Base model using skip grams \n",
    "NUM_CLASSES = 149\n",
    "\n",
    "def get_model_sg(vocab_size_sg):\n",
    "    input_skip_grams = tf.keras.layers.Input(shape=(MAX_LEN_SG,))\n",
    "\n",
    "    embedding_layer = tf.keras.layers.Embedding(vocab_size_sg, 32)(input_skip_grams)\n",
    "\n",
    "    LSTM_Layer_1 = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(70,return_sequences=True))(embedding_layer)\n",
    "    attention_output_1 = attention(return_sequences=False)(LSTM_Layer_1)\n",
    "    dropout = tf.keras.layers.Dropout(0.3)(attention_output_1)\n",
    "    dense_layer_1 = tf.keras.layers.Dense(NUM_CLASSES, activation='sigmoid')(dropout)\n",
    "\n",
    "    model = tf.keras.models.Model(\n",
    "        inputs=input_skip_grams, \n",
    "        outputs=dense_layer_1\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "03b912ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 149 #For molecular function (Change according to aspects)\n",
    "\n",
    "def get_model_ng_sg(vocab_size_ng, vocab_size_sg):\n",
    "    #Input layers\n",
    "\n",
    "    input_ngrams = tf.keras.layers.Input(shape=(MAX_LEN_NG,)) \n",
    "    input_skip_grams = tf.keras.layers.Input(shape=(MAX_LEN_SG,)) \n",
    "\n",
    "    #embeddings\n",
    "    embedding_layer_ngrams = tf.keras.layers.Embedding(vocab_size_ng, 32)(input_ngrams)\n",
    "    embedding_layer_skip_grams = tf.keras.layers.Embedding(vocab_size_sg, 32)(input_skip_grams)\n",
    "\n",
    "    #BI-LSTMs for each of the inputs\n",
    "    sequence_output_1 = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(70,return_sequences=True))(embedding_layer_ngrams)\n",
    "    attention_output_1 = attention(return_sequences=False)(sequence_output_1)\n",
    "    dropout_1 = tf.keras.layers.Dropout(0.3)(attention_output_1)\n",
    "    dense_layer_1 = tf.keras.layers.Dense(NUM_CLASSES, activation='sigmoid')(dropout_1)\n",
    "\n",
    "    sequence_output_2 = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(70, return_sequences=True))(embedding_layer_skip_grams)\n",
    "    attention_output_2 = attention(return_sequences=False)(sequence_output_2)\n",
    "    dropout_2 = tf.keras.layers.Dropout(0.3)(attention_output_2)\n",
    "    dense_layer_2 = tf.keras.layers.Dense(NUM_CLASSES, activation='sigmoid')(dropout_2)\n",
    "\n",
    "    max_layer = tf.keras.layers.Maximum()([dense_layer_1,dense_layer_2])\n",
    "\n",
    "    model = tf.keras.models.Model(\n",
    "        inputs=[\n",
    "            input_ngrams,\n",
    "            input_skip_grams\n",
    "        ], \n",
    "        outputs=max_layer)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4236670c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(predictions, start_index, end_index):\n",
    "    final_predictions = []\n",
    "    actual_y_test = []\n",
    "\n",
    "    current_entry = ''\n",
    "    counter = 0\n",
    "    total_counts = 0\n",
    "\n",
    "    if len(predictions) == len(training_data[start_index: end_index]):\n",
    "        temp = np.zeros(NUM_CLASSES)\n",
    "        for i in range(len(predictions)):\n",
    "            if current_entry != training_data[start_index+i][0]:\n",
    "                #compute prev\n",
    "                if i!=0:\n",
    "                    temp /= counter\n",
    "                    final_predictions.append(temp)\n",
    "\n",
    "                #reset\n",
    "                total_counts += counter\n",
    "                counter = 1\n",
    "                temp = np.zeros(NUM_CLASSES)\n",
    "\n",
    "                #init new\n",
    "                current_entry = training_data[start_index+i][0]\n",
    "                temp += np.array(predictions[i])\n",
    "                actual_y_test.append(training_data[start_index+i][2])\n",
    "            else:\n",
    "                temp += np.array(predictions[i])\n",
    "                counter += 1\n",
    "\n",
    "        total_counts += counter\n",
    "        temp /= counter\n",
    "        final_predictions.append(temp)\n",
    "\n",
    "    else:\n",
    "        print('Lengths of predictions dont match with test data')\n",
    "    \n",
    "    final_predictions = np.array(final_predictions, dtype=float)\n",
    "    actual_y_test = np.array(actual_y_test, dtype=float)\n",
    "    \n",
    "    rec = recall(actual_y_test,final_predictions)\n",
    "    prec = precision(actual_y_test,final_predictions)\n",
    "    f1 = f1_score(actual_y_test,final_predictions)\n",
    "    \n",
    "    return rec,prec,f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1454935",
   "metadata": {},
   "source": [
    "### Model using ngrams only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "894bcfa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Considering ngrams\n",
    "X_ng = [' '.join(sample[1]) for sample in training_data_ngrams]\n",
    "y = [sample[2] for sample in training_data]\n",
    "\n",
    "del training_data_ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f6664241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "****Fold: 1 ****\n",
      "Splitting into train-test...\n",
      "Tokenizing...\n",
      "Shuffling...\n",
      "Training...\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-26 23:21:57.225729: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17446/17446 [==============================] - 189s 11ms/step - loss: 0.1935 - recall: 0.3500 - precision: 0.2812 - f1_score: 0.3095 - val_loss: 0.1616 - val_recall: 0.4319 - val_precision: 0.3327 - val_f1_score: 0.3744\n",
      "Epoch 2/5\n",
      "17446/17446 [==============================] - 185s 11ms/step - loss: 0.1513 - recall: 0.4733 - precision: 0.3610 - f1_score: 0.4081 - val_loss: 0.1385 - val_recall: 0.4997 - val_precision: 0.4147 - val_f1_score: 0.4520\n",
      "Epoch 3/5\n",
      "17446/17446 [==============================] - 185s 11ms/step - loss: 0.1328 - recall: 0.5374 - precision: 0.4167 - f1_score: 0.4681 - val_loss: 0.1254 - val_recall: 0.5570 - val_precision: 0.4509 - val_f1_score: 0.4973\n",
      "Epoch 4/5\n",
      "17446/17446 [==============================] - 185s 11ms/step - loss: 0.1214 - recall: 0.5761 - precision: 0.4527 - f1_score: 0.5058 - val_loss: 0.1178 - val_recall: 0.5837 - val_precision: 0.4763 - val_f1_score: 0.5235\n",
      "Epoch 5/5\n",
      "17446/17446 [==============================] - 185s 11ms/step - loss: 0.1136 - recall: 0.6016 - precision: 0.4770 - f1_score: 0.5310 - val_loss: 0.1126 - val_recall: 0.5934 - val_precision: 0.5025 - val_f1_score: 0.5432\n",
      "\n",
      "Evaluating model...\n",
      "5452/5452 [==============================] - 22s 4ms/step\n",
      "\n",
      "\n",
      "****Fold: 2 ****\n",
      "Splitting into train-test...\n",
      "Tokenizing...\n",
      "Shuffling...\n",
      "Training...\n",
      "Epoch 1/5\n",
      "17446/17446 [==============================] - 187s 11ms/step - loss: 0.1947 - recall: 0.3502 - precision: 0.2756 - f1_score: 0.3061 - val_loss: 0.1641 - val_recall: 0.4255 - val_precision: 0.3300 - val_f1_score: 0.3704\n",
      "Epoch 2/5\n",
      "17446/17446 [==============================] - 183s 10ms/step - loss: 0.1539 - recall: 0.4690 - precision: 0.3541 - f1_score: 0.4021 - val_loss: 0.1429 - val_recall: 0.4939 - val_precision: 0.3991 - val_f1_score: 0.4402\n",
      "Epoch 3/5\n",
      "17446/17446 [==============================] - 185s 11ms/step - loss: 0.1356 - recall: 0.5288 - precision: 0.4100 - f1_score: 0.4606 - val_loss: 0.1280 - val_recall: 0.5484 - val_precision: 0.4450 - val_f1_score: 0.4902\n",
      "Epoch 4/5\n",
      "17446/17446 [==============================] - 184s 11ms/step - loss: 0.1239 - recall: 0.5669 - precision: 0.4463 - f1_score: 0.4982 - val_loss: 0.1199 - val_recall: 0.5729 - val_precision: 0.4798 - val_f1_score: 0.5212\n",
      "Epoch 5/5\n",
      "17446/17446 [==============================] - 182s 10ms/step - loss: 0.1159 - recall: 0.5932 - precision: 0.4702 - f1_score: 0.5234 - val_loss: 0.1151 - val_recall: 0.5853 - val_precision: 0.4956 - val_f1_score: 0.5357\n",
      "\n",
      "Evaluating model...\n",
      "5452/5452 [==============================] - 22s 4ms/step\n",
      "\n",
      "\n",
      "****Fold: 3 ****\n",
      "Splitting into train-test...\n",
      "Tokenizing...\n",
      "Shuffling...\n",
      "Training...\n",
      "Epoch 1/5\n",
      "17446/17446 [==============================] - 185s 10ms/step - loss: 0.1963 - recall: 0.3459 - precision: 0.2724 - f1_score: 0.3024 - val_loss: 0.1647 - val_recall: 0.4190 - val_precision: 0.3244 - val_f1_score: 0.3643\n",
      "Epoch 2/5\n",
      "17446/17446 [==============================] - 183s 10ms/step - loss: 0.1547 - recall: 0.4643 - precision: 0.3499 - f1_score: 0.3975 - val_loss: 0.1411 - val_recall: 0.4952 - val_precision: 0.4024 - val_f1_score: 0.4427\n",
      "Epoch 3/5\n",
      "17446/17446 [==============================] - 182s 10ms/step - loss: 0.1359 - recall: 0.5270 - precision: 0.4072 - f1_score: 0.4580 - val_loss: 0.1280 - val_recall: 0.5436 - val_precision: 0.4441 - val_f1_score: 0.4877\n",
      "Epoch 4/5\n",
      "17446/17446 [==============================] - 184s 11ms/step - loss: 0.1242 - recall: 0.5652 - precision: 0.4441 - f1_score: 0.4962 - val_loss: 0.1196 - val_recall: 0.5713 - val_precision: 0.4781 - val_f1_score: 0.5195\n",
      "Epoch 5/5\n",
      "17446/17446 [==============================] - 184s 11ms/step - loss: 0.1160 - recall: 0.5929 - precision: 0.4692 - f1_score: 0.5227 - val_loss: 0.1141 - val_recall: 0.5958 - val_precision: 0.4917 - val_f1_score: 0.5377\n",
      "\n",
      "Evaluating model...\n",
      "5452/5452 [==============================] - 20s 4ms/step\n",
      "\n",
      "\n",
      "****Fold: 4 ****\n",
      "Splitting into train-test...\n",
      "Tokenizing...\n",
      "Shuffling...\n",
      "Training...\n",
      "Epoch 1/5\n",
      "17446/17446 [==============================] - 185s 10ms/step - loss: 0.1960 - recall: 0.3436 - precision: 0.2790 - f1_score: 0.3055 - val_loss: 0.1641 - val_recall: 0.4169 - val_precision: 0.3354 - val_f1_score: 0.3704\n",
      "Epoch 2/5\n",
      "17446/17446 [==============================] - 184s 11ms/step - loss: 0.1541 - recall: 0.4647 - precision: 0.3533 - f1_score: 0.4000 - val_loss: 0.1412 - val_recall: 0.4941 - val_precision: 0.4046 - val_f1_score: 0.4437\n",
      "Epoch 3/5\n",
      "17446/17446 [==============================] - 185s 11ms/step - loss: 0.1357 - recall: 0.5280 - precision: 0.4089 - f1_score: 0.4596 - val_loss: 0.1285 - val_recall: 0.5327 - val_precision: 0.4537 - val_f1_score: 0.4890\n",
      "Epoch 4/5\n",
      "17446/17446 [==============================] - 182s 10ms/step - loss: 0.1242 - recall: 0.5665 - precision: 0.4457 - f1_score: 0.4977 - val_loss: 0.1199 - val_recall: 0.5701 - val_precision: 0.4759 - val_f1_score: 0.5177\n",
      "Epoch 5/5\n",
      "17446/17446 [==============================] - 184s 11ms/step - loss: 0.1158 - recall: 0.5930 - precision: 0.4703 - f1_score: 0.5234 - val_loss: 0.1142 - val_recall: 0.5944 - val_precision: 0.4885 - val_f1_score: 0.5353\n",
      "\n",
      "Evaluating model...\n",
      "5452/5452 [==============================] - 21s 4ms/step\n",
      "\n",
      "\n",
      "****Fold: 5 ****\n",
      "Splitting into train-test...\n",
      "Tokenizing...\n",
      "Shuffling...\n",
      "Training...\n",
      "Epoch 1/5\n",
      "17446/17446 [==============================] - 185s 10ms/step - loss: 0.1941 - recall: 0.3529 - precision: 0.2790 - f1_score: 0.3094 - val_loss: 0.1642 - val_recall: 0.4431 - val_precision: 0.3169 - val_f1_score: 0.3682\n",
      "Epoch 2/5\n",
      "17446/17446 [==============================] - 184s 11ms/step - loss: 0.1530 - recall: 0.4740 - precision: 0.3526 - f1_score: 0.4029 - val_loss: 0.1409 - val_recall: 0.5046 - val_precision: 0.3963 - val_f1_score: 0.4427\n",
      "Epoch 3/5\n",
      "17446/17446 [==============================] - 184s 11ms/step - loss: 0.1345 - recall: 0.5348 - precision: 0.4113 - f1_score: 0.4637 - val_loss: 0.1270 - val_recall: 0.5553 - val_precision: 0.4432 - val_f1_score: 0.4918\n",
      "Epoch 4/5\n",
      "17446/17446 [==============================] - 184s 11ms/step - loss: 0.1226 - recall: 0.5738 - precision: 0.4523 - f1_score: 0.5046 - val_loss: 0.1191 - val_recall: 0.5784 - val_precision: 0.4779 - val_f1_score: 0.5224\n",
      "Epoch 5/5\n",
      "17446/17446 [==============================] - 183s 10ms/step - loss: 0.1145 - recall: 0.5991 - precision: 0.4773 - f1_score: 0.5301 - val_loss: 0.1142 - val_recall: 0.5881 - val_precision: 0.4991 - val_f1_score: 0.5390\n",
      "\n",
      "Evaluating model...\n",
      "5452/5452 [==============================] - 20s 4ms/step\n",
      "Recall: 0.6051538765218144\n",
      "Precision: 0.5573750127492767\n",
      "F1-Score: 0.5802537933854532\n"
     ]
    }
   ],
   "source": [
    "Kfolds = 5\n",
    "prev_index = 0\n",
    "\n",
    "recs = []\n",
    "precs = []\n",
    "f1s = []\n",
    "\n",
    "for i in range(Kfolds):\n",
    "    print('\\n\\n****Fold:', i+1,'****')\n",
    "    \n",
    "    print('Splitting into train-test...')\n",
    "    X_train_ng, y_train, X_test_ng, y_test, start_index, prev_index = train_test_split(X_ng,y,i,prev_index,Kfolds)\n",
    "\n",
    "    print('Tokenizing...')\n",
    "    X_train_ng, X_test_ng, vocab_size_ng, tokenizer1 = tokenization(X_train_ng, X_test_ng, MAX_LEN_NG)\n",
    "    \n",
    "    print('Shuffling...')\n",
    "    shuffled = [[X_train_ng[i],y_train[i]] for i in range(len(X_train_ng))]\n",
    "    np.random.shuffle(shuffled)\n",
    "\n",
    "    X_train_ng = [shuffled[i][0] for i in range(len(shuffled))]\n",
    "    y_train = [shuffled[i][1] for i in range(len(shuffled))]\n",
    "    X_train_ng = np.array(X_train_ng)\n",
    "    y_train = np.array(y_train)\n",
    "    \n",
    "    \n",
    "    \n",
    "    model = get_model_ng(vocab_size_ng)\n",
    "    \n",
    "    model.compile(\n",
    "        loss=custom_loss, \n",
    "        optimizer='adam', \n",
    "        metrics=[\n",
    "            recall,\n",
    "            precision,\n",
    "            f1_score\n",
    "        ])\n",
    "    \n",
    "    print('Training...')\n",
    "    history = model.fit(X_train_ng, y_train, batch_size=32, epochs=5,validation_split=0.2)\n",
    "    \n",
    "    print('\\nEvaluating model...')\n",
    "    predictions = model.predict(X_test_ng)\n",
    "    \n",
    "    rec, prec, f1 = compute_metrics(predictions, start_index, prev_index)\n",
    "    \n",
    "    recs.append(rec.numpy())\n",
    "    precs.append(prec.numpy())\n",
    "    f1s.append(f1.numpy())\n",
    "\n",
    "print('Recall:',sum(recs)/len(recs))\n",
    "print('Precision:',sum(precs)/len(precs))\n",
    "print('F1-Score:',sum(f1s)/len(f1s))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e28840b",
   "metadata": {},
   "source": [
    "### Model using skip grams only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "fbba7e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Considering skip grams\n",
    "X_sg = [' '.join(sample[1]) for sample in training_data_skip_grams]\n",
    "y = [sample[2] for sample in training_data]\n",
    "\n",
    "del training_data_skip_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "33e1124b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "****Fold: 1 ****\n",
      "Splitting into train-test...\n",
      "Tokenizing...\n",
      "Shuffling...\n",
      "Training...\n",
      "Epoch 1/5\n",
      "17446/17446 [==============================] - 598s 34ms/step - loss: 0.2009 - recall: 0.3306 - precision: 0.2658 - f1_score: 0.2923 - val_loss: 0.1725 - val_recall: 0.3947 - val_precision: 0.3053 - val_f1_score: 0.3429\n",
      "Epoch 2/5\n",
      "17446/17446 [==============================] - 595s 34ms/step - loss: 0.1610 - recall: 0.4462 - precision: 0.3282 - f1_score: 0.3767 - val_loss: 0.1474 - val_recall: 0.4813 - val_precision: 0.3732 - val_f1_score: 0.4191\n",
      "Epoch 3/5\n",
      "17446/17446 [==============================] - 595s 34ms/step - loss: 0.1413 - recall: 0.5089 - precision: 0.3863 - f1_score: 0.4378 - val_loss: 0.1327 - val_recall: 0.5279 - val_precision: 0.4276 - val_f1_score: 0.4713\n",
      "Epoch 4/5\n",
      "17446/17446 [==============================] - 595s 34ms/step - loss: 0.1284 - recall: 0.5514 - precision: 0.4292 - f1_score: 0.4814 - val_loss: 0.1239 - val_recall: 0.5593 - val_precision: 0.4571 - val_f1_score: 0.5020\n",
      "Epoch 5/5\n",
      "17446/17446 [==============================] - 594s 34ms/step - loss: 0.1196 - recall: 0.5807 - precision: 0.4573 - f1_score: 0.5104 - val_loss: 0.1172 - val_recall: 0.5835 - val_precision: 0.4784 - val_f1_score: 0.5247\n",
      "Evaluating model...\n",
      "5452/5452 [==============================] - 65s 12ms/step\n",
      "\n",
      "\n",
      "****Fold: 2 ****\n",
      "Splitting into train-test...\n",
      "Tokenizing...\n",
      "Shuffling...\n",
      "Training...\n",
      "Epoch 1/5\n",
      "17446/17446 [==============================] - 598s 34ms/step - loss: 0.2041 - recall: 0.3195 - precision: 0.2610 - f1_score: 0.2846 - val_loss: 0.1778 - val_recall: 0.3892 - val_precision: 0.2913 - val_f1_score: 0.3318\n",
      "Epoch 2/5\n",
      "17446/17446 [==============================] - 595s 34ms/step - loss: 0.1638 - recall: 0.4388 - precision: 0.3214 - f1_score: 0.3696 - val_loss: 0.1493 - val_recall: 0.4960 - val_precision: 0.3488 - val_f1_score: 0.4083\n",
      "Epoch 3/5\n",
      "17446/17446 [==============================] - 595s 34ms/step - loss: 0.1425 - recall: 0.5057 - precision: 0.3806 - f1_score: 0.4330 - val_loss: 0.1339 - val_recall: 0.5283 - val_precision: 0.4185 - val_f1_score: 0.4658\n",
      "Epoch 4/5\n",
      "17446/17446 [==============================] - 595s 34ms/step - loss: 0.1294 - recall: 0.5494 - precision: 0.4220 - f1_score: 0.4761 - val_loss: 0.1248 - val_recall: 0.5625 - val_precision: 0.4479 - val_f1_score: 0.4976\n",
      "Epoch 5/5\n",
      "17446/17446 [==============================] - 595s 34ms/step - loss: 0.1203 - recall: 0.5793 - precision: 0.4510 - f1_score: 0.5060 - val_loss: 0.1190 - val_recall: 0.5710 - val_precision: 0.4793 - val_f1_score: 0.5201\n",
      "Evaluating model...\n",
      "5452/5452 [==============================] - 65s 12ms/step\n",
      "\n",
      "\n",
      "****Fold: 3 ****\n",
      "Splitting into train-test...\n",
      "Tokenizing...\n",
      "Shuffling...\n",
      "Training...\n",
      "Epoch 1/5\n",
      "17446/17446 [==============================] - 598s 34ms/step - loss: 0.2044 - recall: 0.3166 - precision: 0.2625 - f1_score: 0.2843 - val_loss: 0.1763 - val_recall: 0.3948 - val_precision: 0.2906 - val_f1_score: 0.3333\n",
      "Epoch 2/5\n",
      "17446/17446 [==============================] - 595s 34ms/step - loss: 0.1655 - recall: 0.4311 - precision: 0.3172 - f1_score: 0.3640 - val_loss: 0.1519 - val_recall: 0.4685 - val_precision: 0.3528 - val_f1_score: 0.4012\n",
      "Epoch 3/5\n",
      "17446/17446 [==============================] - 595s 34ms/step - loss: 0.1465 - recall: 0.4911 - precision: 0.3702 - f1_score: 0.4208 - val_loss: 0.1373 - val_recall: 0.5149 - val_precision: 0.4073 - val_f1_score: 0.4536\n",
      "Epoch 4/5\n",
      "17446/17446 [==============================] - 595s 34ms/step - loss: 0.1339 - recall: 0.5310 - precision: 0.4111 - f1_score: 0.4621 - val_loss: 0.1278 - val_recall: 0.5415 - val_precision: 0.4458 - val_f1_score: 0.4880\n",
      "Epoch 5/5\n",
      "17446/17446 [==============================] - 595s 34ms/step - loss: 0.1247 - recall: 0.5613 - precision: 0.4413 - f1_score: 0.4929 - val_loss: 0.1211 - val_recall: 0.5677 - val_precision: 0.4681 - val_f1_score: 0.5120\n",
      "Evaluating model...\n",
      "5452/5452 [==============================] - 65s 12ms/step\n",
      "\n",
      "\n",
      "****Fold: 4 ****\n",
      "Splitting into train-test...\n",
      "Tokenizing...\n",
      "Shuffling...\n",
      "Training...\n",
      "Epoch 1/5\n",
      "17446/17446 [==============================] - 598s 34ms/step - loss: 0.2047 - recall: 0.3163 - precision: 0.2643 - f1_score: 0.2851 - val_loss: 0.1778 - val_recall: 0.3869 - val_precision: 0.2972 - val_f1_score: 0.3348\n",
      "Epoch 2/5\n",
      "17446/17446 [==============================] - 594s 34ms/step - loss: 0.1645 - recall: 0.4365 - precision: 0.3210 - f1_score: 0.3684 - val_loss: 0.1494 - val_recall: 0.4786 - val_precision: 0.3616 - val_f1_score: 0.4106\n",
      "Epoch 3/5\n",
      "17446/17446 [==============================] - 595s 34ms/step - loss: 0.1431 - recall: 0.5063 - precision: 0.3800 - f1_score: 0.4328 - val_loss: 0.1352 - val_recall: 0.5134 - val_precision: 0.4193 - val_f1_score: 0.4604\n",
      "Epoch 4/5\n",
      "17446/17446 [==============================] - 594s 34ms/step - loss: 0.1299 - recall: 0.5500 - precision: 0.4221 - f1_score: 0.4764 - val_loss: 0.1244 - val_recall: 0.5624 - val_precision: 0.4477 - val_f1_score: 0.4974\n",
      "Epoch 5/5\n",
      "17446/17446 [==============================] - 595s 34ms/step - loss: 0.1206 - recall: 0.5795 - precision: 0.4523 - f1_score: 0.5068 - val_loss: 0.1180 - val_recall: 0.5785 - val_precision: 0.4782 - val_f1_score: 0.5225\n",
      "Evaluating model...\n",
      "5452/5452 [==============================] - 65s 12ms/step\n",
      "\n",
      "\n",
      "****Fold: 5 ****\n",
      "Splitting into train-test...\n",
      "Tokenizing...\n",
      "Shuffling...\n",
      "Training...\n",
      "Epoch 1/5\n",
      "17446/17446 [==============================] - 598s 34ms/step - loss: 0.2058 - recall: 0.3164 - precision: 0.2607 - f1_score: 0.2831 - val_loss: 0.1772 - val_recall: 0.4004 - val_precision: 0.2812 - val_f1_score: 0.3290\n",
      "Epoch 2/5\n",
      "17446/17446 [==============================] - 595s 34ms/step - loss: 0.1654 - recall: 0.4309 - precision: 0.3170 - f1_score: 0.3637 - val_loss: 0.1507 - val_recall: 0.4815 - val_precision: 0.3503 - val_f1_score: 0.4044\n",
      "Epoch 3/5\n",
      "17446/17446 [==============================] - 595s 34ms/step - loss: 0.1455 - recall: 0.4945 - precision: 0.3672 - f1_score: 0.4200 - val_loss: 0.1369 - val_recall: 0.5240 - val_precision: 0.3986 - val_f1_score: 0.4516\n",
      "Epoch 4/5\n",
      "17446/17446 [==============================] - 594s 34ms/step - loss: 0.1333 - recall: 0.5347 - precision: 0.4049 - f1_score: 0.4595 - val_loss: 0.1276 - val_recall: 0.5523 - val_precision: 0.4317 - val_f1_score: 0.4835\n",
      "Epoch 5/5\n",
      "17446/17446 [==============================] - 594s 34ms/step - loss: 0.1242 - recall: 0.5643 - precision: 0.4356 - f1_score: 0.4904 - val_loss: 0.1209 - val_recall: 0.5648 - val_precision: 0.4698 - val_f1_score: 0.5119\n",
      "Evaluating model...\n",
      "5452/5452 [==============================] - 65s 12ms/step\n",
      "Recall: 0.5861966547675397\n",
      "Precision: 0.5366601064138365\n",
      "F1-Score: 0.5602981401025847\n"
     ]
    }
   ],
   "source": [
    "Kfolds = 5\n",
    "prev_index = 0\n",
    "\n",
    "recs = []\n",
    "precs = []\n",
    "f1s = []\n",
    "\n",
    "for i in range(Kfolds):\n",
    "    print('\\n\\n****Fold:', i+1,'****')\n",
    "    \n",
    "    print('Splitting into train-test...')\n",
    "    X_train_sg, y_train, X_test_sg, y_test, start_index, prev_index = train_test_split(X_sg,y,i,prev_index,Kfolds)\n",
    "\n",
    "    print('Tokenizing...')\n",
    "    X_train_sg, X_test_sg, vocab_size_sg, tokenizer1 = tokenization(X_train_sg, X_test_sg, MAX_LEN_SG)\n",
    "    \n",
    "    print('Shuffling...')\n",
    "    shuffled = [[X_train_sg[i],y_train[i]] for i in range(len(X_train_sg))]\n",
    "    np.random.shuffle(shuffled)\n",
    "\n",
    "    X_train_sg = [shuffled[i][0] for i in range(len(shuffled))]\n",
    "    y_train = [shuffled[i][1] for i in range(len(shuffled))]\n",
    "    X_train_sg = np.array(X_train_sg)\n",
    "    y_train = np.array(y_train)\n",
    "    \n",
    "\n",
    "    model = get_model_sg(vocab_size_sg)\n",
    "    \n",
    "    model.compile(\n",
    "        loss=custom_loss, \n",
    "        optimizer='adam', \n",
    "        metrics=[\n",
    "            recall,\n",
    "            precision,\n",
    "            f1_score\n",
    "        ])\n",
    "    \n",
    "    print('Training...')\n",
    "    history = model.fit(X_train_sg, y_train, batch_size=32, epochs=5,validation_split=0.2)\n",
    "    \n",
    "    print('Evaluating model...')\n",
    "    predictions = model.predict(X_test_sg)\n",
    "    \n",
    "    rec, prec, f1 = compute_metrics(predictions, start_index, prev_index)\n",
    "    \n",
    "    recs.append(rec.numpy())\n",
    "    precs.append(prec.numpy())\n",
    "    f1s.append(f1.numpy())\n",
    "\n",
    "print('Recall:',sum(recs)/len(recs))\n",
    "print('Precision:',sum(precs)/len(precs))\n",
    "print('F1-Score:',sum(f1s)/len(f1s))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84680b09",
   "metadata": {},
   "source": [
    "### Model using skip grams and ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "6163d660",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Considering ngrams and skip grams\n",
    "X_ng = [' '.join(sample[1]) for sample in training_data_ngrams]\n",
    "X_sg = [' '.join(sample[1]) for sample in training_data_skip_grams]\n",
    "y = [sample[2] for sample in training_data]\n",
    "\n",
    "del training_data_ngrams\n",
    "del training_data_skip_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "32366243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "872283 872283\n"
     ]
    }
   ],
   "source": [
    "print(len(X_ng),len(X_sg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "eaad7bdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "****Fold: 1 ****\n",
      "Splitting into train-test...\n",
      "Tokenizing...\n",
      "Shuffling...\n",
      "Training...\n",
      "Epoch 1/5\n",
      "17446/17446 [==============================] - 774s 44ms/step - loss: 0.1945 - recall: 0.3513 - precision: 0.2789 - f1_score: 0.3085 - val_loss: 0.1623 - val_recall: 0.4466 - val_precision: 0.3124 - val_f1_score: 0.3664\n",
      "Epoch 2/5\n",
      "17446/17446 [==============================] - 772s 44ms/step - loss: 0.1513 - recall: 0.4778 - precision: 0.3588 - f1_score: 0.4084 - val_loss: 0.1371 - val_recall: 0.5178 - val_precision: 0.4098 - val_f1_score: 0.4563\n",
      "Epoch 3/5\n",
      "17446/17446 [==============================] - 771s 44ms/step - loss: 0.1320 - recall: 0.5416 - precision: 0.4215 - f1_score: 0.4728 - val_loss: 0.1249 - val_recall: 0.5547 - val_precision: 0.4598 - val_f1_score: 0.5017\n",
      "Epoch 4/5\n",
      "17446/17446 [==============================] - 771s 44ms/step - loss: 0.1200 - recall: 0.5810 - precision: 0.4604 - f1_score: 0.5126 - val_loss: 0.1164 - val_recall: 0.5815 - val_precision: 0.4908 - val_f1_score: 0.5313\n",
      "Epoch 5/5\n",
      "17446/17446 [==============================] - 770s 44ms/step - loss: 0.1119 - recall: 0.6068 - precision: 0.4856 - f1_score: 0.5383 - val_loss: 0.1113 - val_recall: 0.6003 - val_precision: 0.5039 - val_f1_score: 0.5469\n",
      "Evaluating model...\n",
      "5452/5452 [==============================] - 83s 15ms/step\n",
      "\n",
      "\n",
      "****Fold: 2 ****\n",
      "Splitting into train-test...\n",
      "Tokenizing...\n",
      "Shuffling...\n",
      "Training...\n",
      "Epoch 1/5\n",
      "17446/17446 [==============================] - 775s 44ms/step - loss: 0.1934 - recall: 0.3567 - precision: 0.2790 - f1_score: 0.3103 - val_loss: 0.1611 - val_recall: 0.4364 - val_precision: 0.3297 - val_f1_score: 0.3742\n",
      "Epoch 2/5\n",
      "17446/17446 [==============================] - 768s 44ms/step - loss: 0.1487 - recall: 0.4956 - precision: 0.3585 - f1_score: 0.4146 - val_loss: 0.1354 - val_recall: 0.5293 - val_precision: 0.4097 - val_f1_score: 0.4607\n",
      "Epoch 3/5\n",
      "17446/17446 [==============================] - 769s 44ms/step - loss: 0.1280 - recall: 0.5654 - precision: 0.4287 - f1_score: 0.4863 - val_loss: 0.1218 - val_recall: 0.5780 - val_precision: 0.4588 - val_f1_score: 0.5105\n",
      "Epoch 4/5\n",
      "17446/17446 [==============================] - 769s 44ms/step - loss: 0.1143 - recall: 0.6108 - precision: 0.4755 - f1_score: 0.5336 - val_loss: 0.1123 - val_recall: 0.6175 - val_precision: 0.4906 - val_f1_score: 0.5457\n",
      "Epoch 5/5\n",
      "17446/17446 [==============================] - 770s 44ms/step - loss: 0.1042 - recall: 0.6442 - precision: 0.5072 - f1_score: 0.5664 - val_loss: 0.1055 - val_recall: 0.6396 - val_precision: 0.5165 - val_f1_score: 0.5704\n",
      "Evaluating model...\n",
      "5452/5452 [==============================] - 83s 15ms/step\n",
      "\n",
      "\n",
      "****Fold: 3 ****\n",
      "Splitting into train-test...\n",
      "Tokenizing...\n",
      "Shuffling...\n",
      "Training...\n",
      "Epoch 1/5\n",
      "17446/17446 [==============================] - 768s 44ms/step - loss: 0.1910 - recall: 0.3554 - precision: 0.2911 - f1_score: 0.3177 - val_loss: 0.1593 - val_recall: 0.4386 - val_precision: 0.3503 - val_f1_score: 0.3882\n",
      "Epoch 2/5\n",
      "17446/17446 [==============================] - 765s 44ms/step - loss: 0.1486 - recall: 0.4843 - precision: 0.3744 - f1_score: 0.4209 - val_loss: 0.1359 - val_recall: 0.5144 - val_precision: 0.4231 - val_f1_score: 0.4631\n",
      "Epoch 3/5\n",
      "17446/17446 [==============================] - 766s 44ms/step - loss: 0.1292 - recall: 0.5494 - precision: 0.4326 - f1_score: 0.4828 - val_loss: 0.1232 - val_recall: 0.5578 - val_precision: 0.4706 - val_f1_score: 0.5095\n",
      "Epoch 4/5\n",
      "17446/17446 [==============================] - 766s 44ms/step - loss: 0.1165 - recall: 0.5924 - precision: 0.4694 - f1_score: 0.5226 - val_loss: 0.1150 - val_recall: 0.5878 - val_precision: 0.4952 - val_f1_score: 0.5365\n",
      "Epoch 5/5\n",
      "17446/17446 [==============================] - 765s 44ms/step - loss: 0.1074 - recall: 0.6222 - precision: 0.4946 - f1_score: 0.5500 - val_loss: 0.1085 - val_recall: 0.6140 - val_precision: 0.5140 - val_f1_score: 0.5585\n",
      "Evaluating model...\n",
      "5452/5452 [==============================] - 83s 15ms/step\n",
      "\n",
      "\n",
      "****Fold: 4 ****\n",
      "Splitting into train-test...\n",
      "Tokenizing...\n",
      "Shuffling...\n",
      "Training...\n",
      "Epoch 1/5\n",
      "17446/17446 [==============================] - 773s 44ms/step - loss: 0.1958 - recall: 0.3525 - precision: 0.2807 - f1_score: 0.3097 - val_loss: 0.1638 - val_recall: 0.4494 - val_precision: 0.3188 - val_f1_score: 0.3717\n",
      "Epoch 2/5\n",
      "17446/17446 [==============================] - 768s 44ms/step - loss: 0.1510 - recall: 0.4835 - precision: 0.3569 - f1_score: 0.4092 - val_loss: 0.1385 - val_recall: 0.5309 - val_precision: 0.3932 - val_f1_score: 0.4506\n",
      "Epoch 3/5\n",
      "17446/17446 [==============================] - 767s 44ms/step - loss: 0.1301 - recall: 0.5532 - precision: 0.4228 - f1_score: 0.4780 - val_loss: 0.1229 - val_recall: 0.5679 - val_precision: 0.4585 - val_f1_score: 0.5062\n",
      "Epoch 4/5\n",
      "17446/17446 [==============================] - 769s 44ms/step - loss: 0.1161 - recall: 0.5986 - precision: 0.4692 - f1_score: 0.5249 - val_loss: 0.1140 - val_recall: 0.5952 - val_precision: 0.4929 - val_f1_score: 0.5381\n",
      "Epoch 5/5\n",
      "17446/17446 [==============================] - 767s 44ms/step - loss: 0.1061 - recall: 0.6303 - precision: 0.5013 - f1_score: 0.5573 - val_loss: 0.1074 - val_recall: 0.6193 - val_precision: 0.5146 - val_f1_score: 0.5611\n",
      "Evaluating model...\n",
      "5452/5452 [==============================] - 83s 15ms/step\n",
      "\n",
      "\n",
      "****Fold: 5 ****\n",
      "Splitting into train-test...\n",
      "Tokenizing...\n",
      "Shuffling...\n",
      "Training...\n",
      "Epoch 1/5\n",
      "17446/17446 [==============================] - 773s 44ms/step - loss: 0.1973 - recall: 0.3487 - precision: 0.2725 - f1_score: 0.3031 - val_loss: 0.1711 - val_recall: 0.4204 - val_precision: 0.2986 - val_f1_score: 0.3479\n",
      "Epoch 2/5\n",
      "17446/17446 [==============================] - 767s 44ms/step - loss: 0.1578 - recall: 0.4628 - precision: 0.3324 - f1_score: 0.3853 - val_loss: 0.1447 - val_recall: 0.5022 - val_precision: 0.3699 - val_f1_score: 0.4246\n",
      "Epoch 3/5\n",
      "17446/17446 [==============================] - 767s 44ms/step - loss: 0.1369 - recall: 0.5302 - precision: 0.3979 - f1_score: 0.4532 - val_loss: 0.1290 - val_recall: 0.5578 - val_precision: 0.4270 - val_f1_score: 0.4825\n",
      "Epoch 4/5\n",
      "17446/17446 [==============================] - 767s 44ms/step - loss: 0.1223 - recall: 0.5783 - precision: 0.4454 - f1_score: 0.5019 - val_loss: 0.1190 - val_recall: 0.5785 - val_precision: 0.4734 - val_f1_score: 0.5195\n",
      "Epoch 5/5\n",
      "17446/17446 [==============================] - 767s 44ms/step - loss: 0.1117 - recall: 0.6122 - precision: 0.4805 - f1_score: 0.5372 - val_loss: 0.1122 - val_recall: 0.6117 - val_precision: 0.4888 - val_f1_score: 0.5423\n",
      "Evaluating model...\n",
      "5452/5452 [==============================] - 83s 15ms/step\n",
      "Recall: 0.620352705164833\n",
      "Precision: 0.5654152267050204\n",
      "F1-Score: 0.5915155692526847\n"
     ]
    }
   ],
   "source": [
    "Kfolds = 5\n",
    "prev_index = 0\n",
    "\n",
    "recs = []\n",
    "precs = []\n",
    "f1s = []\n",
    "\n",
    "for i in range(Kfolds):\n",
    "    print('\\n\\n****Fold:', i+1,'****')\n",
    "    \n",
    "    print('Splitting into train-test...')\n",
    "    X_train_ng, y_train, X_test_ng, y_test, start_index, prev_index1 = train_test_split(X_ng,y,i,prev_index,Kfolds)\n",
    "    X_train_sg, _, X_test_sg, _, _, _= train_test_split(X_sg,y,i,prev_index,Kfolds)\n",
    "    \n",
    "    prev_index = prev_index1\n",
    "    \n",
    "    print('Tokenizing...')\n",
    "    X_train_ng, X_test_ng, vocab_size_ng, tokenizer1 = tokenization(X_train_ng, X_test_ng, MAX_LEN_NG)  \n",
    "    X_train_sg, X_test_sg, vocab_size_sg, tokenizer2 = tokenization(X_train_sg, X_test_sg, MAX_LEN_SG)\n",
    "    \n",
    "   \n",
    "    print('Shuffling...')   \n",
    "    shuffled = [[X_train_ng[i],X_train_sg[i],y_train[i]] for i in range(len(X_train_ng))]\n",
    "    np.random.shuffle(shuffled)\n",
    "\n",
    "    X_train_ng = [shuffled[i][0] for i in range(len(shuffled))]\n",
    "    X_train_sg = [shuffled[i][1] for i in range(len(shuffled))]\n",
    "    y_train = [shuffled[i][2] for i in range(len(shuffled))]\n",
    "    X_train_ng = np.array(X_train_ng)\n",
    "    X_train_sg = np.array(X_train_sg)\n",
    "    y_train = np.array(y_train)\n",
    "    \n",
    "    \n",
    "    model = get_model_ng_sg(vocab_size_ng, vocab_size_sg)\n",
    "    \n",
    "    model.compile(\n",
    "        loss=custom_loss, \n",
    "        optimizer='adam', \n",
    "        metrics=[\n",
    "            recall,\n",
    "            precision,\n",
    "            f1_score\n",
    "        ])\n",
    "\n",
    "    print('Training...')\n",
    "    history = model.fit([X_train_ng,X_train_sg], y_train, batch_size=32, epochs=5,validation_split=0.2)\n",
    "    \n",
    "    print('Evaluating model...')\n",
    "    predictions = model.predict([X_test_ng,X_test_sg])\n",
    "    \n",
    "    rec, prec, f1 = compute_metrics(predictions, start_index, prev_index)\n",
    "    \n",
    "    recs.append(rec.numpy())\n",
    "    precs.append(prec.numpy())\n",
    "    f1s.append(f1.numpy())\n",
    "\n",
    "print('Recall:',sum(recs)/len(recs))\n",
    "print('Precision:',sum(precs)/len(precs))\n",
    "print('F1-Score:',sum(f1s)/len(f1s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d3bd13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
