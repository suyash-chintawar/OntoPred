{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a056c8a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jan 12 18:55:58 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.106.00   Driver Version: 460.106.00   CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-DGXS...  Off  | 00000000:07:00.0 Off |                    0 |\n",
      "| N/A   45C    P0    54W / 300W |  29954MiB / 32508MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-DGXS...  Off  | 00000000:08:00.0 Off |                    0 |\n",
      "| N/A   51C    P0   282W / 300W |  18146MiB / 32508MiB |     99%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla V100-DGXS...  Off  | 00000000:0E:00.0 Off |                    0 |\n",
      "| N/A   51C    P0   273W / 300W |  15932MiB / 32508MiB |     99%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla V100-DGXS...  Off  | 00000000:0F:00.0 Off |                    0 |\n",
      "| N/A   44C    P0    41W / 300W |      3MiB / 32508MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A    342074      C   /usr/bin/python3                29951MiB |\n",
      "|    1   N/A  N/A    342417      C   python                          18143MiB |\n",
      "|    2   N/A  N/A    342417      C   python                          15929MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86add81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fb10a5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-12 18:56:10.005210: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-12 18:56:10.301316: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 15649749367647460464\n",
      "xla_global_id: -1\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 32476168192\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 10653532408418105748\n",
      "physical_device_desc: \"device: 0, name: Tesla V100-DGXS-32GB, pci bus id: 0000:0f:00.0, compute capability: 7.0\"\n",
      "xla_global_id: 416903419\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-12 18:56:14.424589: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-12 18:56:15.282216: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /device:GPU:0 with 30971 MB memory:  -> device: 0, name: Tesla V100-DGXS-32GB, pci bus id: 0000:0f:00.0, compute capability: 7.0\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8be3b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import gdown\n",
    "import numpy as np\n",
    "import pandas  as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import tqdm.notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e38cc5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "aspect = ['biological','process']\n",
    "aspect_abbr = 'bp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ce40988",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1RyeLQPFTMWAIr-OzELTWIx60ln-mZ7g_\n",
      "To: /home/191it109/project/home/biological_process.csv\n",
      "100%|██████████████████████████████████████| 34.6M/34.6M [00:00<00:00, 39.7MB/s]\n"
     ]
    }
   ],
   "source": [
    "file = '_'.join(aspect)+'.csv'\n",
    "if not os.path.exists(file):\n",
    "    url = \"https://drive.google.com/file/d/1RyeLQPFTMWAIr-OzELTWIx60ln-mZ7g_/view?usp=sharing\"\n",
    "    output = file\n",
    "    gdown.download(url=url, output=output, quiet=False, fuzzy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd949fb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Entry</th>\n",
       "      <th>Entry Name</th>\n",
       "      <th>Organism</th>\n",
       "      <th>Length</th>\n",
       "      <th>Sequence</th>\n",
       "      <th>Gene Ontology (biological process)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>F4KFS5</td>\n",
       "      <td>GMI1_ARATH</td>\n",
       "      <td>Arabidopsis thaliana (Mouse-ear cress)</td>\n",
       "      <td>1598</td>\n",
       "      <td>MSSRRSVKRSLVLDDDDDEDIFYNFKVLLPNGTSVKLTLKNPEPEI...</td>\n",
       "      <td>GO:0000724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B4ESY8</td>\n",
       "      <td>URK_PROMH</td>\n",
       "      <td>Proteus mirabilis (strain HI4320)</td>\n",
       "      <td>213</td>\n",
       "      <td>MADTAHQCTIVGIAGASASGKSLIASTLYRELRAQVGDHNIGVIPE...</td>\n",
       "      <td>GO:0016310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Q9H479</td>\n",
       "      <td>FN3K_HUMAN</td>\n",
       "      <td>Homo sapiens (Human)</td>\n",
       "      <td>309</td>\n",
       "      <td>MEQLLRAELRTATLRAFGGPGAGCISEGRAYDTDAGPVFVKVNRRT...</td>\n",
       "      <td>GO:0016310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B4PRE2</td>\n",
       "      <td>DGKH_DROYA</td>\n",
       "      <td>Drosophila yakuba (Fruit fly)</td>\n",
       "      <td>1917</td>\n",
       "      <td>MSHLKLDTLHVQRSPRGSRRSSRSSGRSSACSSGSISPVPIIPIIS...</td>\n",
       "      <td>GO:0016310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>P17129</td>\n",
       "      <td>PSPB_CANLF</td>\n",
       "      <td>Canis lupus familiaris (Dog) (Canis familiaris)</td>\n",
       "      <td>363</td>\n",
       "      <td>LLWLLLLPTLCGLGAADWSAPSLACARGPAFWCQSLEQALQCRALG...</td>\n",
       "      <td>GO:0006629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72465</th>\n",
       "      <td>P44371</td>\n",
       "      <td>RS2_HAEIN</td>\n",
       "      <td>Haemophilus influenzae (strain ATCC 51907 / DS...</td>\n",
       "      <td>240</td>\n",
       "      <td>MAQVSMRDMINAGVHFGHQTRYWNPQMKPFIFGARNGVHIINLEKT...</td>\n",
       "      <td>GO:0006412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72466</th>\n",
       "      <td>A1V880</td>\n",
       "      <td>RS13_BURMS</td>\n",
       "      <td>Burkholderia mallei (strain SAVP1)</td>\n",
       "      <td>121</td>\n",
       "      <td>MARIAGVNIPNHQHTEIGLTAIFGIGRTRARSICVASGVAFSKKVK...</td>\n",
       "      <td>GO:0006412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72467</th>\n",
       "      <td>Q04955</td>\n",
       "      <td>FLIG_CAUVC</td>\n",
       "      <td>Caulobacter vibrioides (strain ATCC 19089 / CB...</td>\n",
       "      <td>340</td>\n",
       "      <td>MAMKLAVNDVKNLSGPEKAAIVLLALGEDHTRIWEALDDEEIKEVS...</td>\n",
       "      <td>GO:0006935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72468</th>\n",
       "      <td>B5R0P3</td>\n",
       "      <td>PURA_SALEP</td>\n",
       "      <td>Salmonella enteritidis PT4 (strain P125109)</td>\n",
       "      <td>432</td>\n",
       "      <td>MGNNVVVLGTQWGDEGKGKIVDLLTERAKYVVRYQGGHNAGHTLVI...</td>\n",
       "      <td>GO:0044208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72469</th>\n",
       "      <td>Q80W96</td>\n",
       "      <td>ISCA1_RAT</td>\n",
       "      <td>Rattus norvegicus (Rat)</td>\n",
       "      <td>129</td>\n",
       "      <td>MSASLVRATVRAVSKRKLQPTRAALTLTPSAVNKIKQLLKDKPEHV...</td>\n",
       "      <td>GO:0016226</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>72470 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Entry  Entry Name                                           Organism  \\\n",
       "0      F4KFS5  GMI1_ARATH             Arabidopsis thaliana (Mouse-ear cress)   \n",
       "1      B4ESY8   URK_PROMH                  Proteus mirabilis (strain HI4320)   \n",
       "2      Q9H479  FN3K_HUMAN                               Homo sapiens (Human)   \n",
       "3      B4PRE2  DGKH_DROYA                      Drosophila yakuba (Fruit fly)   \n",
       "4      P17129  PSPB_CANLF    Canis lupus familiaris (Dog) (Canis familiaris)   \n",
       "...       ...         ...                                                ...   \n",
       "72465  P44371   RS2_HAEIN  Haemophilus influenzae (strain ATCC 51907 / DS...   \n",
       "72466  A1V880  RS13_BURMS                 Burkholderia mallei (strain SAVP1)   \n",
       "72467  Q04955  FLIG_CAUVC  Caulobacter vibrioides (strain ATCC 19089 / CB...   \n",
       "72468  B5R0P3  PURA_SALEP        Salmonella enteritidis PT4 (strain P125109)   \n",
       "72469  Q80W96   ISCA1_RAT                            Rattus norvegicus (Rat)   \n",
       "\n",
       "       Length                                           Sequence  \\\n",
       "0        1598  MSSRRSVKRSLVLDDDDDEDIFYNFKVLLPNGTSVKLTLKNPEPEI...   \n",
       "1         213  MADTAHQCTIVGIAGASASGKSLIASTLYRELRAQVGDHNIGVIPE...   \n",
       "2         309  MEQLLRAELRTATLRAFGGPGAGCISEGRAYDTDAGPVFVKVNRRT...   \n",
       "3        1917  MSHLKLDTLHVQRSPRGSRRSSRSSGRSSACSSGSISPVPIIPIIS...   \n",
       "4         363  LLWLLLLPTLCGLGAADWSAPSLACARGPAFWCQSLEQALQCRALG...   \n",
       "...       ...                                                ...   \n",
       "72465     240  MAQVSMRDMINAGVHFGHQTRYWNPQMKPFIFGARNGVHIINLEKT...   \n",
       "72466     121  MARIAGVNIPNHQHTEIGLTAIFGIGRTRARSICVASGVAFSKKVK...   \n",
       "72467     340  MAMKLAVNDVKNLSGPEKAAIVLLALGEDHTRIWEALDDEEIKEVS...   \n",
       "72468     432  MGNNVVVLGTQWGDEGKGKIVDLLTERAKYVVRYQGGHNAGHTLVI...   \n",
       "72469     129  MSASLVRATVRAVSKRKLQPTRAALTLTPSAVNKIKQLLKDKPEHV...   \n",
       "\n",
       "      Gene Ontology (biological process)  \n",
       "0                             GO:0000724  \n",
       "1                             GO:0016310  \n",
       "2                             GO:0016310  \n",
       "3                             GO:0016310  \n",
       "4                             GO:0006629  \n",
       "...                                  ...  \n",
       "72465                         GO:0006412  \n",
       "72466                         GO:0006412  \n",
       "72467                         GO:0006935  \n",
       "72468                         GO:0044208  \n",
       "72469                         GO:0016226  \n",
       "\n",
       "[72470 rows x 6 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(file)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "724323ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201\n",
      "['GO:0000027', 'GO:0000103', 'GO:0000105', 'GO:0000122', 'GO:0000162', 'GO:0000209', 'GO:0000398', 'GO:0000724', 'GO:0000902', 'GO:0000917', 'GO:0001525', 'GO:0001934', 'GO:0002098', 'GO:0002181', 'GO:0002250', 'GO:0002949', 'GO:0005975', 'GO:0005978', 'GO:0006006', 'GO:0006094', 'GO:0006096', 'GO:0006099', 'GO:0006164', 'GO:0006166', 'GO:0006189', 'GO:0006207', 'GO:0006226', 'GO:0006235', 'GO:0006241', 'GO:0006260', 'GO:0006275', 'GO:0006281', 'GO:0006284', 'GO:0006289', 'GO:0006298', 'GO:0006310', 'GO:0006325', 'GO:0006338', 'GO:0006351', 'GO:0006353', 'GO:0006355', 'GO:0006357', 'GO:0006364', 'GO:0006366', 'GO:0006396', 'GO:0006397', 'GO:0006400', 'GO:0006401', 'GO:0006402', 'GO:0006412', 'GO:0006417', 'GO:0006420', 'GO:0006424', 'GO:0006426', 'GO:0006432', 'GO:0006433', 'GO:0006457', 'GO:0006468', 'GO:0006470', 'GO:0006508', 'GO:0006511', 'GO:0006520', 'GO:0006526', 'GO:0006541', 'GO:0006556', 'GO:0006605', 'GO:0006629', 'GO:0006633', 'GO:0006730', 'GO:0006744', 'GO:0006777', 'GO:0006782', 'GO:0006807', 'GO:0006811', 'GO:0006814', 'GO:0006886', 'GO:0006897', 'GO:0006914', 'GO:0006915', 'GO:0006935', 'GO:0006952', 'GO:0006954', 'GO:0006955', 'GO:0006974', 'GO:0006979', 'GO:0007049', 'GO:0007059', 'GO:0007155', 'GO:0007165', 'GO:0007186', 'GO:0007283', 'GO:0007399', 'GO:0007420', 'GO:0007507', 'GO:0008033', 'GO:0008284', 'GO:0008285', 'GO:0008360', 'GO:0008380', 'GO:0008615', 'GO:0008616', 'GO:0008652', 'GO:0008654', 'GO:0009073', 'GO:0009086', 'GO:0009089', 'GO:0009097', 'GO:0009098', 'GO:0009099', 'GO:0009102', 'GO:0009103', 'GO:0009117', 'GO:0009228', 'GO:0009229', 'GO:0009231', 'GO:0009234', 'GO:0009236', 'GO:0009245', 'GO:0009249', 'GO:0009252', 'GO:0009408', 'GO:0009410', 'GO:0009423', 'GO:0009432', 'GO:0009435', 'GO:0010468', 'GO:0010628', 'GO:0010629', 'GO:0015031', 'GO:0015937', 'GO:0015940', 'GO:0015979', 'GO:0016042', 'GO:0016055', 'GO:0016114', 'GO:0016192', 'GO:0016226', 'GO:0016260', 'GO:0016310', 'GO:0016311', 'GO:0016477', 'GO:0016567', 'GO:0017038', 'GO:0018105', 'GO:0019062', 'GO:0019253', 'GO:0019264', 'GO:0019284', 'GO:0019288', 'GO:0019464', 'GO:0019509', 'GO:0019684', 'GO:0019877', 'GO:0022904', 'GO:0030091', 'GO:0030154', 'GO:0030163', 'GO:0030435', 'GO:0030490', 'GO:0031119', 'GO:0032259', 'GO:0035556', 'GO:0035999', 'GO:0042026', 'GO:0042254', 'GO:0042274', 'GO:0042450', 'GO:0042742', 'GO:0042773', 'GO:0042777', 'GO:0043065', 'GO:0043066', 'GO:0043093', 'GO:0043161', 'GO:0043419', 'GO:0044205', 'GO:0044208', 'GO:0044209', 'GO:0044210', 'GO:0045087', 'GO:0045727', 'GO:0045892', 'GO:0045893', 'GO:0045944', 'GO:0046677', 'GO:0046718', 'GO:0046777', 'GO:0050821', 'GO:0051301', 'GO:0051321', 'GO:0051607', 'GO:0051726', 'GO:0055085', 'GO:0055129', 'GO:0060271', 'GO:0065002', 'GO:0070475', 'GO:0070814', 'GO:0071555', 'GO:0097056', 'GO:2001295']\n"
     ]
    }
   ],
   "source": [
    "go_terms_bp = set()\n",
    "for idx, row in df.iterrows():\n",
    "    for term in row['Gene Ontology ('+' '.join(aspect)+')'].split(';'):\n",
    "        go_terms_bp.add(term)\n",
    "go_terms_bp = list(go_terms_bp)\n",
    "go_terms_bp.sort()\n",
    "print(len(go_terms_bp))\n",
    "print(go_terms_bp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72f8a1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_segments(sequence,segment_size=100,gap=30):\n",
    "    segments = []\n",
    "    start = 0\n",
    "    end = segment_size\n",
    "    while end <= len(sequence):\n",
    "        segments.append(sequence[start:end])\n",
    "        start += gap\n",
    "        end += gap\n",
    "    last_segment = sequence[start:]\n",
    "    segments.append(last_segment)\n",
    "    \n",
    "    return segments\n",
    "\n",
    "def get_training_data(df,segment_size=100,gap=30):\n",
    "    training_data = list()\n",
    "    for idx,row in tqdm.tqdm(df.iterrows()):\n",
    "        labels = [0] * len(go_terms_bp)\n",
    "        for term in row['Gene Ontology ('+' '.join(aspect)+')'].split(';'):\n",
    "            labels[go_terms_bp.index(term)] = 1\n",
    "        segments = get_segments(row['Sequence'],segment_size,gap)\n",
    "        for segment in segments:\n",
    "            training_data.append([row['Entry'],segment,labels])\n",
    "    return training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f1d6c8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdbc7559415a4414a566b78699e59549",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "794486\n"
     ]
    }
   ],
   "source": [
    "training_data = get_training_data(df)\n",
    "print(len(training_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1c0991f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngrams(segment,n=3):\n",
    "    ngrams = []\n",
    "    for i in range(len(segment)-n+1):\n",
    "        ngrams.append(segment[i:i+n])\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5358736d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading saved ngrams...\n",
      "794486\n"
     ]
    }
   ],
   "source": [
    "# Generate training data of ngrams\n",
    "if os.path.exists('bp/training_data_3grams.npy'):\n",
    "    print('Loading saved ngrams...')\n",
    "    training_data_ngrams = np.load('bp/training_data_3grams.npy',allow_pickle=True)\n",
    "else:\n",
    "    print('Preparing from scratch...')\n",
    "    training_data_ngrams = []\n",
    "\n",
    "    for i in tqdm.tqdm(range(len(training_data))):\n",
    "        training_data_ngrams.append([training_data[i][0],get_ngrams(training_data[i][1],n=3),training_data[i][2]])\n",
    "        \n",
    "    np.save('bp/training_data_3grams.npy',training_data_ngrams)\n",
    "    \n",
    "print(len(training_data_ngrams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4acd14a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_skip_grams(segment,skip=1,n=3):\n",
    "    skip_grams = []\n",
    "    window_size = skip + n\n",
    "    for i in range(len(segment)-window_size+1):\n",
    "        window = segment[i:i+window_size]\n",
    "        indices = list(range(window_size))\n",
    "        indices.pop(0)\n",
    "        for idx in indices[::-1]:\n",
    "            temp = ''\n",
    "            for j in range(window_size):\n",
    "                if j!=idx:\n",
    "                    temp+=window[j]\n",
    "            skip_grams.append(temp)\n",
    "\n",
    "    return skip_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fa91e8e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading saved skip grams...\n",
      "794486\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists('bp/training_data_skip1_3grams.npy'):\n",
    "    print('Loading saved skip grams...')\n",
    "    training_data_skip_grams = np.load('bp/training_data_skip1_3grams.npy',allow_pickle=True)\n",
    "else:\n",
    "    print('Preparing from scratch...')\n",
    "    training_data_skip_grams = []\n",
    "    for i in tqdm.tqdm(range(len(training_data))):\n",
    "        training_data_skip_grams.append([training_data[i][0],get_skip_grams(training_data[i][1],n=3),training_data[i][2]])\n",
    "    np.save('bp/training_data_skip1_3grams.npy',training_data_skip_grams)\n",
    "print(len(training_data_skip_grams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "11d856c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0e391f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assuming training_data as global variable\n",
    "\n",
    "def train_test_split(X,y,fold_no,prev_index,Kfolds=5):\n",
    "    test_split = 1/Kfolds\n",
    "    \n",
    "    start_index = prev_index\n",
    "    end_index = (fold_no + 1) * (test_split) * len(X)\n",
    "    end_index = round(end_index)\n",
    "    \n",
    "    if end_index==len(X):\n",
    "        end_index -= 1\n",
    "    \n",
    "    entry = training_data[end_index][0]\n",
    "    entries = [sample[0] for sample in training_data]\n",
    "    \n",
    "    first_occurence = entries.index(entry)\n",
    "    entries.reverse()\n",
    "    \n",
    "    last_occurence = entries.index(entry)\n",
    "    last_occurence = len(entries) - last_occurence - 1\n",
    "    \n",
    "    del entries\n",
    "    gc.collect()\n",
    "    \n",
    "    end_index = first_occurence if (abs(end_index-first_occurence) < abs(end_index-last_occurence)) else last_occurence\n",
    "    \n",
    "    X_test = X[start_index:end_index+1]\n",
    "    y_test = y[start_index:end_index+1]\n",
    "    X_train = X[:start_index]\n",
    "    X_train.extend(X[end_index+1:])\n",
    "    y_train = y[:start_index]\n",
    "    y_train.extend(y[end_index+1:])\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test, start_index, end_index + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "730f9429",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_WORDS = 13824\n",
    "MAX_LEN_NG = 98 #100\n",
    "MAX_LEN_SG = 291 #300\n",
    "\n",
    "def tokenization(X_train,X_test,maxlen):\n",
    "\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=MAX_WORDS)\n",
    "    tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "    X_train = tokenizer.texts_to_sequences(X_train)\n",
    "    X_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "    X_train = tf.keras.preprocessing.sequence.pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
    "    X_test = tf.keras.preprocessing.sequence.pad_sequences(X_test, padding='post', maxlen=maxlen)\n",
    "    \n",
    "    return X_train, X_test, vocab_size, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "10a7f090",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "\n",
    "false_negative_penalty = 6\n",
    "false_positive_penalty = 1\n",
    "\n",
    "def custom_loss(y_true, y_logit):\n",
    "\n",
    "    loss = float(0)\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_logit = tf.cast(y_logit, tf.float32)\n",
    "    \n",
    "    first_term = false_negative_penalty * float(y_true) * - K.log(y_logit + K.epsilon())\n",
    "    second_term = false_positive_penalty * (1 - float(y_true)) * - K.log(1 - y_logit + K.epsilon())\n",
    "    \n",
    "    loss = K.mean(first_term+second_term)\n",
    "\n",
    "    return loss\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    \n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)), axis=1)\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)), axis=1)\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    \n",
    "    return K.mean(precision)\n",
    "\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)), axis=1)\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)), axis=1)\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return K.mean(recall)\n",
    "\n",
    "def f1_score(y_true, y_pred):\n",
    "    rec = recall(y_true,y_pred)\n",
    "    prec = precision(y_true,y_pred)\n",
    "    f1 = 2*prec*rec/(prec+rec)\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "36c30cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class attention(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, return_sequences=True,**kwargs):\n",
    "        self.return_sequences = return_sequences\n",
    "        super(attention,self).__init__()\n",
    "\n",
    "    def get_config(self):\n",
    "      config = super().get_config().copy()\n",
    "      config.update({\n",
    "          'return_sequences': self.return_sequences \n",
    "      })\n",
    "      return config\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        \n",
    "        self.W=self.add_weight(name=\"att_weight\", shape=(input_shape[-1],1),\n",
    "                               initializer=\"normal\")\n",
    "        self.b=self.add_weight(name=\"att_bias\", shape=(input_shape[1],1),\n",
    "                               initializer=\"zeros\")\n",
    "        \n",
    "        super(attention,self).build(input_shape)\n",
    "        \n",
    "    def call(self, x):\n",
    "        \n",
    "        e = K.tanh(K.dot(x,self.W)+self.b)\n",
    "        a = K.softmax(e, axis=1)\n",
    "        output = x*a\n",
    "        \n",
    "        if self.return_sequences:\n",
    "            return output\n",
    "        \n",
    "        return K.sum(output, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b6dbee46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Base model using ngrams \n",
    "NUM_CLASSES = 201\n",
    "\n",
    "def get_model_ng(vocab_size_ng):\n",
    "    input_ngrams = tf.keras.layers.Input(shape=(MAX_LEN_NG,))\n",
    "\n",
    "    embedding_layer = tf.keras.layers.Embedding(vocab_size_ng, 32)(input_ngrams)\n",
    "\n",
    "    LSTM_Layer_1 = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(70,return_sequences=True))(embedding_layer)\n",
    "    attention_output_1 = attention(return_sequences=False)(LSTM_Layer_1)\n",
    "    dropout = tf.keras.layers.Dropout(0.3)(attention_output_1)\n",
    "    dense_layer_1 = tf.keras.layers.Dense(NUM_CLASSES, activation='sigmoid')(dropout)\n",
    "\n",
    "    model = tf.keras.models.Model(\n",
    "        inputs=input_ngrams, \n",
    "        outputs=dense_layer_1\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "014f93a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Base model using skip grams \n",
    "NUM_CLASSES = 201\n",
    "\n",
    "def get_model_sg(vocab_size_sg):\n",
    "    input_skip_grams = tf.keras.layers.Input(shape=(MAX_LEN_SG,))\n",
    "\n",
    "    embedding_layer = tf.keras.layers.Embedding(vocab_size_sg, 32)(input_skip_grams)\n",
    "\n",
    "    LSTM_Layer_1 = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(70,return_sequences=True))(embedding_layer)\n",
    "    attention_output_1 = attention(return_sequences=False)(LSTM_Layer_1)\n",
    "    dropout = tf.keras.layers.Dropout(0.3)(attention_output_1)\n",
    "    dense_layer_1 = tf.keras.layers.Dense(NUM_CLASSES, activation='sigmoid')(dropout)\n",
    "\n",
    "    model = tf.keras.models.Model(\n",
    "        inputs=input_skip_grams, \n",
    "        outputs=dense_layer_1\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "03b912ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 201 #For bp (Change according to aspects)\n",
    "\n",
    "def get_model_ng_sg(vocab_size_ng, vocab_size_sg):\n",
    "    #Input layers\n",
    "\n",
    "    input_ngrams = tf.keras.layers.Input(shape=(MAX_LEN_NG,)) \n",
    "    input_skip_grams = tf.keras.layers.Input(shape=(MAX_LEN_SG,)) \n",
    "\n",
    "    #embeddings\n",
    "    embedding_layer_ngrams = tf.keras.layers.Embedding(vocab_size_ng, 32)(input_ngrams)\n",
    "    embedding_layer_skip_grams = tf.keras.layers.Embedding(vocab_size_sg, 32)(input_skip_grams)\n",
    "\n",
    "    #BI-LSTMs for each of the inputs\n",
    "    sequence_output_1 = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(70,return_sequences=True))(embedding_layer_ngrams)\n",
    "    attention_output_1 = attention(return_sequences=False)(sequence_output_1)\n",
    "    dropout_1 = tf.keras.layers.Dropout(0.3)(attention_output_1)\n",
    "    dense_layer_1 = tf.keras.layers.Dense(NUM_CLASSES, activation='sigmoid')(dropout_1)\n",
    "\n",
    "    sequence_output_2 = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(70, return_sequences=True))(embedding_layer_skip_grams)\n",
    "    attention_output_2 = attention(return_sequences=False)(sequence_output_2)\n",
    "    dropout_2 = tf.keras.layers.Dropout(0.3)(attention_output_2)\n",
    "    dense_layer_2 = tf.keras.layers.Dense(NUM_CLASSES, activation='sigmoid')(dropout_2)\n",
    "\n",
    "    max_layer = tf.keras.layers.Maximum()([dense_layer_1,dense_layer_2])\n",
    "\n",
    "    model = tf.keras.models.Model(\n",
    "        inputs=[\n",
    "            input_ngrams,\n",
    "            input_skip_grams\n",
    "        ], \n",
    "        outputs=max_layer)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4236670c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(predictions, start_index, end_index):\n",
    "    final_predictions = []\n",
    "    actual_y_test = []\n",
    "\n",
    "    current_entry = ''\n",
    "    counter = 0\n",
    "    total_counts = 0\n",
    "\n",
    "    if len(predictions) == len(training_data[start_index: end_index]):\n",
    "        temp = np.zeros(NUM_CLASSES)\n",
    "        for i in range(len(predictions)):\n",
    "            if current_entry != training_data[start_index+i][0]:\n",
    "                #compute prev\n",
    "                if i!=0:\n",
    "                    temp /= counter\n",
    "                    final_predictions.append(temp)\n",
    "\n",
    "                #reset\n",
    "                total_counts += counter\n",
    "                counter = 1\n",
    "                temp = np.zeros(NUM_CLASSES)\n",
    "\n",
    "                #init new\n",
    "                current_entry = training_data[start_index+i][0]\n",
    "                temp += np.array(predictions[i])\n",
    "                actual_y_test.append(training_data[start_index+i][2])\n",
    "            else:\n",
    "                temp += np.array(predictions[i])\n",
    "                counter += 1\n",
    "\n",
    "        total_counts += counter\n",
    "        temp /= counter\n",
    "        final_predictions.append(temp)\n",
    "\n",
    "    else:\n",
    "        print('Lengths of predictions dont match with test data')\n",
    "    \n",
    "    final_predictions = np.array(final_predictions, dtype=float)\n",
    "    actual_y_test = np.array(actual_y_test, dtype=float)\n",
    "    \n",
    "    rec = recall(actual_y_test,final_predictions)\n",
    "    prec = precision(actual_y_test,final_predictions)\n",
    "    f1 = f1_score(actual_y_test,final_predictions)\n",
    "    \n",
    "    return rec,prec,f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1454935",
   "metadata": {},
   "source": [
    "### Model using ngrams only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "894bcfa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Considering ngrams\n",
    "X_ng = [' '.join(sample[1]) for sample in training_data_ngrams]\n",
    "y = [sample[2] for sample in training_data]\n",
    "\n",
    "del training_data_ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f6664241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "****Fold: 1 ****\n",
      "Splitting into train-test...\n",
      "Tokenizing...\n",
      "Shuffling...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-12 19:34:38.409321: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30971 MB memory:  -> device: 0, name: Tesla V100-DGXS-32GB, pci bus id: 0000:0f:00.0, compute capability: 7.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-12 19:34:44.786692: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15890/15890 [==============================] - 184s 11ms/step - loss: 0.1645 - recall: 0.0508 - precision: 0.0420 - f1_score: nan - val_loss: 0.1416 - val_recall: 0.1206 - val_precision: 0.1068 - val_f1_score: nan\n",
      "Epoch 2/5\n",
      "15890/15890 [==============================] - 178s 11ms/step - loss: 0.1313 - recall: 0.1879 - precision: 0.1583 - f1_score: nan - val_loss: 0.1186 - val_recall: 0.2433 - val_precision: 0.2211 - val_f1_score: 0.2308\n",
      "Epoch 3/5\n",
      "15890/15890 [==============================] - 178s 11ms/step - loss: 0.1136 - recall: 0.3020 - precision: 0.2514 - f1_score: 0.2733 - val_loss: 0.1062 - val_recall: 0.3378 - val_precision: 0.2983 - val_f1_score: 0.3160\n",
      "Epoch 4/5\n",
      "15890/15890 [==============================] - 178s 11ms/step - loss: 0.1027 - recall: 0.3743 - precision: 0.3143 - f1_score: 0.3407 - val_loss: 0.0989 - val_recall: 0.3961 - val_precision: 0.3526 - val_f1_score: 0.3723\n",
      "Epoch 5/5\n",
      "15890/15890 [==============================] - 178s 11ms/step - loss: 0.0952 - recall: 0.4219 - precision: 0.3581 - f1_score: 0.3865 - val_loss: 0.0932 - val_recall: 0.4168 - val_precision: 0.3819 - val_f1_score: 0.3979\n",
      "\n",
      "Evaluating model...\n",
      "4966/4966 [==============================] - 20s 4ms/step\n",
      "\n",
      "\n",
      "****Fold: 2 ****\n",
      "Splitting into train-test...\n",
      "Tokenizing...\n",
      "Shuffling...\n",
      "Training...\n",
      "Epoch 1/5\n",
      "15890/15890 [==============================] - 184s 11ms/step - loss: 0.1644 - recall: 0.0526 - precision: 0.0434 - f1_score: nan - val_loss: 0.1415 - val_recall: 0.1180 - val_precision: 0.1081 - val_f1_score: nan\n",
      "Epoch 2/5\n",
      "15890/15890 [==============================] - 178s 11ms/step - loss: 0.1314 - recall: 0.1901 - precision: 0.1643 - f1_score: nan - val_loss: 0.1190 - val_recall: 0.2549 - val_precision: 0.2274 - val_f1_score: 0.2395\n",
      "Epoch 3/5\n",
      "15890/15890 [==============================] - 178s 11ms/step - loss: 0.1140 - recall: 0.3017 - precision: 0.2567 - f1_score: 0.2764 - val_loss: 0.1064 - val_recall: 0.3388 - val_precision: 0.3050 - val_f1_score: 0.3203\n",
      "Epoch 4/5\n",
      "15890/15890 [==============================] - 176s 11ms/step - loss: 0.1033 - recall: 0.3711 - precision: 0.3160 - f1_score: 0.3404 - val_loss: 0.0992 - val_recall: 0.3853 - val_precision: 0.3471 - val_f1_score: 0.3645\n",
      "Epoch 5/5\n",
      "15890/15890 [==============================] - 176s 11ms/step - loss: 0.0961 - recall: 0.4148 - precision: 0.3541 - f1_score: 0.3811 - val_loss: 0.0942 - val_recall: 0.4219 - val_precision: 0.3771 - val_f1_score: 0.3975\n",
      "\n",
      "Evaluating model...\n",
      "4966/4966 [==============================] - 20s 4ms/step\n",
      "\n",
      "\n",
      "****Fold: 3 ****\n",
      "Splitting into train-test...\n",
      "Tokenizing...\n",
      "Shuffling...\n",
      "Training...\n",
      "Epoch 1/5\n",
      "15890/15890 [==============================] - 180s 11ms/step - loss: 0.1633 - recall: 0.0530 - precision: 0.0430 - f1_score: nan - val_loss: 0.1398 - val_recall: 0.1335 - val_precision: 0.1119 - val_f1_score: nan\n",
      "Epoch 2/5\n",
      "15890/15890 [==============================] - 175s 11ms/step - loss: 0.1287 - recall: 0.2031 - precision: 0.1695 - f1_score: nan - val_loss: 0.1167 - val_recall: 0.2650 - val_precision: 0.2345 - val_f1_score: 0.2479\n",
      "Epoch 3/5\n",
      "15890/15890 [==============================] - 176s 11ms/step - loss: 0.1111 - recall: 0.3197 - precision: 0.2683 - f1_score: 0.2907 - val_loss: 0.1041 - val_recall: 0.3539 - val_precision: 0.3192 - val_f1_score: 0.3349\n",
      "Epoch 4/5\n",
      "15890/15890 [==============================] - 175s 11ms/step - loss: 0.1003 - recall: 0.3907 - precision: 0.3317 - f1_score: 0.3578 - val_loss: 0.0974 - val_recall: 0.3988 - val_precision: 0.3604 - val_f1_score: 0.3779\n",
      "Epoch 5/5\n",
      "15890/15890 [==============================] - 176s 11ms/step - loss: 0.0930 - recall: 0.4351 - precision: 0.3718 - f1_score: 0.4000 - val_loss: 0.0916 - val_recall: 0.4363 - val_precision: 0.3977 - val_f1_score: 0.4154\n",
      "\n",
      "Evaluating model...\n",
      "4966/4966 [==============================] - 20s 4ms/step\n",
      "\n",
      "\n",
      "****Fold: 4 ****\n",
      "Splitting into train-test...\n",
      "Tokenizing...\n",
      "Shuffling...\n",
      "Training...\n",
      "Epoch 1/5\n",
      "15890/15890 [==============================] - 180s 11ms/step - loss: 0.1638 - recall: 0.0536 - precision: 0.0455 - f1_score: nan - val_loss: 0.1411 - val_recall: 0.1217 - val_precision: 0.1140 - val_f1_score: nan\n",
      "Epoch 2/5\n",
      "15890/15890 [==============================] - 175s 11ms/step - loss: 0.1297 - recall: 0.1980 - precision: 0.1706 - f1_score: nan - val_loss: 0.1179 - val_recall: 0.2654 - val_precision: 0.2338 - val_f1_score: 0.2476\n",
      "Epoch 3/5\n",
      "15890/15890 [==============================] - 175s 11ms/step - loss: 0.1127 - recall: 0.3065 - precision: 0.2602 - f1_score: nan - val_loss: 0.1066 - val_recall: 0.3344 - val_precision: 0.2999 - val_f1_score: 0.3154\n",
      "Epoch 4/5\n",
      "15890/15890 [==============================] - 175s 11ms/step - loss: 0.1024 - recall: 0.3751 - precision: 0.3202 - f1_score: 0.3445 - val_loss: 0.0991 - val_recall: 0.3806 - val_precision: 0.3506 - val_f1_score: 0.3643\n",
      "Epoch 5/5\n",
      "15890/15890 [==============================] - 175s 11ms/step - loss: 0.0950 - recall: 0.4211 - precision: 0.3601 - f1_score: 0.3873 - val_loss: 0.0934 - val_recall: 0.4223 - val_precision: 0.3852 - val_f1_score: 0.4022\n",
      "\n",
      "Evaluating model...\n",
      "4966/4966 [==============================] - 20s 4ms/step\n",
      "\n",
      "\n",
      "****Fold: 5 ****\n",
      "Splitting into train-test...\n",
      "Tokenizing...\n",
      "Shuffling...\n",
      "Training...\n",
      "Epoch 1/5\n",
      "15890/15890 [==============================] - 180s 11ms/step - loss: 0.1638 - recall: 0.0530 - precision: 0.0456 - f1_score: nan - val_loss: 0.1408 - val_recall: 0.1219 - val_precision: 0.1089 - val_f1_score: nan\n",
      "Epoch 2/5\n",
      "15890/15890 [==============================] - 175s 11ms/step - loss: 0.1296 - recall: 0.1971 - precision: 0.1688 - f1_score: nan - val_loss: 0.1174 - val_recall: 0.2520 - val_precision: 0.2308 - val_f1_score: 0.2401\n",
      "Epoch 3/5\n",
      "15890/15890 [==============================] - 175s 11ms/step - loss: 0.1118 - recall: 0.3150 - precision: 0.2643 - f1_score: 0.2863 - val_loss: 0.1048 - val_recall: 0.3461 - val_precision: 0.3098 - val_f1_score: 0.3262\n",
      "Epoch 4/5\n",
      "15890/15890 [==============================] - 176s 11ms/step - loss: 0.1010 - recall: 0.3839 - precision: 0.3245 - f1_score: 0.3507 - val_loss: 0.0973 - val_recall: 0.3965 - val_precision: 0.3607 - val_f1_score: 0.3771\n",
      "Epoch 5/5\n",
      "15890/15890 [==============================] - 176s 11ms/step - loss: 0.0937 - recall: 0.4287 - precision: 0.3651 - f1_score: 0.3934 - val_loss: 0.0922 - val_recall: 0.4279 - val_precision: 0.3886 - val_f1_score: 0.4066\n",
      "\n",
      "Evaluating model...\n",
      "4966/4966 [==============================] - 20s 4ms/step\n",
      "Recall: 0.45885628576314363\n",
      "Precision: 0.4567105622410706\n",
      "F1-Score: 0.4577777078790065\n"
     ]
    }
   ],
   "source": [
    "Kfolds = 5\n",
    "prev_index = 0\n",
    "\n",
    "recs = []\n",
    "precs = []\n",
    "f1s = []\n",
    "\n",
    "for i in range(Kfolds):\n",
    "    print('\\n\\n****Fold:', i+1,'****')\n",
    "    \n",
    "    print('Splitting into train-test...')\n",
    "    X_train_ng, y_train, X_test_ng, y_test, start_index, prev_index = train_test_split(X_ng,y,i,prev_index,Kfolds)\n",
    "\n",
    "    print('Tokenizing...')\n",
    "    X_train_ng, X_test_ng, vocab_size_ng, tokenizer1 = tokenization(X_train_ng, X_test_ng, MAX_LEN_NG)\n",
    "    \n",
    "    print('Shuffling...')\n",
    "    shuffled = [[X_train_ng[i],y_train[i]] for i in range(len(X_train_ng))]\n",
    "    np.random.shuffle(shuffled)\n",
    "\n",
    "    X_train_ng = [shuffled[i][0] for i in range(len(shuffled))]\n",
    "    y_train = [shuffled[i][1] for i in range(len(shuffled))]\n",
    "    X_train_ng = np.array(X_train_ng)\n",
    "    y_train = np.array(y_train)\n",
    "    \n",
    "    \n",
    "    \n",
    "    model = get_model_ng(vocab_size_ng)\n",
    "    \n",
    "    model.compile(\n",
    "        loss=custom_loss, \n",
    "        optimizer='adam', \n",
    "        metrics=[\n",
    "            recall,\n",
    "            precision,\n",
    "            f1_score\n",
    "        ])\n",
    "    \n",
    "    print('Training...')\n",
    "    history = model.fit(X_train_ng, y_train, batch_size=32, epochs=5,validation_split=0.2)\n",
    "    \n",
    "    print('\\nEvaluating model...')\n",
    "    predictions = model.predict(X_test_ng)\n",
    "    \n",
    "    rec, prec, f1 = compute_metrics(predictions, start_index, prev_index)\n",
    "    \n",
    "    recs.append(rec.numpy())\n",
    "    precs.append(prec.numpy())\n",
    "    f1s.append(f1.numpy())\n",
    "\n",
    "print('Recall:',sum(recs)/len(recs))\n",
    "print('Precision:',sum(precs)/len(precs))\n",
    "print('F1-Score:',sum(f1s)/len(f1s))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e28840b",
   "metadata": {},
   "source": [
    "### Model using skip grams only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fbba7e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Considering skip grams\n",
    "X_sg = [' '.join(sample[1]) for sample in training_data_skip_grams]\n",
    "y = [sample[2] for sample in training_data]\n",
    "\n",
    "del training_data_skip_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "33e1124b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "****Fold: 1 ****\n",
      "Splitting into train-test...\n",
      "Tokenizing...\n",
      "Shuffling...\n",
      "Training...\n",
      "Epoch 1/5\n",
      "15890/15890 [==============================] - 386s 24ms/step - loss: 0.1637 - recall: 0.0490 - precision: 0.0411 - f1_score: nan - val_loss: 0.1446 - val_recall: 0.1165 - val_precision: 0.0972 - val_f1_score: nan\n",
      "Epoch 2/5\n",
      "15890/15890 [==============================] - 381s 24ms/step - loss: 0.1352 - recall: 0.1616 - precision: 0.1362 - f1_score: nan - val_loss: 0.1244 - val_recall: 0.2110 - val_precision: 0.1873 - val_f1_score: 0.1975\n",
      "Epoch 3/5\n",
      "15890/15890 [==============================] - 382s 24ms/step - loss: 0.1178 - recall: 0.2703 - precision: 0.2256 - f1_score: nan - val_loss: 0.1106 - val_recall: 0.3076 - val_precision: 0.2708 - val_f1_score: 0.2871\n",
      "Epoch 4/5\n",
      "15890/15890 [==============================] - 380s 24ms/step - loss: 0.1065 - recall: 0.3455 - precision: 0.2894 - f1_score: 0.3139 - val_loss: 0.1028 - val_recall: 0.3601 - val_precision: 0.3210 - val_f1_score: 0.3387\n",
      "Epoch 5/5\n",
      "15890/15890 [==============================] - 380s 24ms/step - loss: 0.0987 - recall: 0.3947 - precision: 0.3331 - f1_score: 0.3603 - val_loss: 0.0967 - val_recall: 0.4008 - val_precision: 0.3622 - val_f1_score: 0.3799\n",
      "Evaluating model...\n",
      "4966/4966 [==============================] - 48s 10ms/step\n",
      "\n",
      "\n",
      "****Fold: 2 ****\n",
      "Splitting into train-test...\n",
      "Tokenizing...\n",
      "Shuffling...\n",
      "Training...\n",
      "Epoch 1/5\n",
      "15890/15890 [==============================] - 385s 24ms/step - loss: 0.1628 - recall: 0.0553 - precision: 0.0465 - f1_score: nan - val_loss: 0.1421 - val_recall: 0.1232 - val_precision: 0.1047 - val_f1_score: nan\n",
      "Epoch 2/5\n",
      "15890/15890 [==============================] - 381s 24ms/step - loss: 0.1326 - recall: 0.1800 - precision: 0.1524 - f1_score: nan - val_loss: 0.1206 - val_recall: 0.2406 - val_precision: 0.2109 - val_f1_score: 0.2239\n",
      "Epoch 3/5\n",
      "15890/15890 [==============================] - 378s 24ms/step - loss: 0.1156 - recall: 0.2895 - precision: 0.2412 - f1_score: 0.2620 - val_loss: 0.1084 - val_recall: 0.3256 - val_precision: 0.2852 - val_f1_score: 0.3032\n",
      "Epoch 4/5\n",
      "15890/15890 [==============================] - 380s 24ms/step - loss: 0.1050 - recall: 0.3581 - precision: 0.3009 - f1_score: 0.3260 - val_loss: 0.1010 - val_recall: 0.3707 - val_precision: 0.3403 - val_f1_score: 0.3541\n",
      "Epoch 5/5\n",
      "15890/15890 [==============================] - 379s 24ms/step - loss: 0.0977 - recall: 0.4039 - precision: 0.3424 - f1_score: 0.3696 - val_loss: 0.0961 - val_recall: 0.4022 - val_precision: 0.3690 - val_f1_score: 0.3842\n",
      "Evaluating model...\n",
      "4966/4966 [==============================] - 47s 9ms/step\n",
      "\n",
      "\n",
      "****Fold: 3 ****\n",
      "Splitting into train-test...\n",
      "Tokenizing...\n",
      "Shuffling...\n",
      "Training...\n",
      "Epoch 1/5\n",
      "15890/15890 [==============================] - 384s 24ms/step - loss: 0.1650 - recall: 0.0439 - precision: 0.0385 - f1_score: nan - val_loss: 0.1438 - val_recall: 0.1074 - val_precision: 0.0958 - val_f1_score: nan\n",
      "Epoch 2/5\n",
      "15890/15890 [==============================] - 380s 24ms/step - loss: 0.1348 - recall: 0.1662 - precision: 0.1419 - f1_score: nan - val_loss: 0.1224 - val_recall: 0.2276 - val_precision: 0.1987 - val_f1_score: 0.2111\n",
      "Epoch 3/5\n",
      "15890/15890 [==============================] - 379s 24ms/step - loss: 0.1174 - recall: 0.2758 - precision: 0.2281 - f1_score: 0.2485 - val_loss: 0.1096 - val_recall: 0.3168 - val_precision: 0.2742 - val_f1_score: 0.2931\n",
      "Epoch 4/5\n",
      "15890/15890 [==============================] - 377s 24ms/step - loss: 0.1064 - recall: 0.3485 - precision: 0.2899 - f1_score: 0.3154 - val_loss: 0.1024 - val_recall: 0.3597 - val_precision: 0.3195 - val_f1_score: 0.3376\n",
      "Epoch 5/5\n",
      "15890/15890 [==============================] - 379s 24ms/step - loss: 0.0988 - recall: 0.3953 - precision: 0.3320 - f1_score: 0.3599 - val_loss: 0.0960 - val_recall: 0.3966 - val_precision: 0.3601 - val_f1_score: 0.3768\n",
      "Evaluating model...\n",
      "4966/4966 [==============================] - 48s 9ms/step\n",
      "\n",
      "\n",
      "****Fold: 4 ****\n",
      "Splitting into train-test...\n",
      "Tokenizing...\n",
      "Shuffling...\n",
      "Training...\n",
      "Epoch 1/5\n",
      "15890/15890 [==============================] - 383s 24ms/step - loss: 0.1654 - recall: 0.0442 - precision: 0.0376 - f1_score: nan - val_loss: 0.1455 - val_recall: 0.0975 - val_precision: 0.0834 - val_f1_score: nan\n",
      "Epoch 2/5\n",
      "15890/15890 [==============================] - 382s 24ms/step - loss: 0.1350 - recall: 0.1651 - precision: 0.1360 - f1_score: nan - val_loss: 0.1224 - val_recall: 0.2279 - val_precision: 0.1943 - val_f1_score: 0.2087\n",
      "Epoch 3/5\n",
      "15890/15890 [==============================] - 381s 24ms/step - loss: 0.1175 - recall: 0.2746 - precision: 0.2269 - f1_score: 0.2472 - val_loss: 0.1095 - val_recall: 0.3075 - val_precision: 0.2680 - val_f1_score: 0.2855\n",
      "Epoch 4/5\n",
      "15890/15890 [==============================] - 381s 24ms/step - loss: 0.1064 - recall: 0.3475 - precision: 0.2894 - f1_score: 0.3147 - val_loss: 0.1015 - val_recall: 0.3626 - val_precision: 0.3241 - val_f1_score: 0.3415\n",
      "Epoch 5/5\n",
      "15890/15890 [==============================] - 381s 24ms/step - loss: 0.0988 - recall: 0.3950 - precision: 0.3307 - f1_score: 0.3590 - val_loss: 0.0962 - val_recall: 0.4006 - val_precision: 0.3610 - val_f1_score: 0.3791\n",
      "Evaluating model...\n",
      "4966/4966 [==============================] - 47s 9ms/step\n",
      "\n",
      "\n",
      "****Fold: 5 ****\n",
      "Splitting into train-test...\n",
      "Tokenizing...\n",
      "Shuffling...\n",
      "Training...\n",
      "Epoch 1/5\n",
      "15890/15890 [==============================] - 385s 24ms/step - loss: 0.1638 - recall: 0.0492 - precision: 0.0411 - f1_score: nan - val_loss: 0.1434 - val_recall: 0.1033 - val_precision: 0.0883 - val_f1_score: nan\n",
      "Epoch 2/5\n",
      "15890/15890 [==============================] - 380s 24ms/step - loss: 0.1339 - recall: 0.1691 - precision: 0.1413 - f1_score: nan - val_loss: 0.1218 - val_recall: 0.2244 - val_precision: 0.1992 - val_f1_score: 0.2101\n",
      "Epoch 3/5\n",
      "15890/15890 [==============================] - 380s 24ms/step - loss: 0.1166 - recall: 0.2786 - precision: 0.2338 - f1_score: 0.2531 - val_loss: 0.1089 - val_recall: 0.3163 - val_precision: 0.2795 - val_f1_score: 0.2959\n",
      "Epoch 4/5\n",
      "15890/15890 [==============================] - 381s 24ms/step - loss: 0.1056 - recall: 0.3496 - precision: 0.2959 - f1_score: 0.3195 - val_loss: 0.1015 - val_recall: 0.3610 - val_precision: 0.3288 - val_f1_score: 0.3435\n",
      "Epoch 5/5\n",
      "15890/15890 [==============================] - 381s 24ms/step - loss: 0.0981 - recall: 0.3968 - precision: 0.3376 - f1_score: 0.3638 - val_loss: 0.0961 - val_recall: 0.3934 - val_precision: 0.3621 - val_f1_score: 0.3764\n",
      "Evaluating model...\n",
      "4966/4966 [==============================] - 47s 9ms/step\n",
      "Recall: 0.4325576011383091\n",
      "Precision: 0.4313841703724625\n",
      "F1-Score: 0.43196778616543324\n"
     ]
    }
   ],
   "source": [
    "Kfolds = 5\n",
    "prev_index = 0\n",
    "\n",
    "recs = []\n",
    "precs = []\n",
    "f1s = []\n",
    "\n",
    "for i in range(Kfolds):\n",
    "    print('\\n\\n****Fold:', i+1,'****')\n",
    "    \n",
    "    print('Splitting into train-test...')\n",
    "    X_train_sg, y_train, X_test_sg, y_test, start_index, prev_index = train_test_split(X_sg,y,i,prev_index,Kfolds)\n",
    "\n",
    "    print('Tokenizing...')\n",
    "    X_train_sg, X_test_sg, vocab_size_sg, tokenizer1 = tokenization(X_train_sg, X_test_sg, MAX_LEN_SG)\n",
    "    \n",
    "    print('Shuffling...')\n",
    "    shuffled = [[X_train_sg[i],y_train[i]] for i in range(len(X_train_sg))]\n",
    "    np.random.shuffle(shuffled)\n",
    "\n",
    "    X_train_sg = [shuffled[i][0] for i in range(len(shuffled))]\n",
    "    y_train = [shuffled[i][1] for i in range(len(shuffled))]\n",
    "    X_train_sg = np.array(X_train_sg)\n",
    "    y_train = np.array(y_train)\n",
    "    \n",
    "\n",
    "    model = get_model_sg(vocab_size_sg)\n",
    "    \n",
    "    model.compile(\n",
    "        loss=custom_loss, \n",
    "        optimizer='adam', \n",
    "        metrics=[\n",
    "            recall,\n",
    "            precision,\n",
    "            f1_score\n",
    "        ])\n",
    "    \n",
    "    print('Training...')\n",
    "    history = model.fit(X_train_sg, y_train, batch_size=32, epochs=5,validation_split=0.2)\n",
    "    \n",
    "    print('Evaluating model...')\n",
    "    predictions = model.predict(X_test_sg)\n",
    "    \n",
    "    rec, prec, f1 = compute_metrics(predictions, start_index, prev_index)\n",
    "    \n",
    "    recs.append(rec.numpy())\n",
    "    precs.append(prec.numpy())\n",
    "    f1s.append(f1.numpy())\n",
    "\n",
    "print('Recall:',sum(recs)/len(recs))\n",
    "print('Precision:',sum(precs)/len(precs))\n",
    "print('F1-Score:',sum(f1s)/len(f1s))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84680b09",
   "metadata": {},
   "source": [
    "### Model using skip grams and ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6163d660",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Considering ngrams and skip grams\n",
    "X_ng = [' '.join(sample[1]) for sample in training_data_ngrams]\n",
    "X_sg = [' '.join(sample[1]) for sample in training_data_skip_grams]\n",
    "y = [sample[2] for sample in training_data]\n",
    "\n",
    "del training_data_ngrams\n",
    "del training_data_skip_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "32366243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "794486 794486\n"
     ]
    }
   ],
   "source": [
    "print(len(X_ng),len(X_sg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "eaad7bdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "****Fold: 1 ****\n",
      "Splitting into train-test...\n",
      "Tokenizing...\n",
      "Shuffling...\n",
      "Training...\n",
      "Epoch 1/5\n",
      "15890/15890 [==============================] - 526s 33ms/step - loss: 0.1604 - recall: 0.0736 - precision: 0.0629 - f1_score: nan - val_loss: 0.1374 - val_recall: 0.1604 - val_precision: 0.1328 - val_f1_score: nan\n",
      "Epoch 2/5\n",
      "15890/15890 [==============================] - 520s 33ms/step - loss: 0.1259 - recall: 0.2283 - precision: 0.1861 - f1_score: nan - val_loss: 0.1141 - val_recall: 0.2981 - val_precision: 0.2565 - val_f1_score: 0.2748\n",
      "Epoch 3/5\n",
      "15890/15890 [==============================] - 520s 33ms/step - loss: 0.1065 - recall: 0.3581 - precision: 0.2964 - f1_score: 0.3232 - val_loss: 0.1003 - val_recall: 0.3948 - val_precision: 0.3503 - val_f1_score: 0.3704\n",
      "Epoch 4/5\n",
      "15890/15890 [==============================] - 516s 33ms/step - loss: 0.0942 - recall: 0.4379 - precision: 0.3703 - f1_score: 0.4003 - val_loss: 0.0919 - val_recall: 0.4504 - val_precision: 0.4037 - val_f1_score: 0.4251\n",
      "Epoch 5/5\n",
      "15890/15890 [==============================] - 517s 33ms/step - loss: 0.0856 - recall: 0.4884 - precision: 0.4173 - f1_score: 0.4492 - val_loss: 0.0861 - val_recall: 0.4863 - val_precision: 0.4354 - val_f1_score: 0.4588\n",
      "Evaluating model...\n",
      "4966/4966 [==============================] - 64s 13ms/step\n",
      "\n",
      "\n",
      "****Fold: 2 ****\n",
      "Splitting into train-test...\n",
      "Tokenizing...\n",
      "Shuffling...\n",
      "Training...\n",
      "Epoch 1/5\n",
      "15890/15890 [==============================] - 536s 33ms/step - loss: 0.1610 - recall: 0.0662 - precision: 0.0580 - f1_score: nan - val_loss: 0.1363 - val_recall: 0.1494 - val_precision: 0.1350 - val_f1_score: nan\n",
      "Epoch 2/5\n",
      "15890/15890 [==============================] - 527s 33ms/step - loss: 0.1253 - recall: 0.2321 - precision: 0.1979 - f1_score: nan - val_loss: 0.1145 - val_recall: 0.2921 - val_precision: 0.2579 - val_f1_score: 0.2729\n",
      "Epoch 3/5\n",
      "15890/15890 [==============================] - 528s 33ms/step - loss: 0.1079 - recall: 0.3493 - precision: 0.2972 - f1_score: 0.3201 - val_loss: 0.1026 - val_recall: 0.3655 - val_precision: 0.3353 - val_f1_score: 0.3491\n",
      "Epoch 4/5\n",
      "15890/15890 [==============================] - 528s 33ms/step - loss: 0.0973 - recall: 0.4171 - precision: 0.3566 - f1_score: 0.3836 - val_loss: 0.0956 - val_recall: 0.4147 - val_precision: 0.3780 - val_f1_score: 0.3948\n",
      "Epoch 5/5\n",
      "15890/15890 [==============================] - 529s 33ms/step - loss: 0.0896 - recall: 0.4626 - precision: 0.3981 - f1_score: 0.4270 - val_loss: 0.0897 - val_recall: 0.4639 - val_precision: 0.4140 - val_f1_score: 0.4368\n",
      "Evaluating model...\n",
      "4966/4966 [==============================] - 65s 13ms/step\n",
      "\n",
      "\n",
      "****Fold: 3 ****\n",
      "Splitting into train-test...\n",
      "Tokenizing...\n",
      "Shuffling...\n",
      "Training...\n",
      "Epoch 1/5\n",
      "15890/15890 [==============================] - 541s 34ms/step - loss: 0.1615 - recall: 0.0694 - precision: 0.0615 - f1_score: nan - val_loss: 0.1352 - val_recall: 0.1685 - val_precision: 0.1458 - val_f1_score: nan\n",
      "Epoch 2/5\n",
      "15890/15890 [==============================] - 532s 33ms/step - loss: 0.1255 - recall: 0.2326 - precision: 0.1987 - f1_score: nan - val_loss: 0.1137 - val_recall: 0.2845 - val_precision: 0.2565 - val_f1_score: 0.2689\n",
      "Epoch 3/5\n",
      "15890/15890 [==============================] - 533s 34ms/step - loss: 0.1081 - recall: 0.3465 - precision: 0.2931 - f1_score: 0.3165 - val_loss: 0.1013 - val_recall: 0.3867 - val_precision: 0.3354 - val_f1_score: 0.3584\n",
      "Epoch 4/5\n",
      "15890/15890 [==============================] - 532s 33ms/step - loss: 0.0969 - recall: 0.4171 - precision: 0.3579 - f1_score: 0.3843 - val_loss: 0.0938 - val_recall: 0.4281 - val_precision: 0.3865 - val_f1_score: 0.4055\n",
      "Epoch 5/5\n",
      "15890/15890 [==============================] - 532s 33ms/step - loss: 0.0892 - recall: 0.4632 - precision: 0.3999 - f1_score: 0.4283 - val_loss: 0.0891 - val_recall: 0.4471 - val_precision: 0.4178 - val_f1_score: 0.4314\n",
      "Evaluating model...\n",
      "4966/4966 [==============================] - 66s 13ms/step\n",
      "\n",
      "\n",
      "****Fold: 4 ****\n",
      "Splitting into train-test...\n",
      "Tokenizing...\n",
      "Shuffling...\n",
      "Training...\n",
      "Epoch 1/5\n",
      "15890/15890 [==============================] - 537s 33ms/step - loss: 0.1593 - recall: 0.0704 - precision: 0.0618 - f1_score: nan - val_loss: 0.1356 - val_recall: 0.1631 - val_precision: 0.1423 - val_f1_score: nan\n",
      "Epoch 2/5\n",
      "15890/15890 [==============================] - 531s 33ms/step - loss: 0.1223 - recall: 0.2508 - precision: 0.2084 - f1_score: nan - val_loss: 0.1107 - val_recall: 0.3281 - val_precision: 0.2815 - val_f1_score: 0.3021\n",
      "Epoch 3/5\n",
      "15890/15890 [==============================] - 532s 34ms/step - loss: 0.1034 - recall: 0.3760 - precision: 0.3178 - f1_score: 0.3435 - val_loss: 0.0989 - val_recall: 0.3893 - val_precision: 0.3595 - val_f1_score: 0.3731\n",
      "Epoch 4/5\n",
      "15890/15890 [==============================] - 534s 34ms/step - loss: 0.0922 - recall: 0.4464 - precision: 0.3824 - f1_score: 0.4110 - val_loss: 0.0908 - val_recall: 0.4606 - val_precision: 0.4120 - val_f1_score: 0.4342\n",
      "Epoch 5/5\n",
      "15890/15890 [==============================] - 531s 33ms/step - loss: 0.0845 - recall: 0.4923 - precision: 0.4237 - f1_score: 0.4546 - val_loss: 0.0855 - val_recall: 0.4859 - val_precision: 0.4429 - val_f1_score: 0.4627\n",
      "Evaluating model...\n",
      "4966/4966 [==============================] - 64s 13ms/step\n",
      "\n",
      "\n",
      "****Fold: 5 ****\n",
      "Splitting into train-test...\n",
      "Tokenizing...\n",
      "Shuffling...\n",
      "Training...\n",
      "Epoch 1/5\n",
      "15890/15890 [==============================] - 536s 33ms/step - loss: 0.1606 - recall: 0.0708 - precision: 0.0626 - f1_score: nan - val_loss: 0.1350 - val_recall: 0.1678 - val_precision: 0.1506 - val_f1_score: nan\n",
      "Epoch 2/5\n",
      "15890/15890 [==============================] - 528s 33ms/step - loss: 0.1236 - recall: 0.2450 - precision: 0.2057 - f1_score: nan - val_loss: 0.1114 - val_recall: 0.3152 - val_precision: 0.2732 - val_f1_score: 0.2918\n",
      "Epoch 3/5\n",
      "15890/15890 [==============================] - 527s 33ms/step - loss: 0.1052 - recall: 0.3680 - precision: 0.3067 - f1_score: 0.3335 - val_loss: 0.0990 - val_recall: 0.3902 - val_precision: 0.3510 - val_f1_score: 0.3688\n",
      "Epoch 4/5\n",
      "15890/15890 [==============================] - 529s 33ms/step - loss: 0.0939 - recall: 0.4370 - precision: 0.3706 - f1_score: 0.4001 - val_loss: 0.0916 - val_recall: 0.4369 - val_precision: 0.3996 - val_f1_score: 0.4167\n",
      "Epoch 5/5\n",
      "15890/15890 [==============================] - 528s 33ms/step - loss: 0.0859 - recall: 0.4829 - precision: 0.4134 - f1_score: 0.4446 - val_loss: 0.0861 - val_recall: 0.4770 - val_precision: 0.4307 - val_f1_score: 0.4520\n",
      "Evaluating model...\n",
      "4966/4966 [==============================] - 64s 13ms/step\n",
      "Recall: 0.5011510643768025\n",
      "Precision: 0.4983812820837801\n",
      "F1-Score: 0.4997568119491115\n"
     ]
    }
   ],
   "source": [
    "Kfolds = 5\n",
    "prev_index = 0\n",
    "\n",
    "recs = []\n",
    "precs = []\n",
    "f1s = []\n",
    "\n",
    "for i in range(Kfolds):\n",
    "    print('\\n\\n****Fold:', i+1,'****')\n",
    "    \n",
    "    print('Splitting into train-test...')\n",
    "    X_train_ng, y_train, X_test_ng, y_test, start_index, prev_index1 = train_test_split(X_ng,y,i,prev_index,Kfolds)\n",
    "    X_train_sg, _, X_test_sg, _, _, _= train_test_split(X_sg,y,i,prev_index,Kfolds)\n",
    "    \n",
    "    prev_index = prev_index1\n",
    "    \n",
    "    print('Tokenizing...')\n",
    "    X_train_ng, X_test_ng, vocab_size_ng, tokenizer1 = tokenization(X_train_ng, X_test_ng, MAX_LEN_NG)  \n",
    "    X_train_sg, X_test_sg, vocab_size_sg, tokenizer2 = tokenization(X_train_sg, X_test_sg, MAX_LEN_SG)\n",
    "    \n",
    "   \n",
    "    print('Shuffling...')   \n",
    "    shuffled = [[X_train_ng[i],X_train_sg[i],y_train[i]] for i in range(len(X_train_ng))]\n",
    "    np.random.shuffle(shuffled)\n",
    "\n",
    "    X_train_ng = [shuffled[i][0] for i in range(len(shuffled))]\n",
    "    X_train_sg = [shuffled[i][1] for i in range(len(shuffled))]\n",
    "    y_train = [shuffled[i][2] for i in range(len(shuffled))]\n",
    "    X_train_ng = np.array(X_train_ng)\n",
    "    X_train_sg = np.array(X_train_sg)\n",
    "    y_train = np.array(y_train)\n",
    "    \n",
    "    \n",
    "    model = get_model_ng_sg(vocab_size_ng, vocab_size_sg)\n",
    "    \n",
    "    model.compile(\n",
    "        loss=custom_loss, \n",
    "        optimizer='adam', \n",
    "        metrics=[\n",
    "            recall,\n",
    "            precision,\n",
    "            f1_score\n",
    "        ])\n",
    "\n",
    "    print('Training...')\n",
    "    history = model.fit([X_train_ng,X_train_sg], y_train, batch_size=32, epochs=5,validation_split=0.2)\n",
    "    \n",
    "    print('Evaluating model...')\n",
    "    predictions = model.predict([X_test_ng,X_test_sg])\n",
    "    \n",
    "    rec, prec, f1 = compute_metrics(predictions, start_index, prev_index)\n",
    "    \n",
    "    recs.append(rec.numpy())\n",
    "    precs.append(prec.numpy())\n",
    "    f1s.append(f1.numpy())\n",
    "\n",
    "print('Recall:',sum(recs)/len(recs))\n",
    "print('Precision:',sum(precs)/len(precs))\n",
    "print('F1-Score:',sum(f1s)/len(f1s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d3bd13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
